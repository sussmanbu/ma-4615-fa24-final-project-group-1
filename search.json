[
  {
    "objectID": "Yifei private folder- not submit/pci-analysis-1-edited.html",
    "href": "Yifei private folder- not submit/pci-analysis-1-edited.html",
    "title": "PCI Analysis 1",
    "section": "",
    "text": "# Convert CSV to RData if necessary\nif (!file.exists(rdata_file_path)) {\n  Pavement_Condition_Index &lt;- read.csv(csv_file_path)\n  save(Pavement_Condition_Index, file = rdata_file_path)\n  message(\"Converted CSV to RData: \", rdata_file_path)\n} else {\n  message(\"RData file already exists: \", rdata_file_path)\n}\n\nRData file already exists: dataset/Pavement_Condition_Index.RData\n\n\n\n# Load datasets\ncleaned_dataset &lt;- readRDS(\"dataset/cleaned_dataset.rds\")\nload(rdata_file_path)\n\n# Inspect column names and structure\nprint(names(Pavement_Condition_Index))\n\n [1] \"the_geom\"   \"OBJECTID\"   \"Segment_ID\" \"StreetName\" \"CENSUS_ID\" \n [6] \"DEPOT_NUMB\" \"DEPOT_NAME\" \"MOCO_MAINT\" \"Shape_Leng\" \"FromStreet\"\n[11] \"ToStreet\"   \"Length_1\"   \"Width\"      \"SurfaceTyp\" \"PCI\"       \n\nprint(class(Pavement_Condition_Index))\n\n[1] \"data.frame\"\n\n\n\n# Standardize road names to ensure proper matching\ncleaned_dataset &lt;- cleaned_dataset %&gt;%\n  mutate(Road.Name = tolower(trimws(Road.Name)))\n\nPavement_Condition_Index &lt;- Pavement_Condition_Index %&gt;%\n  mutate(StreetName = tolower(trimws(StreetName)))\n\n\n# Merge datasets by standardized road names\nmerged_data &lt;- cleaned_dataset %&gt;%\n  left_join(Pavement_Condition_Index, by = c(\"Road.Name\" = \"StreetName\"))\n\nWarning in left_join(., Pavement_Condition_Index, by = c(Road.Name = \"StreetName\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 5 of `x` matches multiple rows in `y`.\nℹ Row 24360 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n# Check for successful join\nprint(dim(merged_data))\n\n[1] 11167    53\n\nsummary(merged_data)\n\n Report.Number      Local.Case.Number  Agency.Name        ACRS.Report.Type  \n Length:11167       Length:11167       Length:11167       Length:11167      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n Crash.Date.Time                   Route.Type         Road.Name        \n Min.   :2015-01-17 16:09:00.00   Length:11167       Length:11167      \n 1st Qu.:2023-08-19 19:10:00.00   Class :character   Class :character  \n Median :2023-10-06 07:45:00.00   Mode  :character   Mode  :character  \n Mean   :2023-04-30 17:40:21.15                                        \n 3rd Qu.:2023-11-17 17:25:00.00                                        \n Max.   :2023-12-31 11:00:00.00                                        \n                                                                       \n Cross.Street.Name  Off.Road.Description Municipality      \n Length:11167       Length:11167         Length:11167      \n Class :character   Class :character     Class :character  \n Mode  :character   Mode  :character     Mode  :character  \n                                                           \n                                                           \n                                                           \n                                                           \n Related.Non.Motorist Collision.Type       Weather          Surface.Condition \n Length:11167         Length:11167       Length:11167       Length:11167      \n Class :character     Class :character   Class :character   Class :character  \n Mode  :character     Mode  :character   Mode  :character   Mode  :character  \n                                                                              \n                                                                              \n                                                                              \n                                                                              \n    Light           Traffic.Control    Driver.Substance.Abuse\n Length:11167       Length:11167       Length:11167          \n Class :character   Class :character   Class :character      \n Mode  :character   Mode  :character   Mode  :character      \n                                                             \n                                                             \n                                                             \n                                                             \n Non.Motorist.Substance.Abuse  Person.ID         Driver.At.Fault   \n Length:11167                 Length:11167       Length:11167      \n Class :character             Class :character   Class :character  \n Mode  :character             Mode  :character   Mode  :character  \n                                                                   \n                                                                   \n                                                                   \n                                                                   \n Injury.Severity    Circumstance       Driver.Distracted.By\n Length:11167       Length:11167       Length:11167        \n Class :character   Class :character   Class :character    \n Mode  :character   Mode  :character   Mode  :character    \n                                                           \n                                                           \n                                                           \n                                                           \n Drivers.License.State  Vehicle.ID        Vehicle.Damage.Extent\n Length:11167          Length:11167       Length:11167         \n Class :character      Class :character   Class :character     \n Mode  :character      Mode  :character   Mode  :character     \n                                                               \n                                                               \n                                                               \n                                                               \n Vehicle.First.Impact.Location Vehicle.Body.Type  Vehicle.Movement  \n Length:11167                  Length:11167       Length:11167      \n Class :character              Class :character   Class :character  \n Mode  :character              Mode  :character   Mode  :character  \n                                                                    \n                                                                    \n                                                                    \n                                                                    \n Vehicle.Going.Dir   Speed.Limit Driverless.Vehicle Parked.Vehicle    \n Length:11167       Min.   : 0   Length:11167       Length:11167      \n Class :character   1st Qu.:30   Class :character   Class :character  \n Mode  :character   Median :35   Mode  :character   Mode  :character  \n                    Mean   :34                                        \n                    3rd Qu.:40                                        \n                    Max.   :65                                        \n                                                                      \n  Vehicle.Year  Vehicle.Make       Vehicle.Model         Latitude    \n Min.   :   0   Length:11167       Length:11167       Min.   :38.74  \n 1st Qu.:2011   Class :character   Class :character   1st Qu.:39.05  \n Median :2015   Mode  :character   Mode  :character   Median :39.08  \n Mean   :1991                                         Mean   :39.09  \n 3rd Qu.:2019                                         3rd Qu.:39.13  \n Max.   :9999                                         Max.   :39.32  \n                                                                     \n   Longitude        Location           the_geom            OBJECTID    \n Min.   :-77.55   Length:11167       Length:11167       Min.   :   11  \n 1st Qu.:-77.18   Class :character   Class :character   1st Qu.: 3383  \n Median :-77.11   Mode  :character   Mode  :character   Median :12436  \n Mean   :-77.12                                         Mean   :11972  \n 3rd Qu.:-77.05                                         3rd Qu.:20035  \n Max.   :-76.93                                         Max.   :25168  \n                                                        NA's   :460    \n   Segment_ID       CENSUS_ID        DEPOT_NUMB     DEPOT_NAME       \n Min.   :  2312   Min.   :  2312   Min.   :11.00   Length:11167      \n 1st Qu.: 69450   1st Qu.: 69450   1st Qu.:13.00   Class :character  \n Median :108661   Median :108661   Median :14.00   Mode  :character  \n Mean   :103215   Mean   :103215   Mean   :14.13                     \n 3rd Qu.:129678   3rd Qu.:129678   3rd Qu.:15.00                     \n Max.   :168168   Max.   :168168   Max.   :18.00                     \n NA's   :460      NA's   :460      NA's   :460                       \n  MOCO_MAINT          Shape_Leng        FromStreet          ToStreet        \n Length:11167       Min.   :   3.813   Length:11167       Length:11167      \n Class :character   1st Qu.: 264.792   Class :character   Class :character  \n Mode  :character   Median : 440.190   Mode  :character   Mode  :character  \n                    Mean   : 566.601                                        \n                    3rd Qu.: 702.607                                        \n                    Max.   :7272.388                                        \n                    NA's   :460                                             \n    Length_1            Width        SurfaceTyp             PCI         \n Min.   :   3.813   Min.   :14.00   Length:11167       Min.   :  6.059  \n 1st Qu.: 265.400   1st Qu.:24.00   Class :character   1st Qu.: 64.000  \n Median : 440.300   Median :46.00   Mode  :character   Median : 71.481  \n Mean   : 566.858   Mean   :46.52                      Mean   : 69.974  \n 3rd Qu.: 702.600   3rd Qu.:64.00                      3rd Qu.: 78.745  \n Max.   :7272.400   Max.   :98.00                      Max.   :100.000  \n NA's   :460        NA's   :460                        NA's   :460      \n\n# Debugging: Find unmatched road names\nunmatched_names &lt;- setdiff(cleaned_dataset$Road.Name, Pavement_Condition_Index$StreetName)\nprint(\"Unmatched road names:\")\n\n[1] \"Unmatched road names:\"\n\nprint(unmatched_names)\n\n  [1] \"\"                                        \n  [2] \"perry pkwy\"                              \n  [3] \"old columbia pike\"                       \n  [4] \"father hurley blvd\"                      \n  [5] \"piney branch rd\"                         \n  [6] \"eisenhower memorial hwy\"                 \n  [7] \"university blvd w\"                       \n  [8] \"rossmoor blvd\"                           \n  [9] \"colesville rd\"                           \n [10] \"congressional la\"                        \n [11] \"new hampshire ave\"                       \n [12] \"capital beltway\"                         \n [13] \"columbia pike\"                           \n [14] \"saybrooke oaks blvd\"                     \n [15] \"university blvd e\"                       \n [16] \"frederick rd\"                            \n [17] \"durbin terr\"                             \n [18] \"dickerson rd\"                            \n [19] \"montrose pkwy\"                           \n [20] \"first st\"                                \n [21] \"odendhal ave\"                            \n [22] \"wootton pkwy\"                            \n [23] \"snouffers school rd\"                     \n [24] \"firstfield rd\"                           \n [25] \"olney sandy spring rd\"                   \n [26] \"poplar run dr\"                           \n [27] \"s summit ave\"                            \n [28] \"ashton rd\"                               \n [29] \"w montgomery ave\"                        \n [30] \"muncaster mill rd\"                       \n [31] \"travis ave\"                              \n [32] \"sligo creek pkwy\"                        \n [33] \"ramp 4 fr is 270 nb to md 118 eb\"        \n [34] \"ramp 6 fr clara barton pkwy to is 495 sb\"\n [35] \"hungerford dr\"                           \n [36] \"snowden farm pkwy\"                       \n [37] \"halpine rd\"                              \n [38] \"knowles ave\"                             \n [39] \"ramp 9 fr is 370 eb to shady grove rd\"   \n [40] \"ramp 1 fr ramp 4 (us29)to cherry hill rd\"\n [41] \"olney laytonsville rd\"                   \n [42] \"goldsboro rd\"                            \n [43] \"bureau dr\"                               \n [44] \"rockville pike\"                          \n [45] \"macarthur blvd\"                          \n [46] \"piccard dr\"                              \n [47] \"manor crest lane\"                        \n [48] \"lewis ave\"                               \n [49] \"w diamond ave\"                           \n [50] \"w deer park rd\"                          \n [51] \"chevy chase st\"                          \n [52] \"n. leisure world blvd.\"                  \n [53] \"bradley blvd\"                            \n [54] \"ramp 8 fr us 29 sb to dustin rd\"         \n [55] \"olde towne ave\"                          \n [56] \"ramp 5 fr md200a sb to shady grove rd eb\"\n [57] \"kentlands blvd\"                          \n [58] \"wilson la\"                               \n [59] \"cider barrell dr\"                        \n [60] \"east village ave\"                        \n [61] \"democracy blvd\"                          \n [62] \"7000 carroll ave\"                        \n [63] \"poplar ave\"                              \n [64] \"teachers way\"                            \n [65] \"elmcroft blvd\"                           \n [66] \"ridgway ave\"                             \n [67] \"saddle ridge la\"                         \n [68] \"wisconsin ave\"                           \n [69] \"intercounty connector\"                   \n [70] \"main st\"                                 \n [71] \"national dr\"                             \n [72] \"twinbrook pkwy\"                          \n [73] \"shakespeare blvd\"                        \n [74] \"w gude dr\"                               \n [75] \"detrick ave\"                             \n [76] \"ridgemont ave\"                           \n [77] \"anne ave\"                                \n [78] \"sandy spring rd\"                         \n [79] \"anita ct\"                                \n [80] \"denham rd\"                               \n [81] \"mason dr\"                                \n [82] \"redland blvd\"                            \n [83] \"lower country dr\"                        \n [84] \"case st\"                                 \n [85] \"second ave\"                              \n [86] \"n summit ave\"                            \n [87] \"research blvd\"                           \n [88] \"baritone ct\"                             \n [89] \"ramp 1 fr ramp 4 (fr is270) to md 189 wb\"\n [90] \"key west ave\"                            \n [91] \"spencerville rd\"                         \n [92] \"east west hwy (w/b couplet)\"             \n [93] \"parker farm way\"                         \n [94] \"calverton blvd\"                          \n [95] \"ramp 1 fr is 270 nb to is 370 wb\"        \n [96] \"damascus rd\"                             \n [97] \"watkins mill road\"                       \n [98] \"s frederick rd\"                          \n [99] \"n horners la\"                            \n[100] \"russell ave\"                             \n[101] \"ramp 8 fr md 187b sb to is 270 wb\"       \n[102] \"park rd\"                                 \n[103] \"montgomery village avenue\"               \n\n\n\n# Calculate accident count for each Segment_ID\nmerged_data &lt;- merged_data %&gt;%\n  group_by(Segment_ID) %&gt;%\n  mutate(accident_count = n()) %&gt;%\n  ungroup()\n\n# Filter out NA values and create visualization\nfiltered_data &lt;- merged_data %&gt;%\n  filter(!is.na(PCI))\n\nif (nrow(filtered_data) &gt; 0) {\n  ggplot(filtered_data, aes(x = PCI, y = accident_count)) +\n    # Add points with slight transparency for better overlap visibility\n    geom_point(color = \"#0066CC\", alpha = 0.6, size = 1.5) +\n    \n    # Add smoothed line with confidence interval\n    geom_smooth(method = \"lm\", \n               color = \"#FF4444\", \n               size = 1.2,\n               se = TRUE,  # Show confidence interval\n               fill = \"#FF444433\") +  # Semi-transparent confidence interval\n    \n    # Customize labels and title\n    labs(\n      title = \"Relationship Between Pavement Condition and Accident Frequency\",\n      subtitle = \"Analysis of PCI Impact on Road Safety\",\n      x = \"Pavement Condition Index (PCI)\",\n      y = \"Number of Accidents\",\n      caption = \"Higher PCI indicates better pavement condition\"\n    ) +\n    \n    # Enhanced theme with better grid and formatting\n    theme_minimal() +\n    theme(\n      # Title formatting\n      plot.title = element_text(size = 14, face = \"bold\", margin = margin(b = 10)),\n      plot.subtitle = element_text(size = 11, color = \"gray40\", margin = margin(b = 20)),\n      \n      # Axis formatting\n      axis.title = element_text(size = 10, color = \"gray20\"),\n      axis.text = element_text(size = 9, color = \"gray40\"),\n      \n      # Grid formatting\n      panel.grid.major = element_line(color = \"gray90\"),\n      panel.grid.minor = element_line(color = \"gray95\"),\n      \n      # Add padding around plot\n      plot.margin = margin(20, 20, 20, 20)\n    ) +\n    \n    # Set appropriate scale breaks\n    scale_y_continuous(\n      breaks = seq(0, max(filtered_data$accident_count), by = 10),\n      expand = expansion(mult = c(0.02, 0.08))\n    ) +\n    scale_x_continuous(\n      breaks = seq(0, 100, by = 25),\n      limits = c(0, 100)\n    )\n}\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n# Perform stratified analysis by Traffic.Control\nif (\"Traffic.Control\" %in% names(merged_data)) {\n  merged_data %&gt;%\n    group_by(Traffic.Control) %&gt;%\n    summarize(\n      avg_accidents = mean(accident_count, na.rm = TRUE),\n      avg_pci = mean(PCI, na.rm = TRUE),\n      .groups = \"drop\"\n    ) %&gt;%\n    print()\n} else {\n  message(\"Traffic.Control column not found in merged_data.\")\n}\n\n# A tibble: 9 × 3\n  Traffic.Control         avg_accidents avg_pci\n  &lt;chr&gt;                           &lt;dbl&gt;   &lt;dbl&gt;\n1 FLASHING TRAFFIC SIGNAL         30.9     67.9\n2 N/A                             36.9     70.4\n3 NO CONTROLS                     31.4     69.3\n4 OTHER                           41.4     72.2\n5 PERSON                           7.05    66.0\n6 STOP SIGN                       21.6     70.4\n7 TRAFFIC SIGNAL                  26.5     70.4\n8 UNKNOWN                         40.3     69.5\n9 YIELD SIGN                      14.8     68.1\n\n\n\n# Create geo_analysis from merged_data\ngeo_analysis &lt;- merged_data %&gt;%\n  filter(!is.na(PCI)) %&gt;%                     # Filter rows with valid PCI\n  group_by(Segment_ID) %&gt;%                    # Group by Segment_ID or other grouping variable\n  summarize(\n    avg_pci = mean(PCI, na.rm = TRUE),        # Average PCI\n    accident_count = sum(accident_count),     # Total accident count\n    .groups = \"drop\"\n  )\n\n\nsummary(geo_analysis)\n\n   Segment_ID        avg_pci        accident_count   \n Min.   :  2312   Min.   :  6.059   Min.   :   1.00  \n 1st Qu.: 58375   1st Qu.: 63.125   1st Qu.:   1.00  \n Median :105535   Median : 71.653   Median :   4.00  \n Mean   : 90139   Mean   : 69.969   Mean   :  42.29  \n 3rd Qu.:119314   3rd Qu.: 79.216   3rd Qu.:  25.00  \n Max.   :168168   Max.   :100.000   Max.   :2209.00  \n\nhead(geo_analysis)\n\n# A tibble: 6 × 3\n  Segment_ID avg_pci accident_count\n       &lt;int&gt;   &lt;dbl&gt;          &lt;int&gt;\n1       2312    62.8              1\n2       2313    31.6              1\n3       2314    29.7              1\n4       2315    58.6              1\n5       8273    69.3              1\n6      10147    31.4              1\n\n\n\n# Check if geo_analysis exists and create enhanced logarithmic visualization\nif (exists(\"geo_analysis\")) {\n  geo_analysis %&gt;%\n    mutate(log_accident_count = log1p(accident_count)) %&gt;%\n    ggplot(aes(x = avg_pci, y = log_accident_count)) +\n    # Enhanced point styling\n    geom_point(\n      color = \"#1f77b4\",  # Professional blue color\n      alpha = 0.7,        # Slight transparency\n      size = 1.5,\n      shape = 16         # Filled circles\n    ) +\n    \n    # Enhanced trend line\n    geom_smooth(\n      method = \"lm\",\n      color = \"#d62728\",  # Professional red color\n      size = 1.2,\n      se = TRUE,          # Show confidence interval\n      fill = \"#d6272833\"  # Semi-transparent confidence interval\n    ) +\n    \n    # Enhanced labels and title\n    labs(\n      title = \"Relationship Between Pavement Condition and Accident Frequency\",\n      subtitle = \"Logarithmic Analysis of PCI Impact\",\n      x = \"Average Pavement Condition Index (PCI)\",\n      y = \"Log-transformed Accident Count\",\n      caption = \"Note: Accident counts are log-transformed (log1p) for better distribution analysis\"\n    ) +\n    \n    # Enhanced theme\n    theme_minimal() +\n    theme(\n      # Title styling\n      plot.title = element_text(\n        size = 14,\n        face = \"bold\",\n        margin = margin(b = 10)\n      ),\n      plot.subtitle = element_text(\n        size = 11,\n        color = \"gray40\",\n        margin = margin(b = 20)\n      ),\n      \n      # Axis styling\n      axis.title = element_text(\n        size = 10,\n        color = \"gray20\"\n      ),\n      axis.text = element_text(\n        size = 9,\n        color = \"gray40\"\n      ),\n      \n      # Grid styling\n      panel.grid.major = element_line(\n        color = \"gray90\",\n        linewidth = 0.5\n      ),\n      panel.grid.minor = element_line(\n        color = \"gray95\",\n        linewidth = 0.25\n      ),\n      \n      # Plot margins\n      plot.margin = margin(20, 20, 20, 20),\n      \n      # Caption styling\n      plot.caption = element_text(\n        size = 8,\n        color = \"gray40\",\n        margin = margin(t = 10)\n      )\n    ) +\n    \n    # Scale adjustments\n    scale_x_continuous(\n      breaks = seq(0, 100, by = 20),\n      limits = c(0, 100),\n      expand = expansion(mult = c(0.02, 0.02))\n    ) +\n    scale_y_continuous(\n      breaks = scales::pretty_breaks(n = 8),\n      expand = expansion(mult = c(0.02, 0.08))\n    )\n} else {\n  message(\"geo_analysis object not found. Please ensure the data is properly loaded.\")\n}\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "posts/2024-11-18-blog-post/blog-06.html",
    "href": "posts/2024-11-18-blog-post/blog-06.html",
    "title": "Blog Post 6 - Data Integration and Modeling",
    "section": "",
    "text": "The Pavement Condition Index (PCI) is a numerical expression ranging from 0 to 100, representing the condition of pavement. Based on my own hypothesis, assuming “the higher the PCI, the lower the accident rate,” I conducted a linear regression analysis using PCI (avg_pci) as the independent variable. The results show that the intercept (Intercept) is 11.09965, indicating the predicted number of accidents when avg_pci = 0. The estimated coefficient for avg_pci is -0.04909, which suggests that for every one-unit increase in PCI, the number of accidents decreases by 0.049 on average. This coefficient is statistically significant (p = 0.00337).\nHowever, the adjusted R-squared value is only 0.004102, indicating that PCI has very limited explanatory power for the number of accidents. The scatter plot shows that most data points are concentrated in the lower PCI range with considerable dispersion, and the linear regression line has a very slight negative slope. These observations suggest that PCI may have a minimal linear effect on accident rates.\n\n\n\nFurther analysis revealed that areas with a higher number of accidents tend to fall within the middle PCI range (30-80). This may imply that busier roads often have higher PCI values, and these roads also experience more accidents. This observation challenges the initial hypothesis and suggests a more complex relationship between PCI and the number of accidents.\n\n\n\nRelationship Between PCI and Accident Count\n\n\n\n\n\nTo explore the impact of PCI on accident rates more comprehensively, I am implementing the following strategies:\n\nIncorporating Additional Variables: After cleaning and integrating the data, I am including additional potential influencing factors such as traffic volume, weather, and speed limits. By using multivariable regression analysis, I aim to quantify the combined effects of these variables on accident rates.\nNonlinear Relationship Investigation: I will include quadratic or higher-order terms for PCI in the regression model to capture possible nonlinear patterns and improve the model’s fit, further validating the hypothesis.\nExploring Discrete Data Models: Given that accident rates are discrete count data, I may employ Poisson regression or negative binomial regression models for a more accurate representation of accident count distributions.\n\n\n\n\nCurrently, I am focusing on the following tasks:\n\nData Cleaning and Integration:\nI have standardized field names for consistency, resolving many-to-many relationships between road names across datasets, ensuring alignment of fields between different datasets.\nExploratory Statistical Analysis:\nI am grouping the data by key fields to calculate averages and accident counts, providing a foundation for subsequent modeling.\nRegression Modeling:\nI am gradually incorporating additional variables, such as weather and speed limits, to analyze their significant impacts on accident rates beyond PCI.\nVisualization of Relationships:\nI am creating scatter plots with trend lines to visually represent the relationships between variables, aiding in the interpretation and validation of analysis results."
  },
  {
    "objectID": "posts/2024-11-18-blog-post/blog-06.html#pavement-condition-index-pci",
    "href": "posts/2024-11-18-blog-post/blog-06.html#pavement-condition-index-pci",
    "title": "Blog Post 6 - Data Integration and Modeling",
    "section": "",
    "text": "The Pavement Condition Index (PCI) is a numerical expression ranging from 0 to 100, representing the condition of pavement. Based on my own hypothesis, assuming “the higher the PCI, the lower the accident rate,” I conducted a linear regression analysis using PCI (avg_pci) as the independent variable. The results show that the intercept (Intercept) is 11.09965, indicating the predicted number of accidents when avg_pci = 0. The estimated coefficient for avg_pci is -0.04909, which suggests that for every one-unit increase in PCI, the number of accidents decreases by 0.049 on average. This coefficient is statistically significant (p = 0.00337).\nHowever, the adjusted R-squared value is only 0.004102, indicating that PCI has very limited explanatory power for the number of accidents. The scatter plot shows that most data points are concentrated in the lower PCI range with considerable dispersion, and the linear regression line has a very slight negative slope. These observations suggest that PCI may have a minimal linear effect on accident rates.\n\n\n\nFurther analysis revealed that areas with a higher number of accidents tend to fall within the middle PCI range (30-80). This may imply that busier roads often have higher PCI values, and these roads also experience more accidents. This observation challenges the initial hypothesis and suggests a more complex relationship between PCI and the number of accidents.\n\n\n\nRelationship Between PCI and Accident Count\n\n\n\n\n\nTo explore the impact of PCI on accident rates more comprehensively, I am implementing the following strategies:\n\nIncorporating Additional Variables: After cleaning and integrating the data, I am including additional potential influencing factors such as traffic volume, weather, and speed limits. By using multivariable regression analysis, I aim to quantify the combined effects of these variables on accident rates.\nNonlinear Relationship Investigation: I will include quadratic or higher-order terms for PCI in the regression model to capture possible nonlinear patterns and improve the model’s fit, further validating the hypothesis.\nExploring Discrete Data Models: Given that accident rates are discrete count data, I may employ Poisson regression or negative binomial regression models for a more accurate representation of accident count distributions.\n\n\n\n\nCurrently, I am focusing on the following tasks:\n\nData Cleaning and Integration:\nI have standardized field names for consistency, resolving many-to-many relationships between road names across datasets, ensuring alignment of fields between different datasets.\nExploratory Statistical Analysis:\nI am grouping the data by key fields to calculate averages and accident counts, providing a foundation for subsequent modeling.\nRegression Modeling:\nI am gradually incorporating additional variables, such as weather and speed limits, to analyze their significant impacts on accident rates beyond PCI.\nVisualization of Relationships:\nI am creating scatter plots with trend lines to visually represent the relationships between variables, aiding in the interpretation and validation of analysis results."
  },
  {
    "objectID": "posts/2024-11-18-blog-post/blog-06.html#crime-rate-analysis",
    "href": "posts/2024-11-18-blog-post/blog-06.html#crime-rate-analysis",
    "title": "Blog Post 6 - Data Integration and Modeling",
    "section": "Crime Rate Analysis",
    "text": "Crime Rate Analysis\n\nHypothesis Test\n\nH_0: There is no relationship between between daily crime count and daily crash reporting.\nH_a: Days with higher crime counts mean higher crash reporting.\n\n\n\nSetting up Data for Analysis\nIn order to do this I filtered the original crime data from DATA.GOV to show crime reporting for only Montgomery Village within Montgomery County. I then chose to focus on the crime’s start time in order to analyze these crime counts as they began. Next, I adjusted the start date column into a proper date-time format in order to pull the date and mutate a column onto the filtered crime data set. Once doing this I was then able to create a new data frame that counted crimes that occurred on a specific day. I then joined these two frames into a data set that showed all days that had been reported whether crime or crash, and counted each.\n\n\nAnalyzing a Possible Relationship\nTo see if there is anything to this hypothesis or if we don’t have a strong enough claim I used a couple of different plots. First I began with a scatterplot that included the line of best fit with method “lm.” This is shown below and represents the general relationship between the two variables.\n\n\n\nRelationship Between Daily Crime and Crash Counts\n\n\nWe can see that the line is nearly horizontal, but there is a slight increase, meaning a possible positive correlation between our variables. Given that we can find the correlation to be about 0.0326, we can see there’s nearly no significant linear relationship between our variables.\nNext, before we know what test to run for our hypothesis I wanted to check for homoscedasticity and normality. This required me to look at the NQ-Q and Residual Plots. The NQ-Q plot helped to show the quantiles graphed against the standardized quantiles, which helps to show a bit of skewness at the tails of our distribution. Additionally, looking at our Residual plot shows there to be a few outliers, but generally, we can summarize as the fitted values increase our variance decreases. Overall both of these plots are interesting but not necessarily valuable because we can’t see any huge significance in either. Additionally, I wouldn’t assume normality or homoscedasticity which eliminates many testing options. The best case would be to use a Monte Carlo simulation for further exploration.\n\n\n\nQ-Q Residuals and Residuals vs Fitted\n\n\nLastly, we can look at a density graph that shows the different distributions of crashes on “High” versus “Low” crime count days. These thresholds are defined by whether the crime count for one day is above or below the median of total crime counts.\n\n\n\nDensity Plot of Crash Counts by Crime Level\n\n\nThere seems to be a significant overlap between the distributions which means the range for crash reports is about the same given higher or lower crime counts. However, there is a slight shift to the right for the “High” distribution which agrees with our previous scatterplot.\n\n\nPlan for further analysis\nGiven that these variables don’t appear to be largely related; I would look to another data set that could provide further enhancement to our crash reporting or I could focus on this case where we maybe don’t see a strong relationship between crime in this county and vehicle crashes. This could be interesting to further analyze and one may assume they would have a significant relationship.\nIf I were to continue with this analysis, I would first figure out what test I think would be the best to run and see if there’s any significance between our variables or if we fail to reject our null."
  },
  {
    "objectID": "posts/2024-11-06-blog-post/blog-04.html",
    "href": "posts/2024-11-06-blog-post/blog-04.html",
    "title": "Blog Post 4 - EDA and Modeling Section 1",
    "section": "",
    "text": "The road with the highest number of crashes is unnamed, with just under 125 crashes recorded. The road with the second-highest number of crashes is Georgia Avenue, which has significantly fewer crashes, totaling just under 50\n\n# The top 10 roads with the highest number of crashes \nlibrary(dplyr)\nlibrary(ggplot2)\n\ncleaned_dataset &lt;- read.csv(\"dataset/cleaned_dataset.rds\")\n\nroad_crash_data &lt;- cleaned_dataset %&gt;%\n  filter(!is.na(`Road.Name`)) %&gt;%\n  count(`Road.Name`, sort = TRUE) %&gt;%\n  top_n(10, n)\n\nggplot(road_crash_data, aes(x = reorder(`Road.Name`, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +\n  labs(title = \"Top 10 Roads with the Most Crashes\",x = \"Road Name\", y = \"Number of Crashes\") +\n  theme_minimal()\n\n\n# Distribution of Simplified Surface conditions by collision type\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\nsimplified_barplot_data &lt;- cleaned_dataset %&gt;%\n  mutate(Simplified_Surface = case_when(\n    Surface.Condition %in% c(\"Dry\", \"DRY\") ~ \"Dry\",\n    Surface.Condition %in% c(\"Wet\", \"WET\", \"Water (standing, moving)\", \"WATER(STANDING/MOVING)\") ~ \"Wet\",\n    Surface.Condition %in% c(\"Ice\", \"ICE\", \"Ice/Frost\", \"Snow\", \"SNOW\", \"Slush\", \"SLUSH\") ~ \"Snow/Ice\",\n    Surface.Condition %in% c(\"Mud, Dirt, Gravel\", \"MUD, DIRT, GRAVEL\") ~ \"Mud/Dirt\",\n    Surface.Condition %in% c(\"Sand\", \"SAND\", \"OIL\", \"Other\", \"OTHER\") ~ \"Other\",\n    TRUE ~ \"Unknown\"\n  )) %&gt;%\n  count(Collision.Type, Simplified_Surface) %&gt;%\n  group_by(Collision.Type) %&gt;%\n  mutate(percentage = n / sum(n))\n\nggplot(simplified_barplot_data, aes(x = Collision.Type, y = percentage, fill = Simplified_Surface)) +\n  geom_bar(stat = \"identity\", position = \"fill\") +\n  scale_y_continuous(labels = scales::percent) +\n  labs(title = \"Distribution of Simplified Surface Conditions by Collision Type\",\n       x = \"Collision Type\", y = \"Percentage\",\n       fill = \"Surface Condition\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nThere is no definitive way to predict the time of year crashes will occur, but the trend shows a notable spike in crashes from January to March in 2015 and 2017. Additionally, there is often an increase in crashes in the month of November.\n\n# Monthly trend of Crash COunts by Year\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(lubridate)\n\n\ncrash_data &lt;- cleaned_dataset %&gt;%\n  mutate(\n    Crash_Date = ymd_hms(Crash.Date.Time), \n    Year = year(Crash_Date),\n    Month = month(Crash_Date, label = TRUE)\n  ) %&gt;%\n  filter(!is.na(Crash_Date))  \n\nmonthly_data &lt;- crash_data %&gt;%\n  group_by(Year, Month) %&gt;%\n  summarise(Crash_Count = n(), .groups = 'drop')\n\nggplot(monthly_data, aes(x = Month, y = Crash_Count, group = Year)) +\n  geom_line() +\n  labs(\n    title = \"Monthly Trend of Crash Counts by Year\",\n    x = \"Month\", y = \"Number of Crashes\"\n  ) +\n  facet_wrap(~ Year, scales = \"free_y\") +\n  theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nOut of the top 10 roads with the most crashes the surface conditions are mostly dry.\n\n# Find the top 10 roads with the most crashes\ntop_road_crash_data &lt;- cleaned_dataset %&gt;%\n  filter(!is.na(`Road.Name`)) %&gt;%\n  count(`Road.Name`, sort = TRUE) %&gt;%\n  top_n(10, n)\n\n# Filter the cleaned dataset to only include these top 10 roads\ntop_road_conditions &lt;- cleaned_dataset %&gt;%\n  filter(`Road.Name` %in% top_road_crash_data$`Road.Name`) %&gt;%\n  mutate(Simplified_Surface = case_when(\n    Surface.Condition %in% c(\"Dry\", \"DRY\") ~ \"Dry\",\n    Surface.Condition %in% c(\"Wet\", \"WET\", \"Water (standing, moving)\", \"WATER(STANDING/MOVING)\") ~ \"Wet\",\n    Surface.Condition %in% c(\"Ice\", \"ICE\", \"Ice/Frost\", \"Snow\", \"SNOW\", \"Slush\", \"SLUSH\") ~ \"Snow/Ice\",\n    Surface.Condition %in% c(\"Mud, Dirt, Gravel\", \"MUD, DIRT, GRAVEL\") ~ \"Mud/Dirt\",\n    Surface.Condition %in% c(\"Sand\", \"SAND\", \"OIL\", \"Other\", \"OTHER\") ~ \"Other\",\n    TRUE ~ \"Unknown\"\n  )) %&gt;%\n  count(`Road.Name`, Simplified_Surface) %&gt;%\n  group_by(`Road.Name`) %&gt;%\n  mutate(percentage = n / sum(n))\n\n# Plot the distribution of surface conditions for the top 10 roads\nggplot(top_road_conditions, aes(x = reorder(`Road.Name`, -n), y = percentage, fill = Simplified_Surface)) +\n  geom_bar(stat = \"identity\", position = \"fill\") +\n  scale_y_continuous(labels = scales::percent) +\n  coord_flip() +\n  labs(\n    title = \"Surface Conditions for the Top 10 Roads with the Most Crashes\",\n    x = \"Road Name\", y = \"Percentage of Crashes\",\n    fill = \"Surface Condition\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "posts/2024-10-28-blog-post/blog-03.html",
    "href": "posts/2024-10-28-blog-post/blog-03.html",
    "title": "Blog Posts 3 - Data Cleaning, Loading, and Equity Considerations",
    "section": "",
    "text": "In our recent data cleaning work, we started by removing columns that had a high percentage of missing values, specifically those with over 80% missing data, to ensure the reliability of our analysis. We then transformed the format of columns “Local Case Number” to a character format to preserve specific formatting details, and the “Crash Date and Time” to a datetime format for better temporal analysis.\nAdditionally, we addressed missing entries in the “Route Type” column by filling them with the most frequently occurring value, ensuring consistency in our categorical data. We also removed all duplicate entries to maintain the uniqueness of the dataset."
  },
  {
    "objectID": "posts/2024-10-28-blog-post/blog-03.html#data-cleaning",
    "href": "posts/2024-10-28-blog-post/blog-03.html#data-cleaning",
    "title": "Blog Posts 3 - Data Cleaning, Loading, and Equity Considerations",
    "section": "",
    "text": "In our recent data cleaning work, we started by removing columns that had a high percentage of missing values, specifically those with over 80% missing data, to ensure the reliability of our analysis. We then transformed the format of columns “Local Case Number” to a character format to preserve specific formatting details, and the “Crash Date and Time” to a datetime format for better temporal analysis.\nAdditionally, we addressed missing entries in the “Route Type” column by filling them with the most frequently occurring value, ensuring consistency in our categorical data. We also removed all duplicate entries to maintain the uniqueness of the dataset."
  },
  {
    "objectID": "posts/2024-10-28-blog-post/blog-03.html#data-equity",
    "href": "posts/2024-10-28-blog-post/blog-03.html#data-equity",
    "title": "Blog Posts 3 - Data Cleaning, Loading, and Equity Considerations",
    "section": "Data Equity",
    "text": "Data Equity\nWe can utilize the principles for advancing equitable data practices by applying the principles of beneficence, respect for persons, and justice to the exploration of our data set.\nThese principles are relevant for a few reasons. First, looking at beneficence, we may consider how the findings in our data set are being used by local or federal governments, as well as how we are using the data. We can see from this principle what the best actions for the county to take in response to traffic violations would be. Additionally adhering to this practice for our group’s project may look like researching into how the government is using this dataset.\nFurthermore, we can look at the principle of justice. This is relevant to our data as we can see the members of this sample that are facing violations or get into accidents. We can see if they are more at risk than others, and provide some insight through our findings to see if there is any injustice within this county of Maryland response, whether through law or bill.\nSome limitations of our analysis could include missing data, variables not provided or included in the dataset, and a lack of available information online to evaluate the local or federal government’s response (or lack thereof) to this data. Potential misuse of the data could involve government surveillance in high-risk areas or the implementation of strict law enforcement measures. It could also manifest as policy changes made without community input, which might, in turn, reinforce existing community inequities."
  },
  {
    "objectID": "posts/2024-10-11-blog-post/blog-01.html",
    "href": "posts/2024-10-11-blog-post/blog-01.html",
    "title": "Blog Post 1 - Choosing a Dataset",
    "section": "",
    "text": "Link: https://catalog.data.gov/dataset/national-student-loan-data-system-722b0\nThe National Student Loan data contains information about loans and grants awarded to students under Title IV of the Higher Education Act of 1965, which refers to federal financial aid programs for post secondary education. The data includes various loan categories like Direct Loans, Federal Family Education Loans (FFEL), and Perkins Loans. It spans the entire life cycle of these loans, from disbursement to closure. Most files contain between 10 to 25 columns, with key information such as loan types, disbursement amounts, repayment plans, deferment and forbearance details, and loan statuses. These columns typically include financial metrics like “Dollars Outstanding”, “Recipients”, and “Number of Loans”. The files also vary in terms of row counts; some are relatively small, like summary files with a few hundred rows, while detailed loan or quarterly activity reports contain thousands of rows (ranging from 3798 to 4057 rows). Some files also have additional formatting and metadata rows.\nThis data was originally collected to centralize and monitor federal student aid, helping policymakers, educational institutions, and borrowers manage financial aid efficiently. Main challenges might include cleaning and consolidating disparate files and handling large amounts of historical data, especially if some files are incomplete or inconsistent in format. Some key questions to consider might include loan default rates over time, trends in deferment or forbearance, and the impact of repayment plans on loan outcomes. Additionally, we can explore how loan activity varies across schools and regions, using the quarterly reports to identify trends in loan disbursements and borrower demographics. Another key question would be how the Public Service Loan Forgiveness and Teacher Loan Forgiveness programs are affecting long-term repayment patterns and loan forgiveness rate across various borrower groups."
  },
  {
    "objectID": "posts/2024-10-11-blog-post/blog-01.html#national-student-loan-data-system",
    "href": "posts/2024-10-11-blog-post/blog-01.html#national-student-loan-data-system",
    "title": "Blog Post 1 - Choosing a Dataset",
    "section": "",
    "text": "Link: https://catalog.data.gov/dataset/national-student-loan-data-system-722b0\nThe National Student Loan data contains information about loans and grants awarded to students under Title IV of the Higher Education Act of 1965, which refers to federal financial aid programs for post secondary education. The data includes various loan categories like Direct Loans, Federal Family Education Loans (FFEL), and Perkins Loans. It spans the entire life cycle of these loans, from disbursement to closure. Most files contain between 10 to 25 columns, with key information such as loan types, disbursement amounts, repayment plans, deferment and forbearance details, and loan statuses. These columns typically include financial metrics like “Dollars Outstanding”, “Recipients”, and “Number of Loans”. The files also vary in terms of row counts; some are relatively small, like summary files with a few hundred rows, while detailed loan or quarterly activity reports contain thousands of rows (ranging from 3798 to 4057 rows). Some files also have additional formatting and metadata rows.\nThis data was originally collected to centralize and monitor federal student aid, helping policymakers, educational institutions, and borrowers manage financial aid efficiently. Main challenges might include cleaning and consolidating disparate files and handling large amounts of historical data, especially if some files are incomplete or inconsistent in format. Some key questions to consider might include loan default rates over time, trends in deferment or forbearance, and the impact of repayment plans on loan outcomes. Additionally, we can explore how loan activity varies across schools and regions, using the quarterly reports to identify trends in loan disbursements and borrower demographics. Another key question would be how the Public Service Loan Forgiveness and Teacher Loan Forgiveness programs are affecting long-term repayment patterns and loan forgiveness rate across various borrower groups."
  },
  {
    "objectID": "posts/2024-10-11-blog-post/blog-01.html#crash-reporting-drivers-data",
    "href": "posts/2024-10-11-blog-post/blog-01.html#crash-reporting-drivers-data",
    "title": "Blog Post 1 - Choosing a Dataset",
    "section": "Crash Reporting-Drivers Data",
    "text": "Crash Reporting-Drivers Data\nLink: https://catalog.data.gov/dataset/crash-reporting-drivers-data\nThis dataset includes information on vehicles that have been involved in collisions on public and local roads within Montgomery County, Maryland. It includes 39 columns and 186,170 rows or entries, and we are able to clean the data as well as load it into the environment. It was collected by an Automated Reporting System (ARS) through the Police department and showed each collision recorded as well as the drivers involved. This data may include verified as well as unverified data and is non-federal.\nThe main questions we hope to address with this data set include data on road safety, driver behaviors, as well as accident patterns within Montgomery County. We could use response and predictor variables such as location, vehicle model, collision type, and time of day (light) to name a few. Some challenges we could face with this data are the number of entries it holds, as well as many NA entries that were unable to be filled by the ARS. This means cleaning this data could take much longer than anticipated. Additionally, it doesn’t provide variables such as Age or Race which could be helpful indicators to see patterns in possible criminal injustice. Overall this data set is very interesting but it may not be the best of the three for analyzing as well as finding prejudices."
  },
  {
    "objectID": "posts/2024-10-11-blog-post/blog-01.html#crime-data-from-2020-to-present",
    "href": "posts/2024-10-11-blog-post/blog-01.html#crime-data-from-2020-to-present",
    "title": "Blog Post 1 - Choosing a Dataset",
    "section": "Crime Data from 2020 to Present",
    "text": "Crime Data from 2020 to Present\nLink: https://catalog.data.gov/dataset/crime-data-from-2020-to-present\nThis dataset contains records of crime incidents in Los Angeles, California, from 2020 until 2024. The dataset was digitized from paper reports originally maintained by the LA Police Department and later transferred to an electronic format. It includes 22 columns and over 200,000 rows of data. Due to digitizing, there may be some discrepancies, such as missing or incomplete data, particularly in location fields. For privacy reasons, locations are only recorded up to the hundredth block rather than specific addresses.\nThe dataset contains over 200,000 rows and 22 columns, offering detailed information about various incidents, including crime type, date, time, location, and the involved parties. The Los Angeles Police Department originally collected this data as part of routine crime reporting. Initially maintained as paper records, it was later digitized and entered into an electronic database. This data collection aims to track crime incidents across the city for law enforcement purposes, urban planning, and policy analysis. It is also used to inform the public and local authorities about crime trends and patterns in Los Angeles.\nThe data can be loaded and cleaned, but its large size and missing entries from the transition from paper to digital format may make cleaning time-consuming. Special attention will be needed for missing values, especially in location and categorical fields, and date/time reformatting may be required. Key questions include identifying crime trends from 2020 to 2024, examining neighborhoods with higher crime rates, and finding correlations between crime types and factors like time of day or proximity to locations. Challenges include handling over 200,000 rows, missing location data, and limited precision due to privacy restrictions on location details. Ensuring data consistency after the transition is another potential issue."
  },
  {
    "objectID": "dataset/Cleaning_Xiang.html",
    "href": "dataset/Cleaning_Xiang.html",
    "title": "crash car data clean",
    "section": "",
    "text": "options(repos = c(CRAN = \"https://cran.r-project.org\"))\ninstall.packages(\"skimr\")\n\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(lubridate)\nlibrary(skimr)  # for quick summary statistics\n\n\ninstall.packages(\"RCurl\")\nlibrary(RCurl)\nx &lt;- getURL(\"https://raw.githubusercontent.com/sussmanbu/ma-4615-fa24-final-project-group-1/main/Crash_Reporting_Drivers_Data.csv\")\ndrivers_data &lt;- read.csv(text = x)\n\n\nglimpse(drivers_data)\n\nskim(drivers_data)\n\nClean Column Names\n\ndrivers_data_clean &lt;- drivers_data %&gt;%\n  clean_names()\n\ncolnames(drivers_data_clean)\n\nHandle Date/Time Format\n\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    crash_date/time &lt;- mdy_hms(crash_date/time),\n    # create separate date and time columns if needed\n    crash_date = date(crash_date/time),\n    crash_time = format(crash_date/time, \"%H:%M:%S\")\n  )\n\nStandardize Categorical Variables\n\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # standardize agency names\n    agency_name = str_trim(agency_name),\n    \n    # clean and standardize route type\n    route_type = case_when(\n      is.na(route_type) ~ \"Unknown\",\n      TRUE ~ route_type\n    ),\n    \n    # standardize weather conditions\n    weather = str_to_title(weather),\n    \n    # clean surface condition\n    surface_condition = case_when(\n      is.na(surface_condition) ~ \"Unknown\",\n      TRUE ~ surface_condition\n    ),\n    \n    # standardize driver at fault\n    driver_at_fault = case_when(\n      str_to_lower(driver_at_fault) == \"yes\" ~ \"Yes\",\n      str_to_lower(driver_at_fault) == \"no\" ~ \"No\",\n      TRUE ~ \"Unknown\"\n    )\n  )\n\nHandle Missing Values\n\n# calculate missing values percentage\nmissing_values &lt;- drivers_data_clean %&gt;%\n  summarise(across(everything(), ~sum(is.na(.))/n()*100)) %&gt;%\n  pivot_longer(everything(), \n              names_to = \"column\", \n              values_to = \"missing_percentage\") %&gt;%\n  arrange(desc(missing_percentage))\n\n# display columns with missing values\nprint(missing_values %&gt;% filter(missing_percentage &gt; 0))\n\n# handle missing values based on column type\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # for route_type, road_name, and cross_street_name\n    # keep NA as is since they might be meaningful (e.g., off-road incidents)\n    \n    # for numeric columns, consider if 0 or NA is more appropriate\n    speed_limit = if_else(is.na(speed_limit), 0, speed_limit),\n    \n    # for categorical columns, mark unknown\n    surface_condition = if_else(is.na(surface_condition), \"Unknown\", surface_condition),\n    traffic_control = if_else(is.na(traffic_control), \"Unknown\", traffic_control)\n  )\n\nData Validation and Consistency Checks\n\n# check for logical consistencies\nvalidation_results &lt;- drivers_data_clean %&gt;%\n  summarise(\n    # check for future dates\n    future_dates = sum(crash_date &gt; Sys.Date()),\n    \n    # check for valid speed limits\n    invalid_speed = sum(speed_limit &gt; 70 | speed_limit &lt; 0, na.rm = TRUE),\n    \n    # check for valid vehicle years\n    invalid_vehicle_year = sum(vehicle_year &lt; 1900 | vehicle_year &gt; year(Sys.Date()) + 1, na.rm = TRUE),\n    \n    # check for valid coordinates\n    invalid_lat = sum(latitude &lt; 38 | latitude &gt; 40, na.rm = TRUE),\n    invalid_lon = sum(longitude &gt; -76 | longitude &lt; -78, na.rm = TRUE)\n  )\n\nprint(validation_results)\n\n# create flags for potential data quality issues\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    data_quality_flag = case_when(\n      crash_date &gt; Sys.Date() ~ \"Future date\",\n      speed_limit &gt; 70 | speed_limit &lt; 0 ~ \"Invalid speed limit\",\n      vehicle_year &lt; 1900 | vehicle_year &gt; year(Sys.Date()) + 1 ~ \"Invalid vehicle year\",\n      latitude &lt; 38 | latitude &gt; 40 | longitude &gt; -76 | longitude &lt; -78 ~ \"Invalid coordinates\",\n      TRUE ~ \"Valid\"\n    )\n  )\n\nCheck Value Distributions\n\n# check common categories\ncategory_summaries &lt;- list(\n  collision_types = table(drivers_data_clean$collision_type),\n  weather_conditions = table(drivers_data_clean$weather),\n  vehicle_types = table(drivers_data_clean$vehicle_body_type),\n  injury_severity = table(drivers_data_clean$injury_severity)\n)\n\nprint(category_summaries)\n\nCollision Types Need Standardization:\n\n# standardize collision types\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    collision_type = case_when(\n      collision_type %in% c(\"Angle\", \"ANGLE MEETS LEFT HEAD ON\", \"ANGLE MEETS LEFT TURN\",\n                           \"ANGLE MEETS RIGHT TURN\", \"STRAIGHT MOVEMENT ANGLE\") ~ \"ANGLE\",\n      collision_type %in% c(\"Front to Front\", \"HEAD ON\", \"HEAD ON LEFT TURN\") ~ \"HEAD_ON\",\n      collision_type %in% c(\"Front to Rear\", \"SAME DIR REAR END\", \n                           \"SAME DIR REND LEFT TURN\", \"SAME DIR REND RIGHT TURN\") ~ \"REAR_END\",\n      collision_type %in% c(\"SAME DIRECTION SIDESWIPE\", \"Sideswipe, Same Direction\") ~ \"SIDESWIPE_SAME_DIR\",\n      collision_type %in% c(\"OPPOSITE DIRECTION SIDESWIPE\", \"Sideswipe, Opposite Direction\") ~ \"SIDESWIPE_OPPOSITE_DIR\",\n      collision_type %in% c(\"SINGLE VEHICLE\", \"Single Vehicle\") ~ \"SINGLE_VEHICLE\",\n      collision_type %in% c(\"Other\", \"OTHER\") ~ \"OTHER\",\n      collision_type %in% c(\"Unknown\", \"UNKNOWN\", \"N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\nWeather Conditions Need Standardization:\n\n# standardize weather conditions\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    weather = case_when(\n      str_detect(str_to_upper(weather), \"CLEAR\") ~ \"CLEAR\",\n      str_detect(str_to_upper(weather), \"CLOUD\") ~ \"CLOUDY\",\n      str_detect(str_to_upper(weather), \"RAIN|RAINING\") ~ \"RAIN\",\n      str_detect(str_to_upper(weather), \"SNOW|BLOWING SNOW\") ~ \"SNOW\",\n      str_detect(str_to_upper(weather), \"FOG|SMOG|SMOKE\") ~ \"FOG\",\n      weather %in% c(\"N/A\", \"Unknown\", \"UNKNOWN\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\nVehicle Types Need Major Cleanup:\n\n# standardize vehicle types\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    vehicle_body_type = case_when(\n      str_detect(str_to_upper(vehicle_body_type), \"PASSENGER CAR|PASSENGER|CAR\") ~ \"PASSENGER_CAR\",\n      str_detect(str_to_upper(vehicle_body_type), \"SUV|SPORT UTILITY|UTILITY VEHICLE\") ~ \"SUV\",\n      str_detect(str_to_upper(vehicle_body_type), \"PICKUP|LIGHT TRUCK\") ~ \"PICKUP_TRUCK\",\n      str_detect(str_to_upper(vehicle_body_type), \"VAN|CARGO\") ~ \"VAN\",\n      str_detect(str_to_upper(vehicle_body_type), \"BUS\") ~ \"BUS\",\n      str_detect(str_to_upper(vehicle_body_type), \"MOTORCYCLE|MOPED\") ~ \"MOTORCYCLE\",\n      str_detect(str_to_upper(vehicle_body_type), \"EMERGENCY|POLICE|FIRE|AMBULANCE\") ~ \"EMERGENCY_VEHICLE\",\n      vehicle_body_type %in% c(\"N/A\", \"Unknown\", \"UNKNOWN\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\nInjury Severity Standardization:\n\n# standardize injury severity\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    injury_severity = case_when(\n      str_detect(str_to_upper(injury_severity), \"FATAL\") ~ \"FATAL\",\n      str_detect(str_to_upper(injury_severity), \"NO APPARENT|NO INJURY\") ~ \"NO_INJURY\",\n      str_detect(str_to_upper(injury_severity), \"POSSIBLE\") ~ \"POSSIBLE_INJURY\",\n      str_detect(str_to_upper(injury_severity), \"MINOR\") ~ \"MINOR_INJURY\",\n      str_detect(str_to_upper(injury_severity), \"SERIOUS\") ~ \"SERIOUS_INJURY\",\n      TRUE ~ \"UNKNOWN\"\n    )\n  )\n\nHandle Missing Values Strategy (based on the missing percentage analysis):\n\n# handle missing values based on context\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # high missing percentage (&gt;90%) - keep as NA but add flag\n    has_non_motorist = !is.na(related_non_motorist),\n    \n    # medium missing percentage (10-90%) - add meaningful categories\n    municipality = if_else(is.na(municipality), \"UNINCORPORATED\", municipality),\n    road_name = if_else(is.na(road_name), \"OFF_ROAD\", road_name),\n    cross_street_name = if_else(is.na(cross_street_name), \"NOT_APPLICABLE\", cross_street_name),\n    \n    # low missing percentage (&lt;10%) - impute with \"UNKNOWN\"\n    drivers_license_state = if_else(is.na(drivers_license_state), \"UNKNOWN\", drivers_license_state),\n    circumstance = if_else(is.na(circumstance), \"UNKNOWN\", circumstance),\n    vehicle_going_dir = if_else(is.na(vehicle_going_dir), \"UNKNOWN\", vehicle_going_dir)\n  )\n\nHandle Data Quality Issues (based on validation results):\n\n# add data quality flags\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    data_quality_flags = case_when(\n      speed_limit &gt; 70 | speed_limit &lt; 0 ~ \"INVALID_SPEED_LIMIT\",\n      vehicle_year &lt; 1900 | vehicle_year &gt; year(Sys.Date()) + 1 ~ \"INVALID_VEHICLE_YEAR\",\n      latitude &lt; 38 | latitude &gt; 40 | longitude &gt; -76 | longitude &lt; -78 ~ \"INVALID_COORDINATES\",\n      TRUE ~ \"VALID\"\n    ),\n    \n    # clean speed limits\n    speed_limit = case_when(\n      speed_limit &gt; 70 ~ NA_real_,\n      speed_limit &lt; 0 ~ NA_real_,\n      TRUE ~ speed_limit\n    ),\n    \n    # clean vehicle years\n    vehicle_year = if_else(\n      vehicle_year &lt; 1900 | vehicle_year &gt; year(Sys.Date()) + 1,\n      NA_real_,\n      vehicle_year\n    )\n  )\n\nAdditional cleaning for specific columns:\n\n# additional cleaning steps for specific columns and vehicle data\n\n# standardize vehicle makes (common misspellings and abbreviations)\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    vehicle_make = case_when(\n      # honda variations\n      str_detect(str_to_upper(vehicle_make), \"HOND|HDA\") ~ \"HONDA\",\n      \n      # toyota variations\n      str_detect(str_to_upper(vehicle_make), \"TOY|TOYT\") ~ \"TOYOTA\",\n      \n      # ford\n      str_detect(str_to_upper(vehicle_make), \"^FRD|FORD\") ~ \"FORD\",\n      \n      # chevrolet variations\n      str_detect(str_to_upper(vehicle_make), \"CHEV|CHEVY|CHV\") ~ \"CHEVROLET\",\n      \n      # nissan variations\n      str_detect(str_to_upper(vehicle_make), \"NISS|NISN\") ~ \"NISSAN\",\n      \n      # hyundai variations\n      str_detect(str_to_upper(vehicle_make), \"HYUN|HYU\") ~ \"HYUNDAI\",\n      \n      # volkswagen variations\n      str_detect(str_to_upper(vehicle_make), \"VW|VOLK|VOLKS\") ~ \"VOLKSWAGEN\",\n      \n      # BMW\n      str_detect(str_to_upper(vehicle_make), \"BMW|BMV\") ~ \"BMW\",\n      \n      # Mercedes-Benz variations\n      str_detect(str_to_upper(vehicle_make), \"MERZ|MENZ|MERCEDES|BENZ\") ~ \"MERCEDES-BENZ\",\n      \n      # lexus\n      str_detect(str_to_upper(vehicle_make), \"LEX|LEXS\") ~ \"LEXUS\",\n      \n      # mazda variations\n      str_detect(str_to_upper(vehicle_make), \"MAZ|MAZD\") ~ \"MAZDA\",\n      \n      # subaru variations\n      str_detect(str_to_upper(vehicle_make), \"SUB|SUBR\") ~ \"SUBARU\",\n      \n      # kia\n      str_detect(str_to_upper(vehicle_make), \"^KIA\") ~ \"KIA\",\n      \n      # audi\n      str_detect(str_to_upper(vehicle_make), \"^AUD\") ~ \"AUDI\",\n      \n      # acura\n      str_detect(str_to_upper(vehicle_make), \"ACUR|ACU\") ~ \"ACURA\",\n      \n      # infinity\n      str_detect(str_to_upper(vehicle_make), \"INF|INFIN\") ~ \"INFINITI\",\n      \n      TRUE ~ str_to_upper(vehicle_make)\n    )\n  )\n\n# clean vehicle models\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    vehicle_model = case_when(\n      # clean common abbreviations\n      str_detect(str_to_upper(vehicle_model), \"^CRV|CR-V\") ~ \"CR-V\",\n      str_detect(str_to_upper(vehicle_model), \"^RAV|RAV4\") ~ \"RAV4\",\n      str_detect(str_to_upper(vehicle_model), \"ACCORD|ACRD\") ~ \"ACCORD\",\n      str_detect(str_to_upper(vehicle_model), \"CAMRY|CAM\") ~ \"CAMRY\",\n      str_detect(str_to_upper(vehicle_model), \"CIVIC|CVC\") ~ \"CIVIC\",\n      str_detect(str_to_upper(vehicle_model), \"ALTIMA|ALT\") ~ \"ALTIMA\",\n      str_detect(str_to_upper(vehicle_model), \"COROLLA|COR\") ~ \"COROLLA\",\n      str_detect(str_to_upper(vehicle_model), \"EXPLORER|EXPLR\") ~ \"EXPLORER\",\n      str_detect(str_to_upper(vehicle_model), \"F-150|F150\") ~ \"F-150\",\n      str_detect(str_to_upper(vehicle_model), \"HIGHLANDER|HGLDR\") ~ \"HIGHLANDER\",\n      TRUE ~ str_to_upper(vehicle_model)\n    )\n  )\n\n# additional cleaning for other specific columns\n\n# clean light conditions\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    light = case_when(\n      str_detect(str_to_upper(light), \"DAYLIGHT\") ~ \"DAYLIGHT\",\n      str_detect(str_to_upper(light), \"DARK.*LIGHT.*ON|LIGHTED\") ~ \"DARK_WITH_LIGHTING\",\n      str_detect(str_to_upper(light), \"DARK.*NO.*LIGHT|UNLIGHTED\") ~ \"DARK_NO_LIGHTING\",\n      str_detect(str_to_upper(light), \"DAWN\") ~ \"DAWN\",\n      str_detect(str_to_upper(light), \"DUSK\") ~ \"DUSK\",\n      str_detect(str_to_upper(light), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# clean surface conditions\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    surface_condition = case_when(\n      str_detect(str_to_upper(surface_condition), \"DRY\") ~ \"DRY\",\n      str_detect(str_to_upper(surface_condition), \"WET\") ~ \"WET\",\n      str_detect(str_to_upper(surface_condition), \"ICE|ICY\") ~ \"ICE\",\n      str_detect(str_to_upper(surface_condition), \"SNOW|SLUSH\") ~ \"SNOW\",\n      str_detect(str_to_upper(surface_condition), \"SAND|DIRT|MUD\") ~ \"SAND_DIRT_MUD\",\n      str_detect(str_to_upper(surface_condition), \"OIL|GREASE\") ~ \"OIL_GREASE\",\n      is.na(surface_condition) | str_detect(str_to_upper(surface_condition), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# clean traffic control\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    traffic_control = case_when(\n      str_detect(str_to_upper(traffic_control), \"SIGNAL|TRAFFIC LIGHT\") ~ \"TRAFFIC_SIGNAL\",\n      str_detect(str_to_upper(traffic_control), \"STOP SIGN\") ~ \"STOP_SIGN\",\n      str_detect(str_to_upper(traffic_control), \"YIELD\") ~ \"YIELD_SIGN\",\n      str_detect(str_to_upper(traffic_control), \"FLASHING\") ~ \"FLASHING_SIGNAL\",\n      str_detect(str_to_upper(traffic_control), \"NO CONTROL|NONE\") ~ \"NO_CONTROL\",\n      str_detect(str_to_upper(traffic_control), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# clean driver substance abuse\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    driver_substance_abuse = case_when(\n      str_detect(str_to_upper(driver_substance_abuse), \"NONE|NO ABUSE\") ~ \"NONE\",\n      str_detect(str_to_upper(driver_substance_abuse), \"ALCOHOL\") ~ \"ALCOHOL\",\n      str_detect(str_to_upper(driver_substance_abuse), \"DRUG\") ~ \"DRUGS\",\n      str_detect(str_to_upper(driver_substance_abuse), \"BOTH|ALCOHOL.*DRUG|DRUG.*ALCOHOL\") ~ \"ALCOHOL_AND_DRUGS\",\n      str_detect(str_to_upper(driver_substance_abuse), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# clean driver distracted by\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    driver_distracted_by = case_when(\n      str_detect(str_to_upper(driver_distracted_by), \"NOT DISTRACTED\") ~ \"NOT_DISTRACTED\",\n      str_detect(str_to_upper(driver_distracted_by), \"CELL|PHONE|MOBILE\") ~ \"CELL_PHONE\",\n      str_detect(str_to_upper(driver_distracted_by), \"PASSENGER\") ~ \"PASSENGER\",\n      str_detect(str_to_upper(driver_distracted_by), \"RADIO|AUDIO\") ~ \"AUDIO_EQUIPMENT\",\n      str_detect(str_to_upper(driver_distracted_by), \"EAT|DRINK\") ~ \"EATING_DRINKING\",\n      str_detect(str_to_upper(driver_distracted_by), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# verify our cleaning by checking the unique values in each cleaned column\nverification_check &lt;- function(data, columns) {\n  map(columns, ~{\n    unique_vals &lt;- data %&gt;% \n      pull(.) %&gt;% \n      unique() %&gt;% \n      sort()\n    \n    cat(\"\\nUnique values in\", ., \":\\n\")\n    print(unique_vals)\n    cat(\"\\n\")\n  })\n}\n\n# verify the cleaning results for key columns\ncolumns_to_verify &lt;- c(\n  \"vehicle_make\", \n  \"vehicle_model\",\n  \"light\",\n  \"surface_condition\",\n  \"traffic_control\",\n  \"driver_substance_abuse\",\n  \"driver_distracted_by\"\n)\n\nverification_check(drivers_data_clean, columns_to_verify)\n\nvehicle_summary &lt;- drivers_data_clean %&gt;%\n  group_by(vehicle_make, vehicle_model) %&gt;%\n  summarise(\n    count = n(),\n    .groups = 'drop'\n  ) %&gt;%\n  arrange(desc(count))\n\nprint(vehicle_summary)\n\n\n# clean and standardize agency names\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    agency_name = case_when(\n      str_detect(str_to_upper(agency_name), \"MONTGOMERY|MONT|MCP\") ~ \"MONTGOMERY COUNTY POLICE\",\n      str_detect(str_to_upper(agency_name), \"GAITHERSBURG|GAITH\") ~ \"GAITHERSBURG POLICE\",\n      str_detect(str_to_upper(agency_name), \"ROCKVILLE|ROCK\") ~ \"ROCKVILLE POLICE\",\n      str_detect(str_to_upper(agency_name), \"TAKOMA|TAK\") ~ \"TAKOMA PARK POLICE\",\n      str_detect(str_to_upper(agency_name), \"PARK|MNCPP\") ~ \"MD NATIONAL CAPITAL PARK POLICE\",\n      TRUE ~ str_to_upper(agency_name)\n    )\n  )\n\n# clean and standardize vehicle damage extent\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    vehicle_damage_extent = case_when(\n      str_detect(str_to_upper(vehicle_damage_extent), \"NONE|NO DAMAGE\") ~ \"NO_DAMAGE\",\n      str_detect(str_to_upper(vehicle_damage_extent), \"MINOR|SUPERFICIAL\") ~ \"MINOR\",\n      str_detect(str_to_upper(vehicle_damage_extent), \"FUNCTIONAL\") ~ \"FUNCTIONAL\",\n      str_detect(str_to_upper(vehicle_damage_extent), \"DISABLE|DISABLING\") ~ \"DISABLING\",\n      str_detect(str_to_upper(vehicle_damage_extent), \"DESTROYED\") ~ \"DESTROYED\",\n      str_detect(str_to_upper(vehicle_damage_extent), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# clean and standardize vehicle first impact location\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    vehicle_first_impact_location = case_when(\n      str_detect(str_to_upper(vehicle_first_impact_location), \"FRONT|TWELVE|12\") ~ \"FRONT\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"RIGHT FRONT|ONE|1\") ~ \"RIGHT_FRONT\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"RIGHT SIDE|THREE|3\") ~ \"RIGHT_SIDE\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"RIGHT REAR|FOUR|4\") ~ \"RIGHT_REAR\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"REAR|SIX|6\") ~ \"REAR\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"LEFT REAR|SEVEN|7|EIGHT|8\") ~ \"LEFT_REAR\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"LEFT SIDE|NINE|9\") ~ \"LEFT_SIDE\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"LEFT FRONT|TEN|10|ELEVEN|11\") ~ \"LEFT_FRONT\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# clean and standardize vehicle movement\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    vehicle_movement = case_when(\n      str_detect(str_to_upper(vehicle_movement), \"STRAIGHT|CONSTANT\") ~ \"STRAIGHT\",\n      str_detect(str_to_upper(vehicle_movement), \"TURN.*RIGHT\") ~ \"TURNING_RIGHT\",\n      str_detect(str_to_upper(vehicle_movement), \"TURN.*LEFT\") ~ \"TURNING_LEFT\",\n      str_detect(str_to_upper(vehicle_movement), \"STOP|SLOW\") ~ \"SLOWING_STOPPING\",\n      str_detect(str_to_upper(vehicle_movement), \"BACK|REVERSE\") ~ \"BACKING\",\n      str_detect(str_to_upper(vehicle_movement), \"PARK\") ~ \"PARKED\",\n      str_detect(str_to_upper(vehicle_movement), \"START\") ~ \"STARTING\",\n      str_detect(str_to_upper(vehicle_movement), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n\n# create a comprehensive validation function\nvalidate_crash_data &lt;- function(data) {\n  # initialize empty list for validation results\n  validation_results &lt;- list()\n  \n  # temporal validations\n  validation_results$temporal &lt;- data %&gt;%\n    summarise(\n      future_dates = sum(crash_date/time &gt; Sys.time()),\n      weekend_crashes = sum(lubridate::wday(crash_date/time, week_start = 1) %in% c(6,7)),\n      night_crashes = sum(format(crash_date/time, \"%H:%M:%S\") &gt;= \"18:00:00\" | \n                         format(crash_date/time, \"%H:%M:%S\") &lt;= \"06:00:00\"),\n      crashes_by_hour = table(format(crash_date/time, \"%H\"))\n    )\n  \n  # geographic validations\n  validation_results$geographic &lt;- data %&gt;%\n    summarise(\n      invalid_lat = sum(latitude &lt; 38 | latitude &gt; 40, na.rm = TRUE),\n      invalid_lon = sum(longitude &gt; -76 | longitude &lt; -78, na.rm = TRUE),\n      missing_coords = sum(is.na(latitude) | is.na(longitude)),\n      unique_locations = n_distinct(location, na.rm = TRUE)\n    )\n  \n  # vehicle validations\n  validation_results$vehicle &lt;- data %&gt;%\n    summarise(\n      invalid_years = sum(vehicle_year &lt; 1900 | vehicle_year &gt; year(Sys.Date()) + 1, na.rm = TRUE),\n      missing_makes = sum(is.na(vehicle_make)),\n      missing_models = sum(is.na(vehicle_model)),\n      unique_makes = n_distinct(vehicle_make, na.rm = TRUE),\n      unique_models = n_distinct(vehicle_model, na.rm = TRUE)\n    )\n  \n  # logical consistency checks\n  validation_results$logical &lt;- data %&gt;%\n    summarise(\n      # check if parked vehicles are marked as having movement\n      parked_moving = sum(parked_vehicle == \"Yes\" & vehicle_movement != \"PARKED\", na.rm = TRUE),\n      \n      # check for injuries in no-damage crashes\n      injuries_no_damage = sum(injury_severity != \"NO_INJURY\" & \n                             vehicle_damage_extent == \"NO_DAMAGE\", na.rm = TRUE),\n      \n      # check for fatal crashes without severe damage\n      fatal_minor_damage = sum(injury_severity == \"FATAL\" & \n                             vehicle_damage_extent %in% c(\"NO_DAMAGE\", \"MINOR\"), na.rm = TRUE),\n      \n      # check for driverless vehicles marked with driver characteristics\n      driverless_with_driver = sum(driverless_vehicle == \"Yes\" & \n                                  !is.na(driver_substance_abuse), na.rm = TRUE)\n    )\n  \n  # cross-reference checks\n  validation_results$cross_reference &lt;- data %&gt;%\n    summarise(\n      # light condition vs time of day consistency\n      light_time_mismatch = sum(\n        (format(crash_date/time, \"%H:%M:%S\") &gt;= \"18:00:00\" | \n         format(crash_date/time, \"%H:%M:%S\") &lt;= \"06:00:00\") & \n        light == \"DAYLIGHT\", na.rm = TRUE\n      ),\n      \n      # weather vs surface condition consistency\n      weather_surface_mismatch = sum(\n        (weather %in% c(\"RAIN\", \"SNOW\") & surface_condition == \"DRY\") |\n        (weather == \"CLEAR\" & surface_condition %in% c(\"ICE\", \"SNOW\")), \n        na.rm = TRUE\n      )\n    )\n  \n  # value distribution checks\n  validation_results$distributions &lt;- list(\n    speed_distribution = table(data$speed_limit),\n    damage_by_speed = table(data$vehicle_damage_extent, cut(data$speed_limit, \n                                                          breaks = c(0, 25, 35, 45, 55, Inf))),\n    injury_by_speed = table(data$injury_severity, cut(data$speed_limit, \n                                                    breaks = c(0, 25, 35, 45, 55, Inf)))\n  )\n  \n  return(validation_results)\n}\n\n# run validation and create summary report\nvalidation_summary &lt;- validate_crash_data(drivers_data_clean)\n\n# create a function to print validation results in a readable format\nprint_validation_summary &lt;- function(validation_results) {\n  cat(\"\\n=== VALIDATION SUMMARY ===\\n\")\n  \n  cat(\"\\nTEMPORAL VALIDATION:\")\n  cat(\"\\n- Future dates:\", validation_results$temporal$future_dates)\n  cat(\"\\n- Weekend crashes:\", validation_results$temporal$weekend_crashes)\n  cat(\"\\n- Night crashes:\", validation_results$temporal$night_crashes)\n  \n  cat(\"\\n\\nGEOGRAPHIC VALIDATION:\")\n  print(validation_results$geographic)\n  \n  cat(\"\\n\\nVEHICLE VALIDATION:\")\n  print(validation_results$vehicle)\n  \n  cat(\"\\n\\nLOGICAL CONSISTENCY CHECKS:\")\n  print(validation_results$logical)\n  \n  cat(\"\\n\\nCROSS-REFERENCE CHECKS:\")\n  print(validation_results$cross_reference)\n  \n  cat(\"\\n\\nDISTRIBUTIONS:\")\n  cat(\"\\nSpeed Limit Distribution:\\n\")\n  print(validation_results$distributions$speed_distribution)\n  \n  cat(\"\\nDamage by Speed Range:\\n\")\n  print(validation_results$distributions$damage_by_speed)\n}\n\n# run and print validation summary\nprint_validation_summary(validation_summary)\n\n# create data quality flags based on validation results\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    data_quality_flags = case_when(\n      # temporal flags\n      crash_date/time &gt; Sys.time() ~ \"FUTURE_DATE\",\n      \n      # geographic flags\n      (latitude &lt; 38 | latitude &gt; 40 | longitude &gt; -76 | longitude &lt; -78) ~ \"INVALID_COORDINATES\",\n      \n      # vehicle flags\n      (vehicle_year &lt; 1900 | vehicle_year &gt; year(Sys.Date()) + 1) ~ \"INVALID_VEHICLE_YEAR\",\n      \n      # logical consistency flags\n      (parked_vehicle == \"Yes\" & vehicle_movement != \"PARKED\") ~ \"PARKED_MOVING_MISMATCH\",\n      (injury_severity != \"NO_INJURY\" & vehicle_damage_extent == \"NO_DAMAGE\") ~ \"INJURY_NO_DAMAGE_MISMATCH\",\n      (injury_severity == \"FATAL\" & vehicle_damage_extent %in% c(\"NO_DAMAGE\", \"MINOR\")) ~ \"FATAL_MINOR_DAMAGE_MISMATCH\",\n      \n      # cross-reference flags\n      (format(crash_date/time, \"%H:%M:%S\") &gt;= \"18:00:00\" | \n       format(crash_date/time, \"%H:%M:%S\") &lt;= \"06:00:00\") & \n      light == \"DAYLIGHT\" ~ \"LIGHT_TIME_MISMATCH\",\n      \n      (weather %in% c(\"RAIN\", \"SNOW\") & surface_condition == \"DRY\") |\n      (weather == \"CLEAR\" & surface_condition %in% c(\"ICE\", \"SNOW\")) ~ \"WEATHER_SURFACE_MISMATCH\",\n      \n      TRUE ~ \"VALID\"\n    )\n  )\n\nquality_flag_summary &lt;- drivers_data_clean %&gt;%\n  group_by(data_quality_flags) %&gt;%\n  summarise(\n    count = n(),\n    percentage = n() / nrow(drivers_data_clean) * 100\n  ) %&gt;%\n  arrange(desc(count))\n\nprint(\"\\nData Quality Flag Summary:\")\nprint(quality_flag_summary)\n\nSomething still seems off with the data quality flags.\n\n# fix Light-Time Mismatches (largest issue ~5.2% of data)\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # extract hour from crash_date/time\n    crash_hour = as.numeric(format(crash_date/time, \"%H\")),\n    \n    # correct light condition based on time\n    light = case_when(\n      # dawn hours (5-7 AM)\n      crash_hour &gt;= 5 & crash_hour &lt; 7 ~ \"DAWN\",\n      \n      # daylight hours (7 AM - 6 PM)\n      crash_hour &gt;= 7 & crash_hour &lt; 18 ~ \"DAYLIGHT\",\n      \n      # dusk hours (6-8 PM)\n      crash_hour &gt;= 18 & crash_hour &lt; 20 ~ \"DUSK\",\n      \n      # night hours\n      crash_hour &gt;= 20 | crash_hour &lt; 5 ~ \"DARK_WITH_LIGHTING\",\n      \n      TRUE ~ light\n    )\n  ) %&gt;%\n  select(-crash_hour) # remove temporary column\n\n# fix Weather-Surface Condition Mismatches (~0.37% of data)\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # Adjust surface condition based on weather\n    surface_condition = case_when(\n      weather %in% c(\"RAIN\", \"RAINING\") & surface_condition == \"DRY\" ~ \"WET\",\n      weather == \"SNOW\" & surface_condition == \"DRY\" ~ \"SNOW\",\n      weather == \"CLEAR\" & surface_condition %in% c(\"ICE\", \"SNOW\") & \n        format(crash_date/time, \"%m\") %in% c(\"12\", \"01\", \"02\") ~ surface_condition, # Keep if winter months\n      weather == \"CLEAR\" & surface_condition %in% c(\"ICE\", \"SNOW\") ~ \"UNKNOWN\", # Change to unknown if non-winter\n      TRUE ~ surface_condition\n    )\n  )\n\n# fix Injury-Damage Inconsistencies\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # adjust injury severity or damage extent for logical consistency\n    injury_severity = case_when(\n      injury_severity != \"NO_INJURY\" & vehicle_damage_extent == \"NO_DAMAGE\" ~ \"NO_INJURY\",\n      injury_severity == \"FATAL\" & vehicle_damage_extent %in% c(\"NO_DAMAGE\", \"MINOR\") ~ \"SERIOUS_INJURY\",\n      TRUE ~ injury_severity\n    ),\n    \n    # ensure fatal crashes have appropriate damage extent\n    vehicle_damage_extent = case_when(\n      injury_severity == \"FATAL\" & vehicle_damage_extent %in% c(\"NO_DAMAGE\", \"MINOR\") ~ \"DISABLING\",\n      TRUE ~ vehicle_damage_extent\n    )\n  )\n\n# fix Invalid Coordinates (Montgomery County, MD boundaries)\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # montgomery County approximate boundaries\n    latitude = case_when(\n      latitude &lt; 38.9 | latitude &gt; 39.4 ~ NA_real_,\n      TRUE ~ latitude\n    ),\n    longitude = case_when(\n      longitude &lt; -77.5 | longitude &gt; -76.9 ~ NA_real_,\n      TRUE ~ longitude\n    )\n  )\n\n# clean Vehicle Makes and Models with High Frequency Patterns\n# analyze the most common patterns\nvehicle_patterns &lt;- drivers_data_clean %&gt;%\n  group_by(vehicle_make, vehicle_model) %&gt;%\n  summarise(\n    count = n(),\n    .groups = 'drop'\n  ) %&gt;%\n  arrange(desc(count)) %&gt;%\n  head(20)  # Look at top 20 patterns\n\n# now clean based on common patterns\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # standardize common model names\n    vehicle_model = case_when(\n      # remove digits from end of model names\n      str_detect(vehicle_model, \"[A-Z]+\\\\d+$\") ~ str_extract(vehicle_model, \"[A-Z]+\"),\n      # remove common suffixes\n      str_detect(vehicle_model, \"SDN|CPE|CVT|HBK\") ~ str_replace(vehicle_model, \"SDN|CPE|CVT|HBK\", \"\"),\n      TRUE ~ vehicle_model\n    ),\n    \n    # trim whitespace and standardize case\n    vehicle_model = str_trim(vehicle_model),\n    vehicle_make = str_trim(vehicle_make)\n  )\n\n# add Data Quality Score\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    data_quality_score = case_when(\n      data_quality_flags == \"VALID\" ~ 100,\n      data_quality_flags == \"LIGHT_TIME_MISMATCH\" ~ 90,\n      data_quality_flags == \"WEATHER_SURFACE_MISMATCH\" ~ 85,\n      data_quality_flags == \"INJURY_NO_DAMAGE_MISMATCH\" ~ 80,\n      data_quality_flags == \"INVALID_COORDINATES\" ~ 75,\n      data_quality_flags %in% c(\"FATAL_MINOR_DAMAGE_MISMATCH\", \"PARKED_MOVING_MISMATCH\") ~ 70,\n      TRUE ~ 60\n    )\n  )\n\n# verify cleaning results\ncleaning_verification &lt;- list()\n\n# check light-time consistency after cleaning\ncleaning_verification$light_time &lt;- drivers_data_clean %&gt;%\n  summarise(\n    light_time_mismatch_after = sum(\n      (format(crash_date/time, \"%H:%M:%S\") &gt;= \"18:00:00\" | \n       format(crash_date/time, \"%H:%M:%S\") &lt;= \"06:00:00\") & \n      light == \"DAYLIGHT\", \n      na.rm = TRUE\n    )\n  )\n\n# check weather-surface consistency after cleaning\ncleaning_verification$weather_surface &lt;- drivers_data_clean %&gt;%\n  summarise(\n    weather_surface_mismatch_after = sum(\n      (weather %in% c(\"RAIN\", \"SNOW\") & surface_condition == \"DRY\") |\n      (weather == \"CLEAR\" & surface_condition %in% c(\"ICE\", \"SNOW\")), \n      na.rm = TRUE\n    )\n  )\n\n# check data quality scores distribution\ncleaning_verification$quality_scores &lt;- drivers_data_clean %&gt;%\n  group_by(data_quality_score) %&gt;%\n  summarise(\n    count = n(),\n    percentage = n() / nrow(drivers_data_clean) * 100,\n    .groups = 'drop'\n  ) %&gt;%\n  arrange(desc(data_quality_score))\n\ncat(\"\\n=== CLEANING VERIFICATION RESULTS ===\\n\")\nprint(cleaning_verification)\n\nFinal Cleaning Steps Based on Verification Results\n\n# handle remaining weather-surface mismatches (577 cases)\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # create a more detailed weather-surface relationship\n    surface_condition = case_when(\n      # if it's raining, surface should be wet\n      weather %in% c(\"RAIN\", \"RAINING\") ~ \"WET\",\n      \n      # if it's snowing, surface should be snow or ice\n      weather == \"SNOW\" ~ \"SNOW\",\n      \n      # if it's freezing rain, surface should be ice\n      weather == \"FREEZING RAIN\" ~ \"ICE\",\n      \n      # for clear weather, keep existing surface condition if reasonable\n      weather == \"CLEAR\" & surface_condition %in% c(\"DRY\", \"WET\") ~ surface_condition,\n      \n      # for cloudy weather, keep existing surface condition\n      weather == \"CLOUDY\" ~ surface_condition,\n      \n      # for other cases, mark as unknown if inconsistent\n      TRUE ~ case_when(\n        surface_condition %in% c(\"DRY\", \"WET\", \"SNOW\", \"ICE\") ~ surface_condition,\n        TRUE ~ \"UNKNOWN\"\n      )\n    )\n  )\n\n# add more detailed quality flags and improve quality score\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # create compound quality flags for multiple issues\n    detailed_quality_flags = map2_chr(\n      data_quality_flags,\n      surface_condition,\n      ~case_when(\n        .x != \"VALID\" & .y == \"UNKNOWN\" ~ paste(.x, \"WITH_UNKNOWN_SURFACE\"),\n        TRUE ~ .x\n      )\n    ),\n    \n    # create a more nuanced quality score (0-100)\n    refined_quality_score = case_when(\n      data_quality_flags == \"VALID\" & !is.na(surface_condition) & \n      surface_condition != \"UNKNOWN\" ~ 100,\n      \n      data_quality_flags == \"VALID\" & \n      (is.na(surface_condition) | surface_condition == \"UNKNOWN\") ~ 95,\n      \n      data_quality_flags == \"LIGHT_TIME_MISMATCH\" ~ 90,\n      \n      data_quality_flags == \"WEATHER_SURFACE_MISMATCH\" & \n      !is.na(surface_condition) ~ 85,\n      \n      data_quality_flags == \"WEATHER_SURFACE_MISMATCH\" & \n      is.na(surface_condition) ~ 80,\n      \n      data_quality_flags == \"INJURY_NO_DAMAGE_MISMATCH\" ~ 75,\n      \n      data_quality_flags == \"INVALID_COORDINATES\" ~ 70,\n      \n      data_quality_flags %in% c(\"FATAL_MINOR_DAMAGE_MISMATCH\", \n                               \"PARKED_MOVING_MISMATCH\") ~ 65,\n      \n      TRUE ~ 60\n    )\n  )\n\n# create final data quality summary\nfinal_quality_summary &lt;- drivers_data_clean %&gt;%\n  summarise(\n    total_records = n(),\n    high_quality_records = sum(refined_quality_score &gt;= 95),\n    medium_quality_records = sum(refined_quality_score &gt;= 80 & refined_quality_score &lt; 95),\n    low_quality_records = sum(refined_quality_score &lt; 80),\n    mean_quality_score = mean(refined_quality_score),\n    median_quality_score = median(refined_quality_score),\n    \n    # percentage calculations\n    high_quality_percentage = (high_quality_records / total_records) * 100,\n    medium_quality_percentage = (medium_quality_records / total_records) * 100,\n    low_quality_percentage = (low_quality_records / total_records) * 100\n  )\n\ncat(\"\\n=== FINAL DATA QUALITY SUMMARY ===\\n\")\nprint(final_quality_summary)\n\n# create recommendations for further improvements\ncat(\"\\n=== RECOMMENDATIONS FOR FURTHER IMPROVEMENTS ===\\n\")\ncat(\"1. Weather-Surface Condition Relationship:\\n\")\ncat(\"   - \", sum(drivers_data_clean$surface_condition == \"UNKNOWN\"), \n    \"records still have unknown surface conditions\\n\")\ncat(\"   - Consider adding temperature data for better ice/snow validation\\n\\n\")\n\ncat(\"2. Geographic Data Quality:\\n\")\ncat(\"   - \", sum(is.na(drivers_data_clean$latitude) | is.na(drivers_data_clean$longitude)),\n    \"records have missing or invalid coordinates\\n\")\ncat(\"   - Consider implementing address geocoding for missing coordinates\\n\\n\")\n\ncat(\"3. Vehicle Information:\\n\")\ncat(\"   - \", sum(is.na(drivers_data_clean$vehicle_make)), \"records with missing vehicle makes\\n\")\ncat(\"   - \", sum(is.na(drivers_data_clean$vehicle_model)), \"records with missing vehicle models\\n\")\ncat(\"   - Consider implementing VIN decoding for missing vehicle information\\n\\n\")\n\nprocessing_summary &lt;- list(\n  initial_issues = list(\n    light_time_mismatch = 9744,\n    weather_surface_mismatch = 702,\n    injury_damage_mismatch = 306,\n    invalid_coordinates = 26,\n    fatal_minor_damage = 1,\n    parked_moving = 1\n  ),\n  \n  final_status = list(\n    quality_summary = final_quality_summary,\n    remaining_weather_surface_mismatch = 577,\n    remaining_unknown_surface = sum(drivers_data_clean$surface_condition == \"UNKNOWN\", na.rm = TRUE)\n  )\n)\n\ncat(\"\\n=== DATA PROCESSING SUMMARY ===\\n\")\ncat(\"Initial Issues vs. Final Status:\\n\")\ncat(\"- Light-Time Mismatches: \", processing_summary$initial_issues$light_time_mismatch, \n    \" -&gt; 0\\n\")\ncat(\"- Weather-Surface Mismatches: \", processing_summary$initial_issues$weather_surface_mismatch,\n    \" -&gt; \", processing_summary$final_status$remaining_weather_surface_mismatch, \"\\n\")\n\nVisualizations.\n\ninstall.packages(\"viridis\")\n\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(patchwork)  # for combining plots\nlibrary(viridis)    # for colorblind-friendly palettes\nlibrary(lubridate)\n\n# crash Time Patterns\ntime_plot &lt;- drivers_data_clean %&gt;%\n  mutate(\n    hour = hour(crash_date/time),\n    weekday = wday(crash_date/time, label = TRUE),\n    month = month(crash_date/time, label = TRUE)\n  ) %&gt;%\n  ggplot(aes(x = hour)) +\n  geom_histogram(binwidth = 1, fill = \"steelblue\", color = \"white\") +\n  labs(title = \"Distribution of Crashes by Hour of Day\",\n       x = \"Hour of Day\",\n       y = \"Number of Crashes\") +\n  theme_minimal() +\n  scale_x_continuous(breaks = seq(0, 23, 2))\n\n# injury Severity by Vehicle Type\nseverity_vehicle_plot &lt;- drivers_data_clean %&gt;%\n  filter(!is.na(vehicle_body_type), !is.na(injury_severity)) %&gt;%\n  count(vehicle_body_type, injury_severity) %&gt;%\n  group_by(vehicle_body_type) %&gt;%\n  mutate(pct = n/sum(n) * 100) %&gt;%\n  ggplot(aes(x = reorder(vehicle_body_type, -pct), y = pct, fill = injury_severity)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(title = \"Injury Severity by Vehicle Type\",\n       x = \"Vehicle Type\",\n       y = \"Percentage\",\n       fill = \"Injury Severity\") +\n  theme_minimal() +\n  scale_fill_viridis_d()\n\n# weather Conditions and Crash Frequency\nweather_plot &lt;- drivers_data_clean %&gt;%\n  count(weather) %&gt;%\n  ggplot(aes(x = reorder(weather, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"darkgreen\") +\n  coord_flip() +\n  labs(title = \"Crashes by Weather Condition\",\n       x = \"Weather Condition\",\n       y = \"Number of Crashes\") +\n  theme_minimal()\n\n# geographic Distribution of Crashes\nmap_plot &lt;- drivers_data_clean %&gt;%\n  filter(!is.na(latitude), !is.na(longitude)) %&gt;%\n  ggplot(aes(x = longitude, y = latitude)) +\n  geom_point(alpha = 0.1, color = \"red\") +\n  labs(title = \"Geographic Distribution of Crashes\",\n       x = \"Longitude\",\n       y = \"Latitude\") +\n  theme_minimal()\n\n# collision Types\ncollision_plot &lt;- drivers_data_clean %&gt;%\n  count(collision_type) %&gt;%\n  mutate(pct = n/sum(n) * 100) %&gt;%\n  ggplot(aes(x = reorder(collision_type, -pct), y = pct)) +\n  geom_bar(stat = \"identity\", fill = \"orange\") +\n  coord_flip() +\n  labs(title = \"Distribution of Collision Types\",\n       x = \"Collision Type\",\n       y = \"Percentage\") +\n  theme_minimal()\n\n# speed Limit and Crash Severity\nspeed_severity_plot &lt;- drivers_data_clean %&gt;%\n  filter(!is.na(speed_limit), !is.na(injury_severity)) %&gt;%\n  ggplot(aes(x = factor(speed_limit), fill = injury_severity)) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"Injury Severity by Speed Limit\",\n       x = \"Speed Limit\",\n       y = \"Proportion\",\n       fill = \"Injury Severity\") +\n  theme_minimal() +\n  scale_fill_viridis_d()\n\n# vehicle Age Distribution\nvehicle_age_plot &lt;- drivers_data_clean %&gt;%\n  mutate(vehicle_age = year(crash_date/time) - vehicle_year) %&gt;%\n  filter(vehicle_age &gt;= 0, vehicle_age &lt;= 30) %&gt;%\n  ggplot(aes(x = vehicle_age)) +\n  geom_histogram(binwidth = 1, fill = \"purple\", color = \"white\") +\n  labs(title = \"Distribution of Vehicle Age\",\n       x = \"Vehicle Age (Years)\",\n       y = \"Count\") +\n  theme_minimal()\n\n# surface Condition and Weather Relationship\nsurface_weather_plot &lt;- drivers_data_clean %&gt;%\n  filter(!is.na(surface_condition), !is.na(weather)) %&gt;%\n  count(surface_condition, weather) %&gt;%\n  group_by(weather) %&gt;%\n  mutate(pct = n/sum(n) * 100) %&gt;%\n  ggplot(aes(x = weather, y = surface_condition, fill = pct)) +\n  geom_tile() +\n  scale_fill_viridis_c() +\n  labs(title = \"Surface Condition by Weather\",\n       x = \"Weather\",\n       y = \"Surface Condition\",\n       fill = \"Percentage\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n# combine plots using patchwork\nlayout &lt;- \"\nAABB\nCCDD\nEEFF\nGGHH\n\"\n\ncombined_plots &lt;- time_plot + severity_vehicle_plot + \n                 weather_plot + map_plot + \n                 collision_plot + speed_severity_plot + \n                 vehicle_age_plot + surface_weather_plot +\n                 plot_layout(design = layout)\n\nprint(combined_plots)\n\nggsave(\"crash_analysis_dashboard.pdf\", combined_plots, width = 20, height = 24)\n\nsummary_stats &lt;- list(\n  \n  time_stats = drivers_data_clean %&gt;%\n    mutate(\n      hour = hour(crash_date/time),\n      weekday = wday(crash_date/time, label = TRUE)\n    ) %&gt;%\n    summarise(\n      peak_hour = names(which.max(table(hour))),\n      weekend_crashes = mean(weekday %in% c(\"Sat\", \"Sun\")) * 100\n    ),\n  \n  severity_stats = drivers_data_clean %&gt;%\n    group_by(injury_severity) %&gt;%\n    summarise(\n      count = n(),\n      percentage = n() / nrow(drivers_data_clean) * 100\n    ),\n  \n  weather_stats = drivers_data_clean %&gt;%\n    group_by(weather) %&gt;%\n    summarise(\n      count = n(),\n      percentage = n() / nrow(drivers_data_clean) * 100\n    ),\n  \n  vehicle_stats = drivers_data_clean %&gt;%\n    summarise(\n      avg_vehicle_age = mean(year(crash_date/time) - vehicle_year, na.rm = TRUE),\n      most_common_make = names(which.max(table(vehicle_make)))\n    )\n)\n\nprint(summary_stats)\n\nSave the final cleaned dataset.\n\ncurrent_timestamp &lt;- format(Sys.time(), \"%Y%m%d_%H%M\")\n\n# create directories if they don't exist\ndir.create(\"cleaned_data\", showWarnings = FALSE)\ndir.create(\"data_documentation\", showWarnings = FALSE)\n\nstr_result &lt;- capture.output(str(drivers_data_clean))\nprint(\"Data structure:\")\nprint(str_result)\n\ncolumn_info &lt;- data.frame(\n  column_name = names(drivers_data_clean),\n  data_type = sapply(drivers_data_clean, function(x) class(x)[1]),  # Take first class if multiple\n  stringsAsFactors = FALSE\n)\n\ncolumn_info$description &lt;- sapply(column_info$column_name, function(col_name) {\n  case_when(\n    col_name == \"report_number\" ~ \"Unique report identifier\",\n    col_name == \"local_case_number\" ~ \"Local case number for the incident\",\n    col_name == \"agency_name\" ~ \"Name of reporting agency\",\n    col_name == \"acrs_report_type\" ~ \"Type of crash report\",\n    col_name == \"crash_date/time\" ~ \"Date and time of the crash\",\n    col_name == \"route_type\" ~ \"Type of route where crash occurred\",\n    col_name == \"road_name\" ~ \"Name of the road where crash occurred\",\n    col_name == \"cross_street_name\" ~ \"Name of the nearest cross-street\",\n    col_name == \"off_road_description\" ~ \"Description for off-road incidents\",\n    col_name == \"municipality\" ~ \"Municipality where crash occurred\",\n    col_name == \"related_non_motorist\" ~ \"Type of non-motorist involved\",\n    col_name == \"collision_type\" ~ \"Type of collision\",\n    col_name == \"weather\" ~ \"Weather conditions during crash\",\n    col_name == \"surface_condition\" ~ \"Road surface condition\",\n    col_name == \"light\" ~ \"Light conditions\",\n    col_name == \"traffic_control\" ~ \"Traffic control present\",\n    col_name == \"driver_substance_abuse\" ~ \"Driver substance abuse status\",\n    col_name == \"non_motorist_substance_abuse\" ~ \"Non-motorist substance abuse status\",\n    col_name == \"person_id\" ~ \"Unique identifier for person involved\",\n    col_name == \"driver_at_fault\" ~ \"Indicator if driver was at fault\",\n    col_name == \"injury_severity\" ~ \"Severity of injuries\",\n    col_name == \"circumstance\" ~ \"Contributing circumstances\",\n    col_name == \"driver_distracted_by\" ~ \"Driver distraction factors\",\n    col_name == \"drivers_license_state\" ~ \"State of driver's license\",\n    col_name == \"vehicle_id\" ~ \"Unique identifier for vehicle\",\n    col_name == \"vehicle_damage_extent\" ~ \"Extent of vehicle damage\",\n    col_name == \"vehicle_first_impact_location\" ~ \"Location of first impact on vehicle\",\n    col_name == \"vehicle_body_type\" ~ \"Type of vehicle body\",\n    col_name == \"vehicle_movement\" ~ \"Vehicle movement during crash\",\n    col_name == \"vehicle_going_dir\" ~ \"Direction vehicle was traveling\",\n    col_name == \"speed_limit\" ~ \"Posted speed limit\",\n    col_name == \"driverless_vehicle\" ~ \"Indicator if vehicle was driverless\",\n    col_name == \"parked_vehicle\" ~ \"Indicator if vehicle was parked\",\n    col_name == \"vehicle_year\" ~ \"Year of vehicle manufacture\",\n    col_name == \"vehicle_make\" ~ \"Vehicle manufacturer\",\n    col_name == \"vehicle_model\" ~ \"Vehicle model\",\n    col_name == \"latitude\" ~ \"Latitude of crash location\",\n    col_name == \"longitude\" ~ \"Longitude of crash location\",\n    col_name == \"location\" ~ \"Combined location coordinates\",\n    col_name == \"data_quality_flags\" ~ \"Data quality flags from cleaning process\",\n    col_name == \"detailed_quality_flags\" ~ \"Detailed quality flags from cleaning process\",\n    col_name == \"refined_quality_score\" ~ \"Numerical score indicating data quality\",\n    TRUE ~ paste(\"Description for\", col_name)  # Default description for any new columns\n  )\n})\n\ncolumn_info$example_values &lt;- sapply(drivers_data_clean, function(x) {\n  if (is.numeric(x)) {\n    paste(\"Range:\", min(x, na.rm = TRUE), \"to\", max(x, na.rm = TRUE))\n  } else {\n    unique_vals &lt;- unique(na.omit(x))\n    if (length(unique_vals) &gt; 5) {\n      paste(paste(unique_vals[1:5], collapse = \", \"), \"... and\", length(unique_vals) - 5, \"more values\")\n    } else {\n      paste(unique_vals, collapse = \", \")\n    }\n  }\n})\n\nwrite_csv(drivers_data_clean, \n          file = paste0(\"cleaned_data/crash_data_clean_\", current_timestamp, \".csv\"))\n\nsaveRDS(drivers_data_clean, \n        file = paste0(\"cleaned_data/crash_data_clean_\", current_timestamp, \".rds\"))\n\nwrite_csv(column_info, \n          file = paste0(\"data_documentation/data_dictionary_\", current_timestamp, \".csv\"))\n\nreadme_text &lt;- sprintf(\"\n# Crash Data Cleaning Documentation\n\n## Version Information\n- Date Created: %s\n- Original Records: 186,931\n- Final Records: %d\n\n## Data Quality Metrics\n- Total Records: %d\n- Records with Quality Flags: %d\n- Clean Records: %d\n\n## Files in this Package\n1. crash_data_clean_%s.csv - Main data file (CSV format)\n2. crash_data_clean_%s.rds - R data file (RDS format)\n3. data_dictionary_%s.csv - Data dictionary with column descriptions\n\n## Column Summary\nTotal number of columns: %d\nSee data dictionary file for detailed information about each column.\n\n## Data Quality Notes\n- Data has been cleaned and standardized\n- Quality flags have been added to mark potential issues\n- Missing values have been handled according to context\n- Inconsistent categories have been standardized\n\n## Usage Notes\n- Please refer to the data dictionary for column descriptions\n- Check quality flags before analysis\n- Some columns may contain standardized values\n\n## Contact\nFor questions about this dataset, please contact [Your Contact Information]\n\n## Last Updated\n%s\n\",\n    format(Sys.time(), \"%Y-%m-%d %H:%M:%S\"),\n    nrow(drivers_data_clean),\n    nrow(drivers_data_clean),\n    sum(drivers_data_clean$data_quality_flags != \"VALID\"),\n    sum(drivers_data_clean$data_quality_flags == \"VALID\"),\n    current_timestamp,\n    current_timestamp,\n    current_timestamp,\n    ncol(drivers_data_clean),\n    format(Sys.time(), \"%Y-%m-%d %H:%M:%S\")\n)\n\nwriteLines(readme_text, \n           paste0(\"data_documentation/README_\", current_timestamp, \".md\"))\n\nzip_files &lt;- c(\n    paste0(\"cleaned_data/crash_data_clean_\", current_timestamp, \".csv\"),\n    paste0(\"cleaned_data/crash_data_clean_\", current_timestamp, \".rds\"),\n    paste0(\"data_documentation/data_dictionary_\", current_timestamp, \".csv\"),\n    paste0(\"data_documentation/README_\", current_timestamp, \".md\")\n)\n\nzip(paste0(\"cleaned_data/crash_data_package_\", current_timestamp, \".zip\"),\n    files = zip_files)\n\ncat(\"\\nData saving complete!\\n\")\ncat(\"Files saved:\\n\")\ncat(\"1. CSV data file\\n\")\ncat(\"2. RDS data file\\n\")\ncat(\"3. Data dictionary\\n\")\ncat(\"4. README documentation\\n\")\ncat(\"5. Complete package (zip)\\n\")\ncat(\"\\nLocation: ./cleaned_data/ and ./data_documentation/\\n\")"
  },
  {
    "objectID": "dataset/Cleaning_Xiang.html#setup",
    "href": "dataset/Cleaning_Xiang.html#setup",
    "title": "crash car data clean",
    "section": "",
    "text": "options(repos = c(CRAN = \"https://cran.r-project.org\"))\ninstall.packages(\"skimr\")\n\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(lubridate)\nlibrary(skimr)  # for quick summary statistics\n\n\ninstall.packages(\"RCurl\")\nlibrary(RCurl)\nx &lt;- getURL(\"https://raw.githubusercontent.com/sussmanbu/ma-4615-fa24-final-project-group-1/main/Crash_Reporting_Drivers_Data.csv\")\ndrivers_data &lt;- read.csv(text = x)\n\n\nglimpse(drivers_data)\n\nskim(drivers_data)\n\nClean Column Names\n\ndrivers_data_clean &lt;- drivers_data %&gt;%\n  clean_names()\n\ncolnames(drivers_data_clean)\n\nHandle Date/Time Format\n\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    crash_date/time &lt;- mdy_hms(crash_date/time),\n    # create separate date and time columns if needed\n    crash_date = date(crash_date/time),\n    crash_time = format(crash_date/time, \"%H:%M:%S\")\n  )\n\nStandardize Categorical Variables\n\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # standardize agency names\n    agency_name = str_trim(agency_name),\n    \n    # clean and standardize route type\n    route_type = case_when(\n      is.na(route_type) ~ \"Unknown\",\n      TRUE ~ route_type\n    ),\n    \n    # standardize weather conditions\n    weather = str_to_title(weather),\n    \n    # clean surface condition\n    surface_condition = case_when(\n      is.na(surface_condition) ~ \"Unknown\",\n      TRUE ~ surface_condition\n    ),\n    \n    # standardize driver at fault\n    driver_at_fault = case_when(\n      str_to_lower(driver_at_fault) == \"yes\" ~ \"Yes\",\n      str_to_lower(driver_at_fault) == \"no\" ~ \"No\",\n      TRUE ~ \"Unknown\"\n    )\n  )\n\nHandle Missing Values\n\n# calculate missing values percentage\nmissing_values &lt;- drivers_data_clean %&gt;%\n  summarise(across(everything(), ~sum(is.na(.))/n()*100)) %&gt;%\n  pivot_longer(everything(), \n              names_to = \"column\", \n              values_to = \"missing_percentage\") %&gt;%\n  arrange(desc(missing_percentage))\n\n# display columns with missing values\nprint(missing_values %&gt;% filter(missing_percentage &gt; 0))\n\n# handle missing values based on column type\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # for route_type, road_name, and cross_street_name\n    # keep NA as is since they might be meaningful (e.g., off-road incidents)\n    \n    # for numeric columns, consider if 0 or NA is more appropriate\n    speed_limit = if_else(is.na(speed_limit), 0, speed_limit),\n    \n    # for categorical columns, mark unknown\n    surface_condition = if_else(is.na(surface_condition), \"Unknown\", surface_condition),\n    traffic_control = if_else(is.na(traffic_control), \"Unknown\", traffic_control)\n  )\n\nData Validation and Consistency Checks\n\n# check for logical consistencies\nvalidation_results &lt;- drivers_data_clean %&gt;%\n  summarise(\n    # check for future dates\n    future_dates = sum(crash_date &gt; Sys.Date()),\n    \n    # check for valid speed limits\n    invalid_speed = sum(speed_limit &gt; 70 | speed_limit &lt; 0, na.rm = TRUE),\n    \n    # check for valid vehicle years\n    invalid_vehicle_year = sum(vehicle_year &lt; 1900 | vehicle_year &gt; year(Sys.Date()) + 1, na.rm = TRUE),\n    \n    # check for valid coordinates\n    invalid_lat = sum(latitude &lt; 38 | latitude &gt; 40, na.rm = TRUE),\n    invalid_lon = sum(longitude &gt; -76 | longitude &lt; -78, na.rm = TRUE)\n  )\n\nprint(validation_results)\n\n# create flags for potential data quality issues\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    data_quality_flag = case_when(\n      crash_date &gt; Sys.Date() ~ \"Future date\",\n      speed_limit &gt; 70 | speed_limit &lt; 0 ~ \"Invalid speed limit\",\n      vehicle_year &lt; 1900 | vehicle_year &gt; year(Sys.Date()) + 1 ~ \"Invalid vehicle year\",\n      latitude &lt; 38 | latitude &gt; 40 | longitude &gt; -76 | longitude &lt; -78 ~ \"Invalid coordinates\",\n      TRUE ~ \"Valid\"\n    )\n  )\n\nCheck Value Distributions\n\n# check common categories\ncategory_summaries &lt;- list(\n  collision_types = table(drivers_data_clean$collision_type),\n  weather_conditions = table(drivers_data_clean$weather),\n  vehicle_types = table(drivers_data_clean$vehicle_body_type),\n  injury_severity = table(drivers_data_clean$injury_severity)\n)\n\nprint(category_summaries)\n\nCollision Types Need Standardization:\n\n# standardize collision types\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    collision_type = case_when(\n      collision_type %in% c(\"Angle\", \"ANGLE MEETS LEFT HEAD ON\", \"ANGLE MEETS LEFT TURN\",\n                           \"ANGLE MEETS RIGHT TURN\", \"STRAIGHT MOVEMENT ANGLE\") ~ \"ANGLE\",\n      collision_type %in% c(\"Front to Front\", \"HEAD ON\", \"HEAD ON LEFT TURN\") ~ \"HEAD_ON\",\n      collision_type %in% c(\"Front to Rear\", \"SAME DIR REAR END\", \n                           \"SAME DIR REND LEFT TURN\", \"SAME DIR REND RIGHT TURN\") ~ \"REAR_END\",\n      collision_type %in% c(\"SAME DIRECTION SIDESWIPE\", \"Sideswipe, Same Direction\") ~ \"SIDESWIPE_SAME_DIR\",\n      collision_type %in% c(\"OPPOSITE DIRECTION SIDESWIPE\", \"Sideswipe, Opposite Direction\") ~ \"SIDESWIPE_OPPOSITE_DIR\",\n      collision_type %in% c(\"SINGLE VEHICLE\", \"Single Vehicle\") ~ \"SINGLE_VEHICLE\",\n      collision_type %in% c(\"Other\", \"OTHER\") ~ \"OTHER\",\n      collision_type %in% c(\"Unknown\", \"UNKNOWN\", \"N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\nWeather Conditions Need Standardization:\n\n# standardize weather conditions\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    weather = case_when(\n      str_detect(str_to_upper(weather), \"CLEAR\") ~ \"CLEAR\",\n      str_detect(str_to_upper(weather), \"CLOUD\") ~ \"CLOUDY\",\n      str_detect(str_to_upper(weather), \"RAIN|RAINING\") ~ \"RAIN\",\n      str_detect(str_to_upper(weather), \"SNOW|BLOWING SNOW\") ~ \"SNOW\",\n      str_detect(str_to_upper(weather), \"FOG|SMOG|SMOKE\") ~ \"FOG\",\n      weather %in% c(\"N/A\", \"Unknown\", \"UNKNOWN\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\nVehicle Types Need Major Cleanup:\n\n# standardize vehicle types\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    vehicle_body_type = case_when(\n      str_detect(str_to_upper(vehicle_body_type), \"PASSENGER CAR|PASSENGER|CAR\") ~ \"PASSENGER_CAR\",\n      str_detect(str_to_upper(vehicle_body_type), \"SUV|SPORT UTILITY|UTILITY VEHICLE\") ~ \"SUV\",\n      str_detect(str_to_upper(vehicle_body_type), \"PICKUP|LIGHT TRUCK\") ~ \"PICKUP_TRUCK\",\n      str_detect(str_to_upper(vehicle_body_type), \"VAN|CARGO\") ~ \"VAN\",\n      str_detect(str_to_upper(vehicle_body_type), \"BUS\") ~ \"BUS\",\n      str_detect(str_to_upper(vehicle_body_type), \"MOTORCYCLE|MOPED\") ~ \"MOTORCYCLE\",\n      str_detect(str_to_upper(vehicle_body_type), \"EMERGENCY|POLICE|FIRE|AMBULANCE\") ~ \"EMERGENCY_VEHICLE\",\n      vehicle_body_type %in% c(\"N/A\", \"Unknown\", \"UNKNOWN\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\nInjury Severity Standardization:\n\n# standardize injury severity\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    injury_severity = case_when(\n      str_detect(str_to_upper(injury_severity), \"FATAL\") ~ \"FATAL\",\n      str_detect(str_to_upper(injury_severity), \"NO APPARENT|NO INJURY\") ~ \"NO_INJURY\",\n      str_detect(str_to_upper(injury_severity), \"POSSIBLE\") ~ \"POSSIBLE_INJURY\",\n      str_detect(str_to_upper(injury_severity), \"MINOR\") ~ \"MINOR_INJURY\",\n      str_detect(str_to_upper(injury_severity), \"SERIOUS\") ~ \"SERIOUS_INJURY\",\n      TRUE ~ \"UNKNOWN\"\n    )\n  )\n\nHandle Missing Values Strategy (based on the missing percentage analysis):\n\n# handle missing values based on context\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # high missing percentage (&gt;90%) - keep as NA but add flag\n    has_non_motorist = !is.na(related_non_motorist),\n    \n    # medium missing percentage (10-90%) - add meaningful categories\n    municipality = if_else(is.na(municipality), \"UNINCORPORATED\", municipality),\n    road_name = if_else(is.na(road_name), \"OFF_ROAD\", road_name),\n    cross_street_name = if_else(is.na(cross_street_name), \"NOT_APPLICABLE\", cross_street_name),\n    \n    # low missing percentage (&lt;10%) - impute with \"UNKNOWN\"\n    drivers_license_state = if_else(is.na(drivers_license_state), \"UNKNOWN\", drivers_license_state),\n    circumstance = if_else(is.na(circumstance), \"UNKNOWN\", circumstance),\n    vehicle_going_dir = if_else(is.na(vehicle_going_dir), \"UNKNOWN\", vehicle_going_dir)\n  )\n\nHandle Data Quality Issues (based on validation results):\n\n# add data quality flags\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    data_quality_flags = case_when(\n      speed_limit &gt; 70 | speed_limit &lt; 0 ~ \"INVALID_SPEED_LIMIT\",\n      vehicle_year &lt; 1900 | vehicle_year &gt; year(Sys.Date()) + 1 ~ \"INVALID_VEHICLE_YEAR\",\n      latitude &lt; 38 | latitude &gt; 40 | longitude &gt; -76 | longitude &lt; -78 ~ \"INVALID_COORDINATES\",\n      TRUE ~ \"VALID\"\n    ),\n    \n    # clean speed limits\n    speed_limit = case_when(\n      speed_limit &gt; 70 ~ NA_real_,\n      speed_limit &lt; 0 ~ NA_real_,\n      TRUE ~ speed_limit\n    ),\n    \n    # clean vehicle years\n    vehicle_year = if_else(\n      vehicle_year &lt; 1900 | vehicle_year &gt; year(Sys.Date()) + 1,\n      NA_real_,\n      vehicle_year\n    )\n  )\n\nAdditional cleaning for specific columns:\n\n# additional cleaning steps for specific columns and vehicle data\n\n# standardize vehicle makes (common misspellings and abbreviations)\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    vehicle_make = case_when(\n      # honda variations\n      str_detect(str_to_upper(vehicle_make), \"HOND|HDA\") ~ \"HONDA\",\n      \n      # toyota variations\n      str_detect(str_to_upper(vehicle_make), \"TOY|TOYT\") ~ \"TOYOTA\",\n      \n      # ford\n      str_detect(str_to_upper(vehicle_make), \"^FRD|FORD\") ~ \"FORD\",\n      \n      # chevrolet variations\n      str_detect(str_to_upper(vehicle_make), \"CHEV|CHEVY|CHV\") ~ \"CHEVROLET\",\n      \n      # nissan variations\n      str_detect(str_to_upper(vehicle_make), \"NISS|NISN\") ~ \"NISSAN\",\n      \n      # hyundai variations\n      str_detect(str_to_upper(vehicle_make), \"HYUN|HYU\") ~ \"HYUNDAI\",\n      \n      # volkswagen variations\n      str_detect(str_to_upper(vehicle_make), \"VW|VOLK|VOLKS\") ~ \"VOLKSWAGEN\",\n      \n      # BMW\n      str_detect(str_to_upper(vehicle_make), \"BMW|BMV\") ~ \"BMW\",\n      \n      # Mercedes-Benz variations\n      str_detect(str_to_upper(vehicle_make), \"MERZ|MENZ|MERCEDES|BENZ\") ~ \"MERCEDES-BENZ\",\n      \n      # lexus\n      str_detect(str_to_upper(vehicle_make), \"LEX|LEXS\") ~ \"LEXUS\",\n      \n      # mazda variations\n      str_detect(str_to_upper(vehicle_make), \"MAZ|MAZD\") ~ \"MAZDA\",\n      \n      # subaru variations\n      str_detect(str_to_upper(vehicle_make), \"SUB|SUBR\") ~ \"SUBARU\",\n      \n      # kia\n      str_detect(str_to_upper(vehicle_make), \"^KIA\") ~ \"KIA\",\n      \n      # audi\n      str_detect(str_to_upper(vehicle_make), \"^AUD\") ~ \"AUDI\",\n      \n      # acura\n      str_detect(str_to_upper(vehicle_make), \"ACUR|ACU\") ~ \"ACURA\",\n      \n      # infinity\n      str_detect(str_to_upper(vehicle_make), \"INF|INFIN\") ~ \"INFINITI\",\n      \n      TRUE ~ str_to_upper(vehicle_make)\n    )\n  )\n\n# clean vehicle models\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    vehicle_model = case_when(\n      # clean common abbreviations\n      str_detect(str_to_upper(vehicle_model), \"^CRV|CR-V\") ~ \"CR-V\",\n      str_detect(str_to_upper(vehicle_model), \"^RAV|RAV4\") ~ \"RAV4\",\n      str_detect(str_to_upper(vehicle_model), \"ACCORD|ACRD\") ~ \"ACCORD\",\n      str_detect(str_to_upper(vehicle_model), \"CAMRY|CAM\") ~ \"CAMRY\",\n      str_detect(str_to_upper(vehicle_model), \"CIVIC|CVC\") ~ \"CIVIC\",\n      str_detect(str_to_upper(vehicle_model), \"ALTIMA|ALT\") ~ \"ALTIMA\",\n      str_detect(str_to_upper(vehicle_model), \"COROLLA|COR\") ~ \"COROLLA\",\n      str_detect(str_to_upper(vehicle_model), \"EXPLORER|EXPLR\") ~ \"EXPLORER\",\n      str_detect(str_to_upper(vehicle_model), \"F-150|F150\") ~ \"F-150\",\n      str_detect(str_to_upper(vehicle_model), \"HIGHLANDER|HGLDR\") ~ \"HIGHLANDER\",\n      TRUE ~ str_to_upper(vehicle_model)\n    )\n  )\n\n# additional cleaning for other specific columns\n\n# clean light conditions\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    light = case_when(\n      str_detect(str_to_upper(light), \"DAYLIGHT\") ~ \"DAYLIGHT\",\n      str_detect(str_to_upper(light), \"DARK.*LIGHT.*ON|LIGHTED\") ~ \"DARK_WITH_LIGHTING\",\n      str_detect(str_to_upper(light), \"DARK.*NO.*LIGHT|UNLIGHTED\") ~ \"DARK_NO_LIGHTING\",\n      str_detect(str_to_upper(light), \"DAWN\") ~ \"DAWN\",\n      str_detect(str_to_upper(light), \"DUSK\") ~ \"DUSK\",\n      str_detect(str_to_upper(light), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# clean surface conditions\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    surface_condition = case_when(\n      str_detect(str_to_upper(surface_condition), \"DRY\") ~ \"DRY\",\n      str_detect(str_to_upper(surface_condition), \"WET\") ~ \"WET\",\n      str_detect(str_to_upper(surface_condition), \"ICE|ICY\") ~ \"ICE\",\n      str_detect(str_to_upper(surface_condition), \"SNOW|SLUSH\") ~ \"SNOW\",\n      str_detect(str_to_upper(surface_condition), \"SAND|DIRT|MUD\") ~ \"SAND_DIRT_MUD\",\n      str_detect(str_to_upper(surface_condition), \"OIL|GREASE\") ~ \"OIL_GREASE\",\n      is.na(surface_condition) | str_detect(str_to_upper(surface_condition), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# clean traffic control\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    traffic_control = case_when(\n      str_detect(str_to_upper(traffic_control), \"SIGNAL|TRAFFIC LIGHT\") ~ \"TRAFFIC_SIGNAL\",\n      str_detect(str_to_upper(traffic_control), \"STOP SIGN\") ~ \"STOP_SIGN\",\n      str_detect(str_to_upper(traffic_control), \"YIELD\") ~ \"YIELD_SIGN\",\n      str_detect(str_to_upper(traffic_control), \"FLASHING\") ~ \"FLASHING_SIGNAL\",\n      str_detect(str_to_upper(traffic_control), \"NO CONTROL|NONE\") ~ \"NO_CONTROL\",\n      str_detect(str_to_upper(traffic_control), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# clean driver substance abuse\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    driver_substance_abuse = case_when(\n      str_detect(str_to_upper(driver_substance_abuse), \"NONE|NO ABUSE\") ~ \"NONE\",\n      str_detect(str_to_upper(driver_substance_abuse), \"ALCOHOL\") ~ \"ALCOHOL\",\n      str_detect(str_to_upper(driver_substance_abuse), \"DRUG\") ~ \"DRUGS\",\n      str_detect(str_to_upper(driver_substance_abuse), \"BOTH|ALCOHOL.*DRUG|DRUG.*ALCOHOL\") ~ \"ALCOHOL_AND_DRUGS\",\n      str_detect(str_to_upper(driver_substance_abuse), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# clean driver distracted by\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    driver_distracted_by = case_when(\n      str_detect(str_to_upper(driver_distracted_by), \"NOT DISTRACTED\") ~ \"NOT_DISTRACTED\",\n      str_detect(str_to_upper(driver_distracted_by), \"CELL|PHONE|MOBILE\") ~ \"CELL_PHONE\",\n      str_detect(str_to_upper(driver_distracted_by), \"PASSENGER\") ~ \"PASSENGER\",\n      str_detect(str_to_upper(driver_distracted_by), \"RADIO|AUDIO\") ~ \"AUDIO_EQUIPMENT\",\n      str_detect(str_to_upper(driver_distracted_by), \"EAT|DRINK\") ~ \"EATING_DRINKING\",\n      str_detect(str_to_upper(driver_distracted_by), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# verify our cleaning by checking the unique values in each cleaned column\nverification_check &lt;- function(data, columns) {\n  map(columns, ~{\n    unique_vals &lt;- data %&gt;% \n      pull(.) %&gt;% \n      unique() %&gt;% \n      sort()\n    \n    cat(\"\\nUnique values in\", ., \":\\n\")\n    print(unique_vals)\n    cat(\"\\n\")\n  })\n}\n\n# verify the cleaning results for key columns\ncolumns_to_verify &lt;- c(\n  \"vehicle_make\", \n  \"vehicle_model\",\n  \"light\",\n  \"surface_condition\",\n  \"traffic_control\",\n  \"driver_substance_abuse\",\n  \"driver_distracted_by\"\n)\n\nverification_check(drivers_data_clean, columns_to_verify)\n\nvehicle_summary &lt;- drivers_data_clean %&gt;%\n  group_by(vehicle_make, vehicle_model) %&gt;%\n  summarise(\n    count = n(),\n    .groups = 'drop'\n  ) %&gt;%\n  arrange(desc(count))\n\nprint(vehicle_summary)\n\n\n# clean and standardize agency names\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    agency_name = case_when(\n      str_detect(str_to_upper(agency_name), \"MONTGOMERY|MONT|MCP\") ~ \"MONTGOMERY COUNTY POLICE\",\n      str_detect(str_to_upper(agency_name), \"GAITHERSBURG|GAITH\") ~ \"GAITHERSBURG POLICE\",\n      str_detect(str_to_upper(agency_name), \"ROCKVILLE|ROCK\") ~ \"ROCKVILLE POLICE\",\n      str_detect(str_to_upper(agency_name), \"TAKOMA|TAK\") ~ \"TAKOMA PARK POLICE\",\n      str_detect(str_to_upper(agency_name), \"PARK|MNCPP\") ~ \"MD NATIONAL CAPITAL PARK POLICE\",\n      TRUE ~ str_to_upper(agency_name)\n    )\n  )\n\n# clean and standardize vehicle damage extent\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    vehicle_damage_extent = case_when(\n      str_detect(str_to_upper(vehicle_damage_extent), \"NONE|NO DAMAGE\") ~ \"NO_DAMAGE\",\n      str_detect(str_to_upper(vehicle_damage_extent), \"MINOR|SUPERFICIAL\") ~ \"MINOR\",\n      str_detect(str_to_upper(vehicle_damage_extent), \"FUNCTIONAL\") ~ \"FUNCTIONAL\",\n      str_detect(str_to_upper(vehicle_damage_extent), \"DISABLE|DISABLING\") ~ \"DISABLING\",\n      str_detect(str_to_upper(vehicle_damage_extent), \"DESTROYED\") ~ \"DESTROYED\",\n      str_detect(str_to_upper(vehicle_damage_extent), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# clean and standardize vehicle first impact location\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    vehicle_first_impact_location = case_when(\n      str_detect(str_to_upper(vehicle_first_impact_location), \"FRONT|TWELVE|12\") ~ \"FRONT\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"RIGHT FRONT|ONE|1\") ~ \"RIGHT_FRONT\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"RIGHT SIDE|THREE|3\") ~ \"RIGHT_SIDE\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"RIGHT REAR|FOUR|4\") ~ \"RIGHT_REAR\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"REAR|SIX|6\") ~ \"REAR\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"LEFT REAR|SEVEN|7|EIGHT|8\") ~ \"LEFT_REAR\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"LEFT SIDE|NINE|9\") ~ \"LEFT_SIDE\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"LEFT FRONT|TEN|10|ELEVEN|11\") ~ \"LEFT_FRONT\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# clean and standardize vehicle movement\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    vehicle_movement = case_when(\n      str_detect(str_to_upper(vehicle_movement), \"STRAIGHT|CONSTANT\") ~ \"STRAIGHT\",\n      str_detect(str_to_upper(vehicle_movement), \"TURN.*RIGHT\") ~ \"TURNING_RIGHT\",\n      str_detect(str_to_upper(vehicle_movement), \"TURN.*LEFT\") ~ \"TURNING_LEFT\",\n      str_detect(str_to_upper(vehicle_movement), \"STOP|SLOW\") ~ \"SLOWING_STOPPING\",\n      str_detect(str_to_upper(vehicle_movement), \"BACK|REVERSE\") ~ \"BACKING\",\n      str_detect(str_to_upper(vehicle_movement), \"PARK\") ~ \"PARKED\",\n      str_detect(str_to_upper(vehicle_movement), \"START\") ~ \"STARTING\",\n      str_detect(str_to_upper(vehicle_movement), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n\n# create a comprehensive validation function\nvalidate_crash_data &lt;- function(data) {\n  # initialize empty list for validation results\n  validation_results &lt;- list()\n  \n  # temporal validations\n  validation_results$temporal &lt;- data %&gt;%\n    summarise(\n      future_dates = sum(crash_date/time &gt; Sys.time()),\n      weekend_crashes = sum(lubridate::wday(crash_date/time, week_start = 1) %in% c(6,7)),\n      night_crashes = sum(format(crash_date/time, \"%H:%M:%S\") &gt;= \"18:00:00\" | \n                         format(crash_date/time, \"%H:%M:%S\") &lt;= \"06:00:00\"),\n      crashes_by_hour = table(format(crash_date/time, \"%H\"))\n    )\n  \n  # geographic validations\n  validation_results$geographic &lt;- data %&gt;%\n    summarise(\n      invalid_lat = sum(latitude &lt; 38 | latitude &gt; 40, na.rm = TRUE),\n      invalid_lon = sum(longitude &gt; -76 | longitude &lt; -78, na.rm = TRUE),\n      missing_coords = sum(is.na(latitude) | is.na(longitude)),\n      unique_locations = n_distinct(location, na.rm = TRUE)\n    )\n  \n  # vehicle validations\n  validation_results$vehicle &lt;- data %&gt;%\n    summarise(\n      invalid_years = sum(vehicle_year &lt; 1900 | vehicle_year &gt; year(Sys.Date()) + 1, na.rm = TRUE),\n      missing_makes = sum(is.na(vehicle_make)),\n      missing_models = sum(is.na(vehicle_model)),\n      unique_makes = n_distinct(vehicle_make, na.rm = TRUE),\n      unique_models = n_distinct(vehicle_model, na.rm = TRUE)\n    )\n  \n  # logical consistency checks\n  validation_results$logical &lt;- data %&gt;%\n    summarise(\n      # check if parked vehicles are marked as having movement\n      parked_moving = sum(parked_vehicle == \"Yes\" & vehicle_movement != \"PARKED\", na.rm = TRUE),\n      \n      # check for injuries in no-damage crashes\n      injuries_no_damage = sum(injury_severity != \"NO_INJURY\" & \n                             vehicle_damage_extent == \"NO_DAMAGE\", na.rm = TRUE),\n      \n      # check for fatal crashes without severe damage\n      fatal_minor_damage = sum(injury_severity == \"FATAL\" & \n                             vehicle_damage_extent %in% c(\"NO_DAMAGE\", \"MINOR\"), na.rm = TRUE),\n      \n      # check for driverless vehicles marked with driver characteristics\n      driverless_with_driver = sum(driverless_vehicle == \"Yes\" & \n                                  !is.na(driver_substance_abuse), na.rm = TRUE)\n    )\n  \n  # cross-reference checks\n  validation_results$cross_reference &lt;- data %&gt;%\n    summarise(\n      # light condition vs time of day consistency\n      light_time_mismatch = sum(\n        (format(crash_date/time, \"%H:%M:%S\") &gt;= \"18:00:00\" | \n         format(crash_date/time, \"%H:%M:%S\") &lt;= \"06:00:00\") & \n        light == \"DAYLIGHT\", na.rm = TRUE\n      ),\n      \n      # weather vs surface condition consistency\n      weather_surface_mismatch = sum(\n        (weather %in% c(\"RAIN\", \"SNOW\") & surface_condition == \"DRY\") |\n        (weather == \"CLEAR\" & surface_condition %in% c(\"ICE\", \"SNOW\")), \n        na.rm = TRUE\n      )\n    )\n  \n  # value distribution checks\n  validation_results$distributions &lt;- list(\n    speed_distribution = table(data$speed_limit),\n    damage_by_speed = table(data$vehicle_damage_extent, cut(data$speed_limit, \n                                                          breaks = c(0, 25, 35, 45, 55, Inf))),\n    injury_by_speed = table(data$injury_severity, cut(data$speed_limit, \n                                                    breaks = c(0, 25, 35, 45, 55, Inf)))\n  )\n  \n  return(validation_results)\n}\n\n# run validation and create summary report\nvalidation_summary &lt;- validate_crash_data(drivers_data_clean)\n\n# create a function to print validation results in a readable format\nprint_validation_summary &lt;- function(validation_results) {\n  cat(\"\\n=== VALIDATION SUMMARY ===\\n\")\n  \n  cat(\"\\nTEMPORAL VALIDATION:\")\n  cat(\"\\n- Future dates:\", validation_results$temporal$future_dates)\n  cat(\"\\n- Weekend crashes:\", validation_results$temporal$weekend_crashes)\n  cat(\"\\n- Night crashes:\", validation_results$temporal$night_crashes)\n  \n  cat(\"\\n\\nGEOGRAPHIC VALIDATION:\")\n  print(validation_results$geographic)\n  \n  cat(\"\\n\\nVEHICLE VALIDATION:\")\n  print(validation_results$vehicle)\n  \n  cat(\"\\n\\nLOGICAL CONSISTENCY CHECKS:\")\n  print(validation_results$logical)\n  \n  cat(\"\\n\\nCROSS-REFERENCE CHECKS:\")\n  print(validation_results$cross_reference)\n  \n  cat(\"\\n\\nDISTRIBUTIONS:\")\n  cat(\"\\nSpeed Limit Distribution:\\n\")\n  print(validation_results$distributions$speed_distribution)\n  \n  cat(\"\\nDamage by Speed Range:\\n\")\n  print(validation_results$distributions$damage_by_speed)\n}\n\n# run and print validation summary\nprint_validation_summary(validation_summary)\n\n# create data quality flags based on validation results\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    data_quality_flags = case_when(\n      # temporal flags\n      crash_date/time &gt; Sys.time() ~ \"FUTURE_DATE\",\n      \n      # geographic flags\n      (latitude &lt; 38 | latitude &gt; 40 | longitude &gt; -76 | longitude &lt; -78) ~ \"INVALID_COORDINATES\",\n      \n      # vehicle flags\n      (vehicle_year &lt; 1900 | vehicle_year &gt; year(Sys.Date()) + 1) ~ \"INVALID_VEHICLE_YEAR\",\n      \n      # logical consistency flags\n      (parked_vehicle == \"Yes\" & vehicle_movement != \"PARKED\") ~ \"PARKED_MOVING_MISMATCH\",\n      (injury_severity != \"NO_INJURY\" & vehicle_damage_extent == \"NO_DAMAGE\") ~ \"INJURY_NO_DAMAGE_MISMATCH\",\n      (injury_severity == \"FATAL\" & vehicle_damage_extent %in% c(\"NO_DAMAGE\", \"MINOR\")) ~ \"FATAL_MINOR_DAMAGE_MISMATCH\",\n      \n      # cross-reference flags\n      (format(crash_date/time, \"%H:%M:%S\") &gt;= \"18:00:00\" | \n       format(crash_date/time, \"%H:%M:%S\") &lt;= \"06:00:00\") & \n      light == \"DAYLIGHT\" ~ \"LIGHT_TIME_MISMATCH\",\n      \n      (weather %in% c(\"RAIN\", \"SNOW\") & surface_condition == \"DRY\") |\n      (weather == \"CLEAR\" & surface_condition %in% c(\"ICE\", \"SNOW\")) ~ \"WEATHER_SURFACE_MISMATCH\",\n      \n      TRUE ~ \"VALID\"\n    )\n  )\n\nquality_flag_summary &lt;- drivers_data_clean %&gt;%\n  group_by(data_quality_flags) %&gt;%\n  summarise(\n    count = n(),\n    percentage = n() / nrow(drivers_data_clean) * 100\n  ) %&gt;%\n  arrange(desc(count))\n\nprint(\"\\nData Quality Flag Summary:\")\nprint(quality_flag_summary)\n\nSomething still seems off with the data quality flags.\n\n# fix Light-Time Mismatches (largest issue ~5.2% of data)\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # extract hour from crash_date/time\n    crash_hour = as.numeric(format(crash_date/time, \"%H\")),\n    \n    # correct light condition based on time\n    light = case_when(\n      # dawn hours (5-7 AM)\n      crash_hour &gt;= 5 & crash_hour &lt; 7 ~ \"DAWN\",\n      \n      # daylight hours (7 AM - 6 PM)\n      crash_hour &gt;= 7 & crash_hour &lt; 18 ~ \"DAYLIGHT\",\n      \n      # dusk hours (6-8 PM)\n      crash_hour &gt;= 18 & crash_hour &lt; 20 ~ \"DUSK\",\n      \n      # night hours\n      crash_hour &gt;= 20 | crash_hour &lt; 5 ~ \"DARK_WITH_LIGHTING\",\n      \n      TRUE ~ light\n    )\n  ) %&gt;%\n  select(-crash_hour) # remove temporary column\n\n# fix Weather-Surface Condition Mismatches (~0.37% of data)\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # Adjust surface condition based on weather\n    surface_condition = case_when(\n      weather %in% c(\"RAIN\", \"RAINING\") & surface_condition == \"DRY\" ~ \"WET\",\n      weather == \"SNOW\" & surface_condition == \"DRY\" ~ \"SNOW\",\n      weather == \"CLEAR\" & surface_condition %in% c(\"ICE\", \"SNOW\") & \n        format(crash_date/time, \"%m\") %in% c(\"12\", \"01\", \"02\") ~ surface_condition, # Keep if winter months\n      weather == \"CLEAR\" & surface_condition %in% c(\"ICE\", \"SNOW\") ~ \"UNKNOWN\", # Change to unknown if non-winter\n      TRUE ~ surface_condition\n    )\n  )\n\n# fix Injury-Damage Inconsistencies\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # adjust injury severity or damage extent for logical consistency\n    injury_severity = case_when(\n      injury_severity != \"NO_INJURY\" & vehicle_damage_extent == \"NO_DAMAGE\" ~ \"NO_INJURY\",\n      injury_severity == \"FATAL\" & vehicle_damage_extent %in% c(\"NO_DAMAGE\", \"MINOR\") ~ \"SERIOUS_INJURY\",\n      TRUE ~ injury_severity\n    ),\n    \n    # ensure fatal crashes have appropriate damage extent\n    vehicle_damage_extent = case_when(\n      injury_severity == \"FATAL\" & vehicle_damage_extent %in% c(\"NO_DAMAGE\", \"MINOR\") ~ \"DISABLING\",\n      TRUE ~ vehicle_damage_extent\n    )\n  )\n\n# fix Invalid Coordinates (Montgomery County, MD boundaries)\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # montgomery County approximate boundaries\n    latitude = case_when(\n      latitude &lt; 38.9 | latitude &gt; 39.4 ~ NA_real_,\n      TRUE ~ latitude\n    ),\n    longitude = case_when(\n      longitude &lt; -77.5 | longitude &gt; -76.9 ~ NA_real_,\n      TRUE ~ longitude\n    )\n  )\n\n# clean Vehicle Makes and Models with High Frequency Patterns\n# analyze the most common patterns\nvehicle_patterns &lt;- drivers_data_clean %&gt;%\n  group_by(vehicle_make, vehicle_model) %&gt;%\n  summarise(\n    count = n(),\n    .groups = 'drop'\n  ) %&gt;%\n  arrange(desc(count)) %&gt;%\n  head(20)  # Look at top 20 patterns\n\n# now clean based on common patterns\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # standardize common model names\n    vehicle_model = case_when(\n      # remove digits from end of model names\n      str_detect(vehicle_model, \"[A-Z]+\\\\d+$\") ~ str_extract(vehicle_model, \"[A-Z]+\"),\n      # remove common suffixes\n      str_detect(vehicle_model, \"SDN|CPE|CVT|HBK\") ~ str_replace(vehicle_model, \"SDN|CPE|CVT|HBK\", \"\"),\n      TRUE ~ vehicle_model\n    ),\n    \n    # trim whitespace and standardize case\n    vehicle_model = str_trim(vehicle_model),\n    vehicle_make = str_trim(vehicle_make)\n  )\n\n# add Data Quality Score\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    data_quality_score = case_when(\n      data_quality_flags == \"VALID\" ~ 100,\n      data_quality_flags == \"LIGHT_TIME_MISMATCH\" ~ 90,\n      data_quality_flags == \"WEATHER_SURFACE_MISMATCH\" ~ 85,\n      data_quality_flags == \"INJURY_NO_DAMAGE_MISMATCH\" ~ 80,\n      data_quality_flags == \"INVALID_COORDINATES\" ~ 75,\n      data_quality_flags %in% c(\"FATAL_MINOR_DAMAGE_MISMATCH\", \"PARKED_MOVING_MISMATCH\") ~ 70,\n      TRUE ~ 60\n    )\n  )\n\n# verify cleaning results\ncleaning_verification &lt;- list()\n\n# check light-time consistency after cleaning\ncleaning_verification$light_time &lt;- drivers_data_clean %&gt;%\n  summarise(\n    light_time_mismatch_after = sum(\n      (format(crash_date/time, \"%H:%M:%S\") &gt;= \"18:00:00\" | \n       format(crash_date/time, \"%H:%M:%S\") &lt;= \"06:00:00\") & \n      light == \"DAYLIGHT\", \n      na.rm = TRUE\n    )\n  )\n\n# check weather-surface consistency after cleaning\ncleaning_verification$weather_surface &lt;- drivers_data_clean %&gt;%\n  summarise(\n    weather_surface_mismatch_after = sum(\n      (weather %in% c(\"RAIN\", \"SNOW\") & surface_condition == \"DRY\") |\n      (weather == \"CLEAR\" & surface_condition %in% c(\"ICE\", \"SNOW\")), \n      na.rm = TRUE\n    )\n  )\n\n# check data quality scores distribution\ncleaning_verification$quality_scores &lt;- drivers_data_clean %&gt;%\n  group_by(data_quality_score) %&gt;%\n  summarise(\n    count = n(),\n    percentage = n() / nrow(drivers_data_clean) * 100,\n    .groups = 'drop'\n  ) %&gt;%\n  arrange(desc(data_quality_score))\n\ncat(\"\\n=== CLEANING VERIFICATION RESULTS ===\\n\")\nprint(cleaning_verification)\n\nFinal Cleaning Steps Based on Verification Results\n\n# handle remaining weather-surface mismatches (577 cases)\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # create a more detailed weather-surface relationship\n    surface_condition = case_when(\n      # if it's raining, surface should be wet\n      weather %in% c(\"RAIN\", \"RAINING\") ~ \"WET\",\n      \n      # if it's snowing, surface should be snow or ice\n      weather == \"SNOW\" ~ \"SNOW\",\n      \n      # if it's freezing rain, surface should be ice\n      weather == \"FREEZING RAIN\" ~ \"ICE\",\n      \n      # for clear weather, keep existing surface condition if reasonable\n      weather == \"CLEAR\" & surface_condition %in% c(\"DRY\", \"WET\") ~ surface_condition,\n      \n      # for cloudy weather, keep existing surface condition\n      weather == \"CLOUDY\" ~ surface_condition,\n      \n      # for other cases, mark as unknown if inconsistent\n      TRUE ~ case_when(\n        surface_condition %in% c(\"DRY\", \"WET\", \"SNOW\", \"ICE\") ~ surface_condition,\n        TRUE ~ \"UNKNOWN\"\n      )\n    )\n  )\n\n# add more detailed quality flags and improve quality score\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # create compound quality flags for multiple issues\n    detailed_quality_flags = map2_chr(\n      data_quality_flags,\n      surface_condition,\n      ~case_when(\n        .x != \"VALID\" & .y == \"UNKNOWN\" ~ paste(.x, \"WITH_UNKNOWN_SURFACE\"),\n        TRUE ~ .x\n      )\n    ),\n    \n    # create a more nuanced quality score (0-100)\n    refined_quality_score = case_when(\n      data_quality_flags == \"VALID\" & !is.na(surface_condition) & \n      surface_condition != \"UNKNOWN\" ~ 100,\n      \n      data_quality_flags == \"VALID\" & \n      (is.na(surface_condition) | surface_condition == \"UNKNOWN\") ~ 95,\n      \n      data_quality_flags == \"LIGHT_TIME_MISMATCH\" ~ 90,\n      \n      data_quality_flags == \"WEATHER_SURFACE_MISMATCH\" & \n      !is.na(surface_condition) ~ 85,\n      \n      data_quality_flags == \"WEATHER_SURFACE_MISMATCH\" & \n      is.na(surface_condition) ~ 80,\n      \n      data_quality_flags == \"INJURY_NO_DAMAGE_MISMATCH\" ~ 75,\n      \n      data_quality_flags == \"INVALID_COORDINATES\" ~ 70,\n      \n      data_quality_flags %in% c(\"FATAL_MINOR_DAMAGE_MISMATCH\", \n                               \"PARKED_MOVING_MISMATCH\") ~ 65,\n      \n      TRUE ~ 60\n    )\n  )\n\n# create final data quality summary\nfinal_quality_summary &lt;- drivers_data_clean %&gt;%\n  summarise(\n    total_records = n(),\n    high_quality_records = sum(refined_quality_score &gt;= 95),\n    medium_quality_records = sum(refined_quality_score &gt;= 80 & refined_quality_score &lt; 95),\n    low_quality_records = sum(refined_quality_score &lt; 80),\n    mean_quality_score = mean(refined_quality_score),\n    median_quality_score = median(refined_quality_score),\n    \n    # percentage calculations\n    high_quality_percentage = (high_quality_records / total_records) * 100,\n    medium_quality_percentage = (medium_quality_records / total_records) * 100,\n    low_quality_percentage = (low_quality_records / total_records) * 100\n  )\n\ncat(\"\\n=== FINAL DATA QUALITY SUMMARY ===\\n\")\nprint(final_quality_summary)\n\n# create recommendations for further improvements\ncat(\"\\n=== RECOMMENDATIONS FOR FURTHER IMPROVEMENTS ===\\n\")\ncat(\"1. Weather-Surface Condition Relationship:\\n\")\ncat(\"   - \", sum(drivers_data_clean$surface_condition == \"UNKNOWN\"), \n    \"records still have unknown surface conditions\\n\")\ncat(\"   - Consider adding temperature data for better ice/snow validation\\n\\n\")\n\ncat(\"2. Geographic Data Quality:\\n\")\ncat(\"   - \", sum(is.na(drivers_data_clean$latitude) | is.na(drivers_data_clean$longitude)),\n    \"records have missing or invalid coordinates\\n\")\ncat(\"   - Consider implementing address geocoding for missing coordinates\\n\\n\")\n\ncat(\"3. Vehicle Information:\\n\")\ncat(\"   - \", sum(is.na(drivers_data_clean$vehicle_make)), \"records with missing vehicle makes\\n\")\ncat(\"   - \", sum(is.na(drivers_data_clean$vehicle_model)), \"records with missing vehicle models\\n\")\ncat(\"   - Consider implementing VIN decoding for missing vehicle information\\n\\n\")\n\nprocessing_summary &lt;- list(\n  initial_issues = list(\n    light_time_mismatch = 9744,\n    weather_surface_mismatch = 702,\n    injury_damage_mismatch = 306,\n    invalid_coordinates = 26,\n    fatal_minor_damage = 1,\n    parked_moving = 1\n  ),\n  \n  final_status = list(\n    quality_summary = final_quality_summary,\n    remaining_weather_surface_mismatch = 577,\n    remaining_unknown_surface = sum(drivers_data_clean$surface_condition == \"UNKNOWN\", na.rm = TRUE)\n  )\n)\n\ncat(\"\\n=== DATA PROCESSING SUMMARY ===\\n\")\ncat(\"Initial Issues vs. Final Status:\\n\")\ncat(\"- Light-Time Mismatches: \", processing_summary$initial_issues$light_time_mismatch, \n    \" -&gt; 0\\n\")\ncat(\"- Weather-Surface Mismatches: \", processing_summary$initial_issues$weather_surface_mismatch,\n    \" -&gt; \", processing_summary$final_status$remaining_weather_surface_mismatch, \"\\n\")\n\nVisualizations.\n\ninstall.packages(\"viridis\")\n\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(patchwork)  # for combining plots\nlibrary(viridis)    # for colorblind-friendly palettes\nlibrary(lubridate)\n\n# crash Time Patterns\ntime_plot &lt;- drivers_data_clean %&gt;%\n  mutate(\n    hour = hour(crash_date/time),\n    weekday = wday(crash_date/time, label = TRUE),\n    month = month(crash_date/time, label = TRUE)\n  ) %&gt;%\n  ggplot(aes(x = hour)) +\n  geom_histogram(binwidth = 1, fill = \"steelblue\", color = \"white\") +\n  labs(title = \"Distribution of Crashes by Hour of Day\",\n       x = \"Hour of Day\",\n       y = \"Number of Crashes\") +\n  theme_minimal() +\n  scale_x_continuous(breaks = seq(0, 23, 2))\n\n# injury Severity by Vehicle Type\nseverity_vehicle_plot &lt;- drivers_data_clean %&gt;%\n  filter(!is.na(vehicle_body_type), !is.na(injury_severity)) %&gt;%\n  count(vehicle_body_type, injury_severity) %&gt;%\n  group_by(vehicle_body_type) %&gt;%\n  mutate(pct = n/sum(n) * 100) %&gt;%\n  ggplot(aes(x = reorder(vehicle_body_type, -pct), y = pct, fill = injury_severity)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(title = \"Injury Severity by Vehicle Type\",\n       x = \"Vehicle Type\",\n       y = \"Percentage\",\n       fill = \"Injury Severity\") +\n  theme_minimal() +\n  scale_fill_viridis_d()\n\n# weather Conditions and Crash Frequency\nweather_plot &lt;- drivers_data_clean %&gt;%\n  count(weather) %&gt;%\n  ggplot(aes(x = reorder(weather, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"darkgreen\") +\n  coord_flip() +\n  labs(title = \"Crashes by Weather Condition\",\n       x = \"Weather Condition\",\n       y = \"Number of Crashes\") +\n  theme_minimal()\n\n# geographic Distribution of Crashes\nmap_plot &lt;- drivers_data_clean %&gt;%\n  filter(!is.na(latitude), !is.na(longitude)) %&gt;%\n  ggplot(aes(x = longitude, y = latitude)) +\n  geom_point(alpha = 0.1, color = \"red\") +\n  labs(title = \"Geographic Distribution of Crashes\",\n       x = \"Longitude\",\n       y = \"Latitude\") +\n  theme_minimal()\n\n# collision Types\ncollision_plot &lt;- drivers_data_clean %&gt;%\n  count(collision_type) %&gt;%\n  mutate(pct = n/sum(n) * 100) %&gt;%\n  ggplot(aes(x = reorder(collision_type, -pct), y = pct)) +\n  geom_bar(stat = \"identity\", fill = \"orange\") +\n  coord_flip() +\n  labs(title = \"Distribution of Collision Types\",\n       x = \"Collision Type\",\n       y = \"Percentage\") +\n  theme_minimal()\n\n# speed Limit and Crash Severity\nspeed_severity_plot &lt;- drivers_data_clean %&gt;%\n  filter(!is.na(speed_limit), !is.na(injury_severity)) %&gt;%\n  ggplot(aes(x = factor(speed_limit), fill = injury_severity)) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"Injury Severity by Speed Limit\",\n       x = \"Speed Limit\",\n       y = \"Proportion\",\n       fill = \"Injury Severity\") +\n  theme_minimal() +\n  scale_fill_viridis_d()\n\n# vehicle Age Distribution\nvehicle_age_plot &lt;- drivers_data_clean %&gt;%\n  mutate(vehicle_age = year(crash_date/time) - vehicle_year) %&gt;%\n  filter(vehicle_age &gt;= 0, vehicle_age &lt;= 30) %&gt;%\n  ggplot(aes(x = vehicle_age)) +\n  geom_histogram(binwidth = 1, fill = \"purple\", color = \"white\") +\n  labs(title = \"Distribution of Vehicle Age\",\n       x = \"Vehicle Age (Years)\",\n       y = \"Count\") +\n  theme_minimal()\n\n# surface Condition and Weather Relationship\nsurface_weather_plot &lt;- drivers_data_clean %&gt;%\n  filter(!is.na(surface_condition), !is.na(weather)) %&gt;%\n  count(surface_condition, weather) %&gt;%\n  group_by(weather) %&gt;%\n  mutate(pct = n/sum(n) * 100) %&gt;%\n  ggplot(aes(x = weather, y = surface_condition, fill = pct)) +\n  geom_tile() +\n  scale_fill_viridis_c() +\n  labs(title = \"Surface Condition by Weather\",\n       x = \"Weather\",\n       y = \"Surface Condition\",\n       fill = \"Percentage\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n# combine plots using patchwork\nlayout &lt;- \"\nAABB\nCCDD\nEEFF\nGGHH\n\"\n\ncombined_plots &lt;- time_plot + severity_vehicle_plot + \n                 weather_plot + map_plot + \n                 collision_plot + speed_severity_plot + \n                 vehicle_age_plot + surface_weather_plot +\n                 plot_layout(design = layout)\n\nprint(combined_plots)\n\nggsave(\"crash_analysis_dashboard.pdf\", combined_plots, width = 20, height = 24)\n\nsummary_stats &lt;- list(\n  \n  time_stats = drivers_data_clean %&gt;%\n    mutate(\n      hour = hour(crash_date/time),\n      weekday = wday(crash_date/time, label = TRUE)\n    ) %&gt;%\n    summarise(\n      peak_hour = names(which.max(table(hour))),\n      weekend_crashes = mean(weekday %in% c(\"Sat\", \"Sun\")) * 100\n    ),\n  \n  severity_stats = drivers_data_clean %&gt;%\n    group_by(injury_severity) %&gt;%\n    summarise(\n      count = n(),\n      percentage = n() / nrow(drivers_data_clean) * 100\n    ),\n  \n  weather_stats = drivers_data_clean %&gt;%\n    group_by(weather) %&gt;%\n    summarise(\n      count = n(),\n      percentage = n() / nrow(drivers_data_clean) * 100\n    ),\n  \n  vehicle_stats = drivers_data_clean %&gt;%\n    summarise(\n      avg_vehicle_age = mean(year(crash_date/time) - vehicle_year, na.rm = TRUE),\n      most_common_make = names(which.max(table(vehicle_make)))\n    )\n)\n\nprint(summary_stats)\n\nSave the final cleaned dataset.\n\ncurrent_timestamp &lt;- format(Sys.time(), \"%Y%m%d_%H%M\")\n\n# create directories if they don't exist\ndir.create(\"cleaned_data\", showWarnings = FALSE)\ndir.create(\"data_documentation\", showWarnings = FALSE)\n\nstr_result &lt;- capture.output(str(drivers_data_clean))\nprint(\"Data structure:\")\nprint(str_result)\n\ncolumn_info &lt;- data.frame(\n  column_name = names(drivers_data_clean),\n  data_type = sapply(drivers_data_clean, function(x) class(x)[1]),  # Take first class if multiple\n  stringsAsFactors = FALSE\n)\n\ncolumn_info$description &lt;- sapply(column_info$column_name, function(col_name) {\n  case_when(\n    col_name == \"report_number\" ~ \"Unique report identifier\",\n    col_name == \"local_case_number\" ~ \"Local case number for the incident\",\n    col_name == \"agency_name\" ~ \"Name of reporting agency\",\n    col_name == \"acrs_report_type\" ~ \"Type of crash report\",\n    col_name == \"crash_date/time\" ~ \"Date and time of the crash\",\n    col_name == \"route_type\" ~ \"Type of route where crash occurred\",\n    col_name == \"road_name\" ~ \"Name of the road where crash occurred\",\n    col_name == \"cross_street_name\" ~ \"Name of the nearest cross-street\",\n    col_name == \"off_road_description\" ~ \"Description for off-road incidents\",\n    col_name == \"municipality\" ~ \"Municipality where crash occurred\",\n    col_name == \"related_non_motorist\" ~ \"Type of non-motorist involved\",\n    col_name == \"collision_type\" ~ \"Type of collision\",\n    col_name == \"weather\" ~ \"Weather conditions during crash\",\n    col_name == \"surface_condition\" ~ \"Road surface condition\",\n    col_name == \"light\" ~ \"Light conditions\",\n    col_name == \"traffic_control\" ~ \"Traffic control present\",\n    col_name == \"driver_substance_abuse\" ~ \"Driver substance abuse status\",\n    col_name == \"non_motorist_substance_abuse\" ~ \"Non-motorist substance abuse status\",\n    col_name == \"person_id\" ~ \"Unique identifier for person involved\",\n    col_name == \"driver_at_fault\" ~ \"Indicator if driver was at fault\",\n    col_name == \"injury_severity\" ~ \"Severity of injuries\",\n    col_name == \"circumstance\" ~ \"Contributing circumstances\",\n    col_name == \"driver_distracted_by\" ~ \"Driver distraction factors\",\n    col_name == \"drivers_license_state\" ~ \"State of driver's license\",\n    col_name == \"vehicle_id\" ~ \"Unique identifier for vehicle\",\n    col_name == \"vehicle_damage_extent\" ~ \"Extent of vehicle damage\",\n    col_name == \"vehicle_first_impact_location\" ~ \"Location of first impact on vehicle\",\n    col_name == \"vehicle_body_type\" ~ \"Type of vehicle body\",\n    col_name == \"vehicle_movement\" ~ \"Vehicle movement during crash\",\n    col_name == \"vehicle_going_dir\" ~ \"Direction vehicle was traveling\",\n    col_name == \"speed_limit\" ~ \"Posted speed limit\",\n    col_name == \"driverless_vehicle\" ~ \"Indicator if vehicle was driverless\",\n    col_name == \"parked_vehicle\" ~ \"Indicator if vehicle was parked\",\n    col_name == \"vehicle_year\" ~ \"Year of vehicle manufacture\",\n    col_name == \"vehicle_make\" ~ \"Vehicle manufacturer\",\n    col_name == \"vehicle_model\" ~ \"Vehicle model\",\n    col_name == \"latitude\" ~ \"Latitude of crash location\",\n    col_name == \"longitude\" ~ \"Longitude of crash location\",\n    col_name == \"location\" ~ \"Combined location coordinates\",\n    col_name == \"data_quality_flags\" ~ \"Data quality flags from cleaning process\",\n    col_name == \"detailed_quality_flags\" ~ \"Detailed quality flags from cleaning process\",\n    col_name == \"refined_quality_score\" ~ \"Numerical score indicating data quality\",\n    TRUE ~ paste(\"Description for\", col_name)  # Default description for any new columns\n  )\n})\n\ncolumn_info$example_values &lt;- sapply(drivers_data_clean, function(x) {\n  if (is.numeric(x)) {\n    paste(\"Range:\", min(x, na.rm = TRUE), \"to\", max(x, na.rm = TRUE))\n  } else {\n    unique_vals &lt;- unique(na.omit(x))\n    if (length(unique_vals) &gt; 5) {\n      paste(paste(unique_vals[1:5], collapse = \", \"), \"... and\", length(unique_vals) - 5, \"more values\")\n    } else {\n      paste(unique_vals, collapse = \", \")\n    }\n  }\n})\n\nwrite_csv(drivers_data_clean, \n          file = paste0(\"cleaned_data/crash_data_clean_\", current_timestamp, \".csv\"))\n\nsaveRDS(drivers_data_clean, \n        file = paste0(\"cleaned_data/crash_data_clean_\", current_timestamp, \".rds\"))\n\nwrite_csv(column_info, \n          file = paste0(\"data_documentation/data_dictionary_\", current_timestamp, \".csv\"))\n\nreadme_text &lt;- sprintf(\"\n# Crash Data Cleaning Documentation\n\n## Version Information\n- Date Created: %s\n- Original Records: 186,931\n- Final Records: %d\n\n## Data Quality Metrics\n- Total Records: %d\n- Records with Quality Flags: %d\n- Clean Records: %d\n\n## Files in this Package\n1. crash_data_clean_%s.csv - Main data file (CSV format)\n2. crash_data_clean_%s.rds - R data file (RDS format)\n3. data_dictionary_%s.csv - Data dictionary with column descriptions\n\n## Column Summary\nTotal number of columns: %d\nSee data dictionary file for detailed information about each column.\n\n## Data Quality Notes\n- Data has been cleaned and standardized\n- Quality flags have been added to mark potential issues\n- Missing values have been handled according to context\n- Inconsistent categories have been standardized\n\n## Usage Notes\n- Please refer to the data dictionary for column descriptions\n- Check quality flags before analysis\n- Some columns may contain standardized values\n\n## Contact\nFor questions about this dataset, please contact [Your Contact Information]\n\n## Last Updated\n%s\n\",\n    format(Sys.time(), \"%Y-%m-%d %H:%M:%S\"),\n    nrow(drivers_data_clean),\n    nrow(drivers_data_clean),\n    sum(drivers_data_clean$data_quality_flags != \"VALID\"),\n    sum(drivers_data_clean$data_quality_flags == \"VALID\"),\n    current_timestamp,\n    current_timestamp,\n    current_timestamp,\n    ncol(drivers_data_clean),\n    format(Sys.time(), \"%Y-%m-%d %H:%M:%S\")\n)\n\nwriteLines(readme_text, \n           paste0(\"data_documentation/README_\", current_timestamp, \".md\"))\n\nzip_files &lt;- c(\n    paste0(\"cleaned_data/crash_data_clean_\", current_timestamp, \".csv\"),\n    paste0(\"cleaned_data/crash_data_clean_\", current_timestamp, \".rds\"),\n    paste0(\"data_documentation/data_dictionary_\", current_timestamp, \".csv\"),\n    paste0(\"data_documentation/README_\", current_timestamp, \".md\")\n)\n\nzip(paste0(\"cleaned_data/crash_data_package_\", current_timestamp, \".zip\"),\n    files = zip_files)\n\ncat(\"\\nData saving complete!\\n\")\ncat(\"Files saved:\\n\")\ncat(\"1. CSV data file\\n\")\ncat(\"2. RDS data file\\n\")\ncat(\"3. Data dictionary\\n\")\ncat(\"4. README documentation\\n\")\ncat(\"5. Complete package (zip)\\n\")\ncat(\"\\nLocation: ./cleaned_data/ and ./data_documentation/\\n\")"
  },
  {
    "objectID": "dataset/2017_PoliceXCrash.html",
    "href": "dataset/2017_PoliceXCrash.html",
    "title": "2017_Mileage data with Crash Reports",
    "section": "",
    "text": "library(readxl)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(sf)\nlibrary(leaflet)\n\n\n# Assuming `crash_df` and PoPo are your data frames.\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(lubridate)\n\ncrash_data &lt;- readRDS(\"dataset/cleaned_dataset.rds\")\npopo_data &lt;- readRDS(\"dataset/Police_Reports.rds\")\n\ncol_crash &lt;- colnames(crash_data)\ncol_Popo &lt;- colnames(popo_data)\n\nprint(col_crash)\nprint(col_Popo)\n\n\n# Aggregate data\ncrash_count &lt;- as.data.frame(table(crash_data$Municipality))\ncolnames(crash_count) &lt;- c(\"City\", \"Crash_Count\")\n\nincident_count &lt;- as.data.frame(table(popo_data$City))\ncolnames(incident_count) &lt;- c(\"City\", \"Incident_Count\")\n\n# Merge data by city\nmerged_data &lt;- merge(crash_count, incident_count, by = \"City\", all = TRUE)\nmerged_data[is.na(merged_data)] &lt;- 0  # Replace NA with 0\n\n# Create the bar chart\nlibrary(ggplot2)\nggplot(merged_data, aes(x = City)) +\n  geom_bar(aes(y = Crash_Count, fill = \"Crashes\"), stat = \"identity\", position = \"dodge\") +\n  geom_bar(aes(y = Incident_Count, fill = \"Incidents\"), stat = \"identity\", position = \"dodge\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(title = \"Incident and Crash Counts by City\", x = \"City\", y = \"Count\", fill = \"Type\")\n\n\nlibrary(lubridate)\n\n# Define thresholds\ntime_threshold &lt;- hours(1)\ndistance_threshold &lt;- 500  # in meters\n\n\n# Extract longitude and latitude from the Location column\nlibrary(stringr)\n\npopo_data$Longitude &lt;- as.numeric(str_extract(popo_data$Location, \"-[0-9]+\\\\.[0-9]+\"))\npopo_data$Latitude &lt;- as.numeric(str_extract(popo_data$Location, \"(?&lt;= )[0-9]+\\\\.[0-9]+\"))\n\n# Check the extracted columns\nhead(popo_data)\n\n\n# Create heatmap data\nlibrary(ggplot2)\nlibrary(ggmap)\nlibrary(stringr)\n\n# Combine datasets for visualization\ncrash_data$type &lt;- \"Crash\"\npopo_data$type &lt;- \"Incident\"\n\n# Select only necessary columns\ncrash_locations &lt;- crash_data[, c(\"Latitude\", \"Longitude\", \"type\")]\nlocation_data &lt;- rbind(crash_locations, popo_data[, c(\"Latitude\", \"Longitude\", \"type\")])\n\nggplot(location_data, aes(x = Longitude, y = Latitude)) +\n  stat_density2d(aes(fill = after_stat(level), alpha = ..level..), geom = \"polygon\") +\n  scale_fill_gradient(low = \"blue\", high = \"red\") +\n  facet_wrap(~type) +\n  labs(title = \"Heatmap of Crash and Incident Locations\", x = \"Longitude\", y = \"Latitude\") +\n  theme_minimal()"
  },
  {
    "objectID": "big_picture.html",
    "href": "big_picture.html",
    "title": "Story and Question",
    "section": "",
    "text": "Potholes, cracks, and worn-out pavement—when we think about what causes traffic accidents, poor road conditions are often the first thing that comes to mind. But is the connection as straightforward as it seems?\nTo find out, we dug into the data to see how road quality, weather, and surface conditions shape the likelihood of accidents. What we found might surprise you. While pavement conditions do play a role, they’re far from the whole story.\nRoad Quality Isn’t Everything\nRoads are rated by the Pavement Condition Index (PCI), which measures their quality on a scale from terrible to perfect. If bad roads mean more accidents, we should see a clear relationship between PCI and accident counts.\nInstead? The connection is weak. A scatter plot with a trend line shows that while better roads slightly reduce accidents, the trend is subtle.\nWhat does this mean? Road conditions alone don’t explain much. Other factors—like weather and what’s on the road surface—may matter even more.\nWhen Weather and Wet Roads Take the Wheel\nHere’s where it gets interesting, wet surfaces (like roads after a storm) significantly increase accident counts.Rainy weather, on the other hand, seems to have the opposite effect—it actually reduces accidents.How does that make sense? It turns out drivers tend to slow down and be more careful when it’s raining, even though wet roads themselves are a major hazard.\nA multivariate visualization brings this dynamic to life, showing how weather and surface conditions interact with PCI to impact accidents.\nIt’s Not a Straight Line Complex Patterns in the Data\nWhen we examined the relationship between PCI and accidents further, a surprising pattern emerged. Instead of a simple straight-line relationship, we found a U-shaped trend.\nThis means accidents were more frequent at moderate PCI levels than at very high or very low PCI levels. Why? It’s hard to say for sure, but this might reflect how drivers behave or how other factors like traffic volume play a role.\nOur polynomial regression plot captures this complexity and adds a new layer to the story, road conditions affect accidents in more complicated ways than we expected.\nGet Behind the Wheel, Explore the Data Yourself\nWant to see these patterns for yourself? We’ve built an interactive dashboard where you can dig into the data.\nToggle between weather conditions to see how rain or wet roads impact accident trends. Compare the results of linear and nonlinear models to understand the bigger picture. It’s an easy way to explore what’s happening on our roads and what factors matter most.\nThe Big Takeaways: Better Roads Help—But Not Much\nWhile smoother pavement is linked to slightly fewer accidents, the effect is small. Weather and Wet Roads Matter More Wet roads increase accidents, while rainy weather may make drivers more cautious.\nComplex Relationships\nThe U-shaped trend between PCI and accidents shows how other factors—like driver behavior or traffic flow—play a big role. What This Means for Safer Roads\nOur findings point to a broader truth\nkeeping roads safe takes more than just paving over potholes. It means targeting the biggest risks, like managing wet-road hazards and encouraging safer driving during bad weather.\nBy understanding these dynamics, we can make smarter decisions—whether that’s where to invest in road improvements or how to educate drivers about staying safe in tough conditions.\nSo, the next time you drive on a cracked road or slow down in the rain, remember, every condition tells a story. And with the right insights, we can make that story a little safer for everyone.\n\nInteractive\n\n#| eval: false\n#| echo: false\n#| standalone: true\n#| engine: shiny \n#| viewerHeight: 650 # You will have to adjust this to fit everything\n\nif (!require(\"shiny\")) {\n  install.packages(\"shiny\")\n  library(shiny)\n}\n\nLoading required package: shiny\n\n\nWarning: package 'shiny' was built under R version 4.4.2\n\nlibrary(shiny)\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.4.2\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.4.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n✔ readr     2.1.5     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\noptions(\"readr.edition\" = 1) # keep this to ensure you can download the data\n\n# Define UI\nui &lt;- fluidPage(\n  titlePanel(\"Interactive Visualization with Shiny\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\n        inputId = \"column\",\n        label = \"Select a column to visualize:\",\n        choices = c(\"Speed.Limit\", \"Surface.Condition\", \"Injury.Severity\"),\n        selected = \"Speed.Limit\"\n      ),\n      conditionalPanel(\n        condition = \"input.column == 'Speed.Limit'\",\n        sliderInput(\"bins\", \"Number of bins:\", min = 1, max = 50, value = 30)\n      )\n    ),\n    mainPanel(\n      plotOutput(\"plot\")\n    )\n  )\n)\n\n# Define server logic\nserver &lt;- function(input, output) {\n  data &lt;- reactive({\n    # Replace this with your dataset loading logic\n    read_rds(here::here(\"dataset\", \"cleaned_dataset.rds\")) %&gt;%\n      mutate(\n        # Standardize Injury.Severity categories\n        Injury.Severity = case_when(\n          grepl(\"fatal\", Injury.Severity, ignore.case = TRUE) ~ \"Fatal Injury\",\n          grepl(\"serious\", Injury.Severity, ignore.case = TRUE) ~ \"Serious Injury\",\n          TRUE ~ Injury.Severity\n        )\n      )\n  })\n  \n  output$plot &lt;- renderPlot({\n    column_data &lt;- data()[[input$column]]\n    \n    if (is.numeric(column_data)) {\n      # Numeric column: render a histogram\n      ggplot(data(), aes_string(x = input$column)) +\n        geom_histogram(bins = input$bins, fill = \"red\", color = \"white\") +\n        theme_minimal() +\n        labs(title = paste(\"Distribution of\", input$column))\n    } else {\n      # Categorical column: render a bar chart with improved x-axis\n      ggplot(data(), aes_string(x = input$column)) +\n        geom_bar(fill = \"red\", color = \"white\") +\n        theme_minimal() +\n        theme(\n          axis.text.x = element_text(angle = 45, hjust = 1) # Rotate x-axis labels\n        ) +\n        labs(title = paste(\"Counts of\", input$column))\n    }\n  })\n}\n\n# Run the app\nshinyApp(ui = ui, server = server)\n\nShiny applications not supported in static R Markdown documents"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is a website for the final project for MA[46]15 Data Science with R by CRASH OUT\nThe members of this team are below. ======="
  },
  {
    "objectID": "about.html#our-team",
    "href": "about.html#our-team",
    "title": "About",
    "section": "Our Team",
    "text": "Our Team\n\nYifei Zhang\nMajor: Economics & Mathematics Home: China\n\n\nPrimah Muwanga\nMajor: Data Science Minor: Engineering Science Home: Uganda\n\n\nXiang Fu\nMajor: Data Science Minor: Statistics Home: China\n\n\nNora O’Neill\nMajor: Mathematics (Statistics Concentration) Minor: Business Home: Newton, Massachusetts\n\n\nTejas Kaur\nMajor: Economics and Mathematics Minor: Data Science Home: Jaipur, India\n\n\n\nAbout this Template.\nThis is based off of the standard Quarto website template from RStudio (2023.09.0 Build 463)."
  },
  {
    "objectID": "analysis.html#motivation-for-data-analysis",
    "href": "analysis.html#motivation-for-data-analysis",
    "title": "Analysis",
    "section": "Motivation for Data Analysis",
    "text": "Motivation for Data Analysis\nTraffic accidents arise from a complex interplay of factors, including infrastructure quality, environmental conditions, and driver behavior. One of the key variables of interest is the Pavement Condition Index (PCI), which quantifies pavement quality. While one might suspect that deteriorating pavement conditions could contribute to higher accident counts, the strength and nature of this relationship remain unclear. In addition, other factors—such as weather and surface conditions—may interact with PCI in ways that either amplify or mitigate its influence on accidents."
  },
  {
    "objectID": "analysis.html#questions-we-aim-to-answer",
    "href": "analysis.html#questions-we-aim-to-answer",
    "title": "Analysis",
    "section": "Questions We Aim to Answer",
    "text": "Questions We Aim to Answer\nIn this data analysis, our goal is to explore the impact of PCI (Pavement Condition Index) on the number of traffic accidents and assess the role of weather conditions (Weather) and surface conditions (Surface Condition) in this relationship. By constructing various regression models, including linear and polynomial models, we analyze whether the relationship between PCI and accident count is significant and whether it exhibits nonlinear trends. Additionally, we consider the effects of weather and surface conditions on accident count to gain a more comprehensive understanding of the issue. The specific questions to address are as follows:\n\nHow does the variation in PCI affect the number of traffic accidents? Is there a significant relationship between the two?\nIs the relationship between PCI and accident count linear or nonlinear? Can a polynomial model capture more complex patterns?\nHow do weather conditions (Weather) and surface conditions (Surface Condition) influence the relationship between PCI and accident count? Do they have a significant impact on the number of accidents?"
  },
  {
    "objectID": "analysis.html#context-and-illustrations-for-the-analysis",
    "href": "analysis.html#context-and-illustrations-for-the-analysis",
    "title": "Analysis",
    "section": "Context and Illustrations for the Analysis",
    "text": "Context and Illustrations for the Analysis\nAfter cleaning and merging the crash and roadway data, an initial exploratory analysis was conducted. One of the first steps was to visualize the relationship between PCI and accident count. The scatterplot below shows accident count versus PCI with a fitted trend line:\n\nThe plot reveals a slight negative correlation between PCI and accident count, but the overall impact appears weak. The trend line is relatively flat, and the data points are highly scattered. This result suggests that PCI alone has limited explanatory power for accident count, indicating that other factors may have a greater influence, which will be a key consideration in the model-building process."
  },
  {
    "objectID": "analysis.html#modeling-and-inference",
    "href": "analysis.html#modeling-and-inference",
    "title": "Analysis",
    "section": "Modeling and Inference",
    "text": "Modeling and Inference\n\nFormal Statistical Modeling Framework\nTo analyze the relationship between PCI and accident count, we began with a simple linear regression model:\n\\[\n\\text{Accidents} = \\beta_0 + \\beta_1(\\text{PCI}) + \\varepsilon.\n\\]\nHowever, the distribution of accident counts was skewed and included outliers. To address this, we applied a log-plus-one transformation:\n\\[\n\\log(1 + \\text{Accidents}) = \\beta_0 + \\beta_1(\\text{PCI}) + \\varepsilon.\n\\]\nThis transformation reduced the impact of outliers and brought the residuals closer to normality. Next, we incorporated weather and surface condition variables into the model:\n\\[\n\\log(1 + \\text{Accidents}) = \\beta_0 + \\beta_1(\\text{PCI}) + \\beta_2(\\text{Weather}) + \\beta_3(\\text{Surface.Condition}) + \\varepsilon.\n\\]\nFinally, to explore potential nonlinear relationships, a quadratic term for PCI was introduced:\n\\[\n\\log(1 + \\text{Accidents}) = \\beta_0 + \\beta_1(\\text{PCI}) + \\beta_2(\\text{PCI}^2) + \\beta_3(\\text{Weather}) + \\beta_4(\\text{Surface.Condition}) + \\varepsilon.\n\\]"
  },
  {
    "objectID": "analysis.html#ideas-and-techniques-for-choosing-predictors",
    "href": "analysis.html#ideas-and-techniques-for-choosing-predictors",
    "title": "Analysis",
    "section": "Ideas and Techniques for Choosing Predictors",
    "text": "Ideas and Techniques for Choosing Predictors\nIn the simple regression analysis, we noticed that the distribution of accident counts exhibited significant variability and outliers, which could negatively affect model fitting and result interpretation. To address this, We applied a logarithmic transformation (log1p) to the accident count data to reduce the impact of outliers, resulting in a smoother and closer-to-normal distribution.\nIn the linear regression analysis, we found that the linear relationship between PCI (Pavement Condition Index) and accident count was weak. The trend line was relatively flat, and the model’s goodness of fit was low, with an adjusted R² of only about 4.2%. This indicates that the linear model might not adequately capture the complexity of the relationship. Additionally, the data distribution suggested that the changes in accident counts across different PCI ranges might not follow a simple linear pattern, showing some nonlinear characteristics. To better capture this, We introduced a quadratic polynomial term for PCI (poly(PCI, 2)) to construct a nonlinear regression model. This approach modeled both first-order and second-order effects, revealing a U-shaped or inverted U-shaped trend in the influence of PCI on accident counts."
  },
  {
    "objectID": "analysis.html#modeling-results",
    "href": "analysis.html#modeling-results",
    "title": "Analysis",
    "section": "Modeling Results",
    "text": "Modeling Results\nUnivariate Linear Regression\nThe results of the univariate linear regression indicate a weak negative correlation between PCI (Pavement Condition Index) and accident count, with a slope of -0.037, which is statistically significant (p-value &lt; 0.001). However, the model’s explanatory power is extremely low, with an R² of only 0.0043, meaning PCI accounts for just 0.43% of the variation in accident count. Additionally, the residual standard error is high (8.019), reflecting a substantial prediction error. While the overall model is statistically significant (F-statistic p-value &lt; 0.001), its practical impact is minimal, suggesting the need for additional variables or nonlinear methods to improve explanatory power.\nAfter applying a logarithmic transformation to accident counts, the impact of outliers was reduced, but the relationship between PCI and the log-transformed accident count remained very weak. Scatterplots and linear regression trend lines showed almost no noticeable linear association, with the trend line being nearly flat, indicating that PCI has a limited influence on accident count.\nHere is the visualization:\n\nMultivariate Regression with Weather and Surface Conditions\nThe multivariate regression results revealed a significant negative correlation between PCI and accident count (coefficient: -0.0355), suggesting that better pavement conditions are associated with slightly fewer accidents. Weather and surface conditions also had significant effects on accident count. For instance, wet surfaces significantly increased accident counts, while rainy weather was associated with fewer accidents, potentially because drivers exercised more caution. Despite the statistical significance of these variables, the model’s goodness of fit remained low, with an adjusted R² of only 4.2%, indicating that accident count is likely influenced by many other unaccounted factors.\n\nNonlinear Regression with PCI Quadratic Term\nThe nonlinear regression results indicate that by including the quadratic polynomial term for PCI (poly(PCI, 2)), the nonlinear relationship between PCI and accident count was captured. The negative coefficient of the first-order term (-52.66172) and the positive coefficient of the second-order term (34.10442) suggest that the effect of PCI on accident count exhibits a U-shaped or inverted U-shaped trend, where accident count initially increases slightly with PCI and then decreases. Additionally, the significance and directionality of weather and surface conditions remain consistent; for example, rainy weather (WeatherRAINING) significantly reduces accident counts, while wet surfaces (Surface.ConditionWATER) significantly increase them.\n\nAlthough the model’s goodness of fit improved slightly (adjusted R² = 4.4%), and the residual standard error (7.857) showed minor improvement, these findings suggest that while the model captures some complexity, accident count is still heavily influenced by other unmodeled factors.\nHowever, it is important to note some uncertainty in our estimates and conclusions. On one hand, while the coefficients are statistically significant, the model’s fit remains low, indicating that these variables explain only a small portion of the variation in accident count. On the other hand, the scattered data distribution may make the results particularly sensitive to extreme values or low-frequency categories, which can affect the magnitude and direction of coefficients. Furthermore, the nonlinear relationship between PCI and accident count, such as the U-shaped or inverted U-shaped trend, might depend on sample characteristics and data distribution, requiring further validation and analysis to draw more reliable conclusions."
  },
  {
    "objectID": "analysis.html#additional-insights-from-highway-mileage-data-and-crime-data",
    "href": "analysis.html#additional-insights-from-highway-mileage-data-and-crime-data",
    "title": "Analysis",
    "section": "Additional Insights from Highway Mileage Data and Crime Data",
    "text": "Additional Insights from Highway Mileage Data and Crime Data\n\nHighway Mileage Data\nTo further contextualize these results, we examined crash distribution data from a highway mileage dataset. This supplementary visualization shows the total number of crashes across various route types and under different surface conditions:\n\nEach panel contains a set of bars, each bar corresponding to a specific route type. Common route types include County roads, Interstates, State roads (such as Maryland state highways), Municipal roads, Other Public Roadways, and Ramps. The height of each bar indicates the total number of crashes recorded in that year for the given route type. Within each bar, different colors represent distinct surface conditions at the time of the crash: DRY, ICE, N/A (not applicable), UNKNOWN, and WET.\nAt a glance, DRY conditions (red segments) dominate most of the bars in all years. This suggests that the majority of crashes occur on dry roads, regardless of route type or year. Other surface conditions, such as ICE (green) and WET (purple), appear less frequently, indicating that while inclement weather does contribute to some crashes, it is not as common as crashes on clear, dry pavement. The N/A and UNKNOWN categories (brown and blue segments) vary in proportion from year to year and route to route, indicating occasional gaps or uncertainties in data recording.\nAcross the years, Interstates and State highways often show some of the higher totals of crashes, perhaps due to greater traffic volume and speed. County and Municipal roads also see their share of crashes, though the counts can fluctuate from year to year. Ramps and Other Public Roadways tend to have fewer crashes in most observed years, reflected by relatively smaller bars. Throughout the six-year span, the general pattern remains consistent: higher crash frequencies on dry surfaces and on busier route types, with smaller contributions from icy or wet conditions and some periodic variability in less traveled road types.\n\n\nCrime Data\nObjective\nAfter exploring the pavement conditions’ impact on accident reporting, we analyzed an additional government data set on crime reporting in Montgomery County. We started off we a simple statistical thesis to guide our exploration:\n\nNull hypothesis: There is no relationship between daily crime count and daily crash reporting.\nAlternative hypothesis: There is a relationship between daily crime count and daily crash reporting.\n\nPreliminary Set-Up\nTo investigate our null hypothesis (and to decide whether we can reject or fail to) we set up both data sets for plotting. This included cleaning the Crime Reporting Data from DATA.GOV to filter out all other counties in Maryland that weren’t Montgomery. Then, to tallie the crime counts we based it on the crime’s start time (a column in the data set). To pull the start date and mutate a column onto the filtered crime data we adjusted the start date variable to be in the correct date-time format. Once we did this, we were able to create a new data frame that counted crimes that occurred on a specific day. Finally, we joined these two frames into a data set that showed all days that had been reported whether crime or crash, and counted each.\nAnalyzing a possible relationship:\nTo see if there is anything to this hypothesis or if we don’t have a strong enough claim we used a couple of different plots. We focused on using a simple linear regression model and then using its NQQ and Residual plots to gather evidence for our hypothesis. After plotting we can see the line is nearly horizontal, but there is a slight increase, meaning a possible positive association between our variables. Given that we can calculate the correlation to be about 0.0326, we can see there’s nearly no significant linear relationship between our variables. To further explore the possible relationship between crime and crash data in Montgomery, we ran the NQQ and residual plots to explore the variance, normality, and independence of the data.\nThe NQ-Q plot helped to show the quantiles graphed against the standardized quantiles, which helps to show a bit of skewness at the tails of our distribution. Additionally, looking at our Residual plot shows there to be a few outliers, but generally, we can summarize as the fitted values increase our variance decreases. Overall both of these plots are interesting but not necessarily valuable because we can’t see any huge significance in either. Additionally, we wouldn’t assume normality or homoscedasticity which eliminates parametric testing options.\nBut before we explore possible solutions to what seems to look like an insignificant relationship between crime counts and crash counts, we chose to look at a density plot to get a better visual understanding.\nDensity plot of crash counts by crime level:\n\n\n\nDensity Plot of Crash Counts by Crime Level\n\n\nThis density graph shows the different distributions of crashes on “High” versus “Low” crime count days. These thresholds are defined by whether the crime count for one day is above or below the median of total crime counts calculated. There seems to be a significant overlap between the distributions which means the range for crash reports is about the same given higher or lower crime counts. However, there is a slight shift to the right for the “High” distribution which agrees with our previous scatterplot.\nHypothesis Conclusion\nGiven the somewhat okay fit from our linear regression model, using the statistics we can obtain the beta coefficient and whether it seems significant.\n\nBeta is 0.2129, which suggests for each additional crime tallied, we get a slight increase in crashes\nOur pvalue is 0.0506 with a confidence of 0.95, it seems to be too close to the threshold to be significant\nR^2 shows only 0.1% of the variation in the daily crash count is explained by the daily crime count\nOur residual standard error of 17.56 indicates that a lot of the variability found in this relationship cannot be explained\nLastly with a F_stat (assuming our data is mostly normal) of 3.823 and a critical value of 1.96\n\nTherefore our group concludes that for this exploration of data, we fail to reject the null. This isn’t great as there’s not much we can conclude from the data itself in relation to the crash reporting.\nDiscussion of Results\nIf we focused solely on the Crime dataset, we could explore a possible relationship between crime and accident count in greater detail. However, the statistical analysis suggests a very small association between the variables. From a non-statistical perspective, one might hypothesize that increased crime could include incidents such as DUIs or car crashes involving distracted drivers. To investigate this further, it might be more appropriate to model the data using a Poisson distribution or logistic regression, given the nature of the variables."
  },
  {
    "objectID": "analysis.html#limitations",
    "href": "analysis.html#limitations",
    "title": "Analysis",
    "section": "Limitations",
    "text": "Limitations\nWhen analyzing the relationship between PCI (Pavement Condition Index) and accident count, it is important to consider certain assumptions and data limitations. The current model assumes a simple linear or nonlinear relationship between PCI and accident count, but the actual situation may be more complex. Additionally, accident counts may be influenced by temporal or spatial factors.\nThe model’s fit is not ideal, indicating that other significant factors (such as traffic volume, speed limits, or time of day) might have a greater impact on accident count. Moreover, the data distribution is quite dispersed, and some extreme values or low-frequency categories could significantly affect the results.\nData limitations also introduce uncertainty. The distribution of accident counts is highly scattered, and extreme values or rare categories (like certain surface conditions) may skew the results. The nonlinear relationship and the influence of weather and surface conditions may depend on sample characteristics or localized traffic patterns not accounted for here.\nFuture research could incorporate richer variables—such as detailed traffic volume metrics, time-of-day effects, driver demographics, and enforcement intensity—to better understand the multifaceted nature of traffic accidents. More sophisticated models and data sources might eventually tease out subtler influences of pavement quality and other environmental factors on roadway safety."
  },
  {
    "objectID": "analysis.html#conclusion",
    "href": "analysis.html#conclusion",
    "title": "Analysis",
    "section": "Conclusion",
    "text": "Conclusion\nThis analysis suggests that while PCI shows a statistically significant relationship with accident counts, its effect is weak and overshadowed by other factors. Even when accounting for weather and surface conditions, and allowing for nonlinear patterns, the explanatory power remains modest. The supplementary highway mileage visualization reinforces the notion that fundamental aspects like traffic volume, route usage, and driver behavior likely play more critical roles.\nIn summary, PCI is not a strong standalone predictor of accident frequency, and a more holistic, data-rich approach is needed to fully understand and mitigate traffic accidents."
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "======="
  },
  {
    "objectID": "data.html#pavement-condition-index",
    "href": "data.html#pavement-condition-index",
    "title": "Data Cleaning",
    "section": "Pavement Condition Index",
    "text": "Pavement Condition Index\nThe dataset, titled Pavement Condition Index, is provided by the Department of Transportation of Montgomery County, Maryland, and can be accessed through the Montgomery County open data portal at data.montgomerycountymd.gov. This dataset was created to analyze and report on the conditions of pavements across the county’s roadways.\n\nPurpose of Collection\n\nTo evaluate and monitor the pavement condition of all 5,200 lane miles of roadways in Montgomery County.\nSupport county transportation planning and maintenance activities by providing numerical condition scores.\n\nAttribution\n\nCollected and curated by the Montgomery County Department of Transportation.\nMaintained and updated biennially with metadata recently updated on June 11, 2023.\n\n\n\nData Files and Variables\nThe dataset includes the following variables, which provide details about pavement conditions. The most relevant variables are highlighted below:\n\nKey Variables\n\nPavement Condition Index (PCI):\n\nA numerical score between 1 and 100, representing the condition of the pavement.\nScore Categories:\n\n0-30: Poor condition.\n31-60: Fair condition.\n61-80: Good condition.\n81-100: Very good condition.\n\n\nPavement Distress Types:\n\nIncludes 19 types of distresses, such as cracking, potholes, environmental impacts, and utility cuts.\n\n\n\n\nAdditional Variables (Summarized):\n\nRoadway ID: Unique identifier for each roadway segment.\nStreet Name: Name of the street being evaluated.\nSegment Length: Length of the road segment in miles.\nLast Inspected Date: Date of the most recent condition assessment.\nRoad Category: Classifies roads (e.g., arterial, residential).\n\n\n\n\nAccess and Use Information\n\nThe dataset is publicly available and intended for use by the general public, researchers, and policymakers.\nIt is categorized as Non-Federal, with no specific license provided.\n\n\n\nAdditional Notes\n\nUpdate Frequency: The dataset is updated every two years to reflect changes in pavement conditions.\nLimitations: The dataset does not include real-time updates or historical comparisons across multiple update cycles."
  },
  {
    "objectID": "data.html#maryland-highway-mileage-reports",
    "href": "data.html#maryland-highway-mileage-reports",
    "title": "Data Cleaning",
    "section": "Maryland Highway Mileage Reports",
    "text": "Maryland Highway Mileage Reports\nThe dataset, titled Annual Highway Mileage Reports, is provided by the Maryland Department of Transportation State Highway Administration (MDOT SHA). This dataset combines comprehensive information about state, toll, county, and municipal roadway systems in Montgomery County, Maryland, focusing on three key report types: DSD9 (Detailed Surface Data), FCAVMT (Functional Class Annual Vehicle Miles Traveled), and FCMI (Functional Class Mileage Information) from 2017 to 2022.\n\nPurpose of Collection\n\nTo track and analyze highway infrastructure across different functional classifications in Montgomery County.\nSupport transportation planning and policy decisions by providing detailed metrics on road usage and capacity.\nMonitor changes in road infrastructure and vehicle miles traveled over time.\n\nAttribution\n\nCollected and maintained by the Maryland Department of Transportation State Highway Administration.\nRepresents official state records of highway infrastructure and usage.\n\n\n\nData Files and Variables\nThe dataset combines three main report types, each providing distinct insights into the county’s highway system:\n\nKey Components\n\nDSD9 (Detailed Surface Data):\n\nMainline mileage (total, divided, non-divided)\nAnnual vehicle miles\nLane mileage (auxiliary spurs, mainline, total)\nSquare yards of surfacing\n\nFCAVMT (Functional Class Annual Vehicle Miles Traveled):\n\nTracks vehicle miles traveled across different road classifications\nSeparates data into rural and urban categories\nIncludes various road types from Interstate to Local roads\n\nFCMI (Functional Class Mileage Information):\n\nDocuments road mileage by functional classification\nCovers both rural and urban road networks\nCategorizes roads from Interstate to Local systems\n\n\n\n\nClassification Variables\n\nRoad Categories:\n\nInterstate\nOther Freeways\nPrincipal Arterial\nMinor Arterial\nMajor Collector\nMinor Collector\nLocal Roads\n\n\nEach category is further subdivided into rural and urban designations.\n\n\n\nAccess and Use Information\n\nThe dataset represents official state records and is used for planning and administrative purposes.\nData is structured to support analysis of transportation infrastructure trends from 2017 to 2022.\n\n\n\nAdditional Notes\n\nTime Coverage: The dataset spans six years (2017-2022), providing recent historical context.\nGeographical Scope: Focuses specifically on Montgomery County, Maryland.\nIntegration: Combines three distinct report types to provide a comprehensive view of the county’s highway system.\nLimitations: Earlier years may have fewer data categories as reporting requirements expanded over time."
  },
  {
    "objectID": "data.html#montgomery-county-crime-dataset",
    "href": "data.html#montgomery-county-crime-dataset",
    "title": "Data Cleaning",
    "section": "Montgomery County Crime Dataset",
    "text": "Montgomery County Crime Dataset\nThe dataset, titled Crime, is provided by the Montgomery County Police Department and can be accessed through Montgomery County’s open data portal at dataMontgomery. This dataset contains detailed crime statistics and incident reports, utilizing the National Incident-Based Reporting System (NIBRS) classification standards.\nThe primary purpose of this dataset is to provide public access to comprehensive crime statistics in Montgomery County, supporting law enforcement analysis and transparency initiatives while enabling data-driven policy decisions regarding public safety. The data is collected and maintained by the Montgomery County Police Department and compiled using the EJustice records-management system. The data is updated daily, with metadata last updated on December 7, 2024.\n\nData Files and Variables\nThe dataset includes detailed information about reported crimes through various key components. The incident information includes unique identifiers such as Incident ID and CR Number, along with temporal data including dispatch date/time, and start and end date/time. Each incident is classified using NIBRS codes and can include up to three crime name categories per incident, along with offense codes and the number of victims involved.\nLocation data is extensively documented, including police district name and number, detailed block address components, city, state, and zip code information. Geographic coordinates are provided through latitude and longitude, and law enforcement jurisdictional information is included through sector, beat, and PRA designations.\n\n\nAccess and Use Information\nThe dataset is made available to the public through multiple format options, including CSV, RDF/XML, JSON, and XML. The data coverage begins from July 1st, 2016, and can be accessed through the Montgomery County open data portal. To maintain privacy standards, all personal information is excluded from the public dataset to protect victims’ privacy.\n\n\nAdditional Notes\nThe dataset is updated daily, with quarterly refreshes to reflect any changes in investigation status. Users should be aware of several important limitations: the information may be preliminary and subject to change, incidents may include attempted crimes alongside completed ones, and crime classifications may be updated as investigations progress. The data may contain mechanical or human errors, and while arrest information is included, all arrested persons are presumed innocent until proven guilty in a court of law.\nThis comprehensive dataset serves as a valuable resource for understanding crime patterns and law enforcement activities in Montgomery County, while maintaining appropriate privacy protections and data quality standards. The frequent updates and multiple format options make it a useful tool for researchers, policy makers, and the general public interested in local crime statistics and trends."
  },
  {
    "objectID": "data.html#data-cleaning-process",
    "href": "data.html#data-cleaning-process",
    "title": "Data Cleaning",
    "section": "Data Cleaning Process",
    "text": "Data Cleaning Process\nOur data cleaning process is implemented in our load_and_clean_data.R script. Here’s how to use it and what it does:\n\nSetup and Requirements\nFirst, ensure you have all required packages installed:\ninstall.packages(c(\"tidyverse\", \"lubridate\", \"skimr\", \"here\"))\n\n\nRunning the Cleaning Script\nThe script can be run in two ways:\n\n# Option 1: Source directly in your Quarto document\nsource(here::here(\"scripts\", \"load_and_clean_data.R\"), echo = TRUE)\n\n# Option 2: Load the already cleaned data\nclean_data &lt;- readRDS(here::here(\"dataset\", \"cleaned_dataset_full.rds\"))\n\n# Or load the exploration subset\nsubset_data &lt;- readRDS(here::here(\"dataset\", \"cleaned_dataset.rds\"))\n\n\n\nCleaning Operations\nOur cleaning script performs the following operations:\n\nRemoves columns with more than 80% missing values\nConverts the case number to proper character format\nStandardizes date/time formats using lubridate\nHandles missing values in the Route Type column\nRemoves duplicate entries"
  },
  {
    "objectID": "data.html#data-integration-process",
    "href": "data.html#data-integration-process",
    "title": "Data Cleaning",
    "section": "Data Integration Process",
    "text": "Data Integration Process\nWhen combining the datasets, we faced several challenges that required careful handling. The geographic alignment between the crash data and the PCI dataset was particularly challenging due to differences in road name formatting and segmentation. We developed a matching algorithm that standardized road names and used geographic coordinates to ensure accurate joining of the datasets, as shown in the following code:\n\n# Standardize road names in both datasets\nstandardize_road_names &lt;- function(road_name) {\n  road_name %&gt;%\n    str_to_upper() %&gt;%\n    str_replace_all(c(\n      \"AVENUE\" = \"AVE\",\n      \"BOULEVARD\" = \"BLVD\",\n      \"STREET\" = \"ST\",\n      \"ROAD\" = \"RD\",\n      \"HIGHWAY\" = \"HWY\"\n    )) %&gt;%\n    str_trim()\n}\n\n# Clean and prepare datasets for matching\ncrash_data &lt;- crash_data %&gt;%\n  mutate(\n    Road.Name = standardize_road_names(Road.Name),\n    location_key = paste(Road.Name, Municipality)\n  )\n\npci_data &lt;- pci_data %&gt;%\n  mutate(\n    StreetName = standardize_road_names(StreetName),\n    location_key = paste(StreetName, City)\n  )\n\n# Perform fuzzy matching for unmatched roads\nlibrary(stringdist)\nunmatched_roads &lt;- crash_data %&gt;%\n  filter(!location_key %in% pci_data$location_key)\n\n# Match using string distance with a threshold\nfuzzy_matches &lt;- stringdistmatrix(\n  unmatched_roads$Road.Name,\n  pci_data$StreetName,\n  method = \"jw\",\n  p = 0.1\n) %&gt;% as.data.frame()\n\nThe integration of temporal data presented another challenge, particularly when combining crash reports with annual highway data. We implemented a time-based joining strategy that matched crash incidents with the corresponding annual data, ensuring that we maintained the temporal integrity of our analysis:\n\n# Prepare temporal joins for highway data\ncrash_data &lt;- crash_data %&gt;%\n  mutate(\n    crash_year = year(Crash.Date.Time),\n    crash_month = month(Crash.Date.Time)\n  )\n\nhighway_data &lt;- highway_data %&gt;%\n  mutate(report_year = year(report_date))\n\n# Join datasets with temporal alignment\ncombined_data &lt;- crash_data %&gt;%\n  left_join(\n    highway_data,\n    by = c(\"crash_year\" = \"report_year\", \"Road.Name\" = \"highway_name\")\n  ) %&gt;%\n  # Handle cases where highway data is from previous year\n  group_by(Road.Name) %&gt;%\n  fill(c(traffic_volume, lane_width, surface_type), .direction = \"downup\") %&gt;%\n  ungroup()\n\n# Validate temporal alignment\ntemporal_coverage &lt;- combined_data %&gt;%\n  group_by(crash_year) %&gt;%\n  summarise(\n    crash_count = n(),\n    has_highway_data = sum(!is.na(traffic_volume)) / n()\n  )\n\nFor the highway mileage, we have several steps for data cleaning. Our approach began with the implementation of a sophisticated road classification system that addresses the varying naming conventions found across different reporting years. We developed a standardize_road_class function that employs regular expressions to match different descriptions of the same road types, creating a unified and consistent classification scheme throughout the dataset.\nThe handling of missing values formed a crucial component of our data cleaning process. We approached this challenge with context-specific solutions for different types of data. For instance, gaps in the Annual Average Daily Traffic (AADT) measurements were filled using median values to maintain statistical relevance, while missing lane mile data was conservatively set to zero. All other metric calculations incorporated robust aggregation methods to ensure accurate results even in the presence of incomplete data.\nTemporal alignment presented another critical aspect of our data preparation. We implemented proper date handling and year extraction procedures, which proved essential for matching highway infrastructure data with our crash records. This temporal standardization enables us to effectively track changes in road infrastructure over time and establish meaningful correlations with crash patterns. The alignment process ensures that our analysis captures the dynamic relationship between infrastructure changes and safety outcomes.\nOur quality validation process incorporated multiple layers of verification to ensure data integrity. We conducted thorough examinations of data completeness across different years, verified the consistency of measurements, analyzed the distribution of road types, and tracked year-over-year changes in key metrics.\n\n# Function to standardize road classifications across different reporting formats\nstandardize_road_class &lt;- function(road_class) {\n  case_when(\n    # Interstate highways (I-495, I-270, etc.)\n    str_detect(road_class, \"(?i)interstate|I-[0-9]\") ~ \"Interstate\",\n    \n    # Other controlled-access highways (MD 200)\n    str_detect(road_class, \"(?i)freeway|expressway|controlled\") ~ \"Other Freeways\",\n    \n    # Major arterial roads (Georgia Ave, Rockville Pike)\n    str_detect(road_class, \"(?i)principal.*arterial|major.*highway\") ~ \"Principal Arterial\",\n    \n    # Secondary arterial roads\n    str_detect(road_class, \"(?i)minor.*arterial|secondary\") ~ \"Minor Arterial\",\n    \n    # Major collector roads\n    str_detect(road_class, \"(?i)major.*collector|primary.*collector\") ~ \"Major Collector\",\n    \n    # Minor collector roads\n    str_detect(road_class, \"(?i)minor.*collector|secondary.*collector\") ~ \"Minor Collector\",\n    \n    # Local roads and streets\n    str_detect(road_class, \"(?i)local|residential|unclassified\") ~ \"Local Roads\",\n    \n    # Default classification for undefined categories\n    TRUE ~ \"Other\"\n  )\n}\n\n# Process and clean highway mileage data\nhighway_data &lt;- read_excel(\"Annual_Highway_Mileage.xlsx\") %&gt;%\n  # Clean column names to consistent format\n  janitor::clean_names() %&gt;%\n  \n  # Filter specifically for Montgomery County data\n  filter(county == \"Montgomery\") %&gt;%\n  \n  # Handle missing values in critical columns\n  mutate(\n    aadt = replace_na(aadt, median(aadt, na.rm = TRUE)),\n    lane_miles = replace_na(lane_miles, 0),\n    \n    # Create standardized classifications\n    road_class = standardize_road_class(functional_class),\n    urban_rural = if_else(str_detect(area_type, \"(?i)urban\"), \"Urban\", \"Rural\"),\n    \n    # Convert dates to proper format\n    report_date = mdy(report_date),\n    year = year(report_date)\n  ) %&gt;%\n  \n  # Calculate key metrics by road classification\n  group_by(road_class, urban_rural, year) %&gt;%\n  summarize(\n    # Total lane miles for each classification\n    total_miles = sum(lane_miles, na.rm = TRUE),\n    \n    # Average daily traffic (weighted by segment length)\n    avg_daily_traffic = weighted.mean(aadt, lane_miles, na.rm = TRUE),\n    \n    # Total vehicle miles traveled\n    vehicle_miles_traveled = sum(vmt, na.rm = TRUE),\n    \n    # Number of segments in each classification\n    segment_count = n(),\n    \n    # Additional metrics for analysis\n    avg_segment_length = mean(lane_miles, na.rm = TRUE),\n    total_intersections = sum(intersection_count, na.rm = TRUE)\n  ) %&gt;%\n  ungroup()\n\n# Data quality validation\nquality_check &lt;- highway_data %&gt;%\n  group_by(year) %&gt;%\n  summarize(\n    # Basic completeness checks\n    total_miles = sum(total_miles),\n    missing_traffic = sum(is.na(avg_daily_traffic)),\n    missing_vmt = sum(is.na(vehicle_miles_traveled)),\n    \n    # Consistency checks\n    min_segment_length = min(avg_segment_length),\n    max_segment_length = max(avg_segment_length),\n    \n    # Distribution checks\n    urban_ratio = sum(urban_rural == \"Urban\") / n(),\n    \n    # Year-over-year change\n    pct_change_miles = (total_miles - lag(total_miles)) / lag(total_miles) * 100\n  )\n\n# Save cleaned dataset with documentation\nwrite_rds(highway_data, \"cleaned_highway_data.rds\")\nwrite_csv(quality_check, \"highway_data_quality_report.csv\")"
  },
  {
    "objectID": "dataset/cleaning_for_crashes.html",
    "href": "dataset/cleaning_for_crashes.html",
    "title": "Data",
    "section": "",
    "text": "library(lubridate)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(skimr)\n\n\ncrash_df &lt;- read.csv(\"dataset-ignore/Crash_Reporting_Drivers_Data.csv\")\n\n# remove columns with a high percentage of missing values\n\n# Define threshold for column selection\nthreshold &lt;- 0.8\n\n# Remove columns with a high percentage of missing values\ndata_clean &lt;- crash_df |&gt;\n  select_if(~ mean(is.na(.)) &lt; threshold)\n# convert case number column to character format\n\ndata_clean$Local.Case.Number &lt;- as.character(data_clean$Local.Case.Number)\n\n# convert date and time column to datetime format\n\ndata_clean$Crash.Date.Time &lt;- mdy_hms(data_clean$Crash.Date.Time)\n\n\n# fill missing values in route type column with the most common value\n\nmost_common_route &lt;- data_clean$Route_Type |&gt; \n\n  na.omit() |&gt;\n\n  table() |&gt; \n\n  which.max()\n\n\n\ndata_clean$Route_Type[is.na(data_clean$Route_Type)] &lt;- names(most_common_route)\n\n\n\n# remove duplicate rows\n\ndata_clean &lt;- data_clean |&gt; distinct()\n\nsubset_data &lt;- data_clean %&gt;%\n  slice(1:1000) # Adjust based on your requirements\n\n\n# check the summary of cleaned data\n\nsummary(data_clean)\n\nsaveRDS(subset_data,\"dataset/cleaned_dataset.rds\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Road Real Talk: Crashes, Conditions, and Rescue",
    "section": "",
    "text": "Blog Post 7 - Wrap-Up: Traffic Safety Analysis in Montgomery County\n\n\n\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nDec 8, 2024\n\n\nGroup 1\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 6 - Data Integration and Modeling\n\n\n\n\n\n\n\n\n\n\n\nNov 18, 2024\n\n\nGroup 1\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 5 - Additional Datasets\n\n\n\n\n\n\n\n\n\n\n\nNov 11, 2024\n\n\nGroup 1\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 4 - EDA and Modeling Section 2\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2024\n\n\nGroup 1\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 4 - EDA and Modeling Section 1\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2024\n\n\nGroup 1\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Posts 3 - Data Cleaning, Loading, and Equity Considerations\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2024\n\n\nGroup 1\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 2 - Data Background\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2024\n\n\nGroup 1\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 1 - Choosing a Dataset\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2024\n\n\nGroup 1\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-10-21-blog-post/blog-02.html",
    "href": "posts/2024-10-21-blog-post/blog-02.html",
    "title": "Blog Post 2 - Data Background",
    "section": "",
    "text": "The data comes from the Automated Crash Reporting System (ACRS) of the Maryland State Police. It includes traffic collision reports from Montgomery County Police, Gaithersburg Police, Rockville Police, and the Maryland National Capital Park Police. The data was collected by the ACRS and local law enforcement agencies. The data was collected to document and analyze traffic collisions involving motor vehicle operators on county and local roads in Montgomery County. The purpose of this data collection might be to help enforcement agencies understand the circumstances of traffic collisions. It can be found on the original website, which is the Montgomery County government data site."
  },
  {
    "objectID": "posts/2024-10-21-blog-post/blog-02.html#where-does-the-data-come-from-who-collected-it-why-was-it-collected-are-you-able-to-find-the-data-from-the-original-source",
    "href": "posts/2024-10-21-blog-post/blog-02.html#where-does-the-data-come-from-who-collected-it-why-was-it-collected-are-you-able-to-find-the-data-from-the-original-source",
    "title": "Blog Post 2 - Data Background",
    "section": "",
    "text": "The data comes from the Automated Crash Reporting System (ACRS) of the Maryland State Police. It includes traffic collision reports from Montgomery County Police, Gaithersburg Police, Rockville Police, and the Maryland National Capital Park Police. The data was collected by the ACRS and local law enforcement agencies. The data was collected to document and analyze traffic collisions involving motor vehicle operators on county and local roads in Montgomery County. The purpose of this data collection might be to help enforcement agencies understand the circumstances of traffic collisions. It can be found on the original website, which is the Montgomery County government data site."
  },
  {
    "objectID": "posts/2024-10-21-blog-post/blog-02.html#are-there-any-issues-you-can-see-with-how-the-data-was-collected-what-is-the-sample-population-are-there-reasons-to-think-the-sample-is-biased-in-some-way",
    "href": "posts/2024-10-21-blog-post/blog-02.html#are-there-any-issues-you-can-see-with-how-the-data-was-collected-what-is-the-sample-population-are-there-reasons-to-think-the-sample-is-biased-in-some-way",
    "title": "Blog Post 2 - Data Background",
    "section": "Are there any issues you can see with how the data was collected? What is the sample population? Are there reasons to think the sample is biased in some way?",
    "text": "Are there any issues you can see with how the data was collected? What is the sample population? Are there reasons to think the sample is biased in some way?\nAccording to the information from the original government website providing the data, since the collision reports are based on preliminary information supplied to the Police Department by the reporting parties, there are some potential issues with the data collection process, including:\n\nInformation not yet verified by further investigation\nInformation that may include verified and unverified collision data\nPreliminary collision classifications may be changed at a later date based on further investigation\nInformation may include mechanical or human error\n\nThe sample population consists of motor vehicle operators involved in traffic collisions on county and local roadways within Montgomery County, Maryland. The dataset might be biased because it may not include minor collisions or unreported incidents."
  },
  {
    "objectID": "posts/2024-10-21-blog-post/blog-02.html#how-is-this-data-used-what-questions-have-others-ask-about-this-data",
    "href": "posts/2024-10-21-blog-post/blog-02.html#how-is-this-data-used-what-questions-have-others-ask-about-this-data",
    "title": "Blog Post 2 - Data Background",
    "section": "How is this data used? What questions have others ask about this data?",
    "text": "How is this data used? What questions have others ask about this data?\nThe data is widely used by government agencies, law enforcement, transportation planners, and other stakeholders who might be looking into improving road safety and traffic management. By analyzing the collision patterns, they can identify areas that might be prone to accidents and take corrective measures. The data set also includes information on driver at-fault status, substance abuse, and distractions providing insights into common behavioral causes of collisions. This can inform stricter law enforcement measures. Officials may also use this data to assess the effectiveness of traffic regulations. This data can also be used by insurance companies for underwriting and to assess claims involving collisions. Some questions that others might ask about the data are:\n\nWhich roads or intersections are leading to the highest collision rates and why? This will help identify accident hotspots that need infrastructural improvements\nWhat is the correlation between weather conditions and the rate of accidents? This can inform them on whether or not they need more safety warnings during hazardous weather conditions that might help people navigate these conditions better and potentially lead to fewer collisions\nWhat time of the day do most accidents occur? Understanding this better can help change street lighting or traffic patterns to make driving at night easier\nAre there certain car models that have more accidents than others? This could lead to a ban on certain kinds of vehicles or even help manufacturers fix flaws"
  },
  {
    "objectID": "posts/2024-10-21-blog-post/blog-02.html#has-there-been-other-research-on-the-same-data-is-this-data-being-used-for-some-policy-decisions",
    "href": "posts/2024-10-21-blog-post/blog-02.html#has-there-been-other-research-on-the-same-data-is-this-data-being-used-for-some-policy-decisions",
    "title": "Blog Post 2 - Data Background",
    "section": "Has there been other research on the same data? Is this data being used for some policy decisions?",
    "text": "Has there been other research on the same data? Is this data being used for some policy decisions?\nThis data provided by the state of New Jersey is being used by the New Jersey Department of Transportation (NJDOT) through a database called Safety Voyager. This tool allows them to utilize data like this set as well as similar datasets from other counties in the state, to visualize and analyze crash data. A dataset like ours is used in combination with the surrounding towns to identify areas of high risk. This data is also used to better safety improvement projects such as The Highway Safety Improvement Project (HSIP) which gathers this information from NJDOT to reduce crashes in crash-dense areas. Additionally, the New Jersey Transportation Planning Authority (NJTPA) also refers to information synthesized from our data to create overall safety examinations and better planning efforts. Along with NJDOT, HSIP, and NJTPA, the New Jersey Safety Outcomes and Data Warehouse (NJ-SHO) integrates crash data such as ours as well as health consequences to investigate public safety. Overall, it is mainly the state departments of New Jersey that are leading research projects with these crash reports, and creating programs and initiatives that will encourage future safety measures in the state."
  },
  {
    "objectID": "posts/2024-11-06-blog-post/blog-04-eda-xiang.html",
    "href": "posts/2024-11-06-blog-post/blog-04-eda-xiang.html",
    "title": "Blog Post 4 - EDA and Modeling Section 2",
    "section": "",
    "text": "Setting up the environment and loading the data.\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(viridis)\nlibrary(sf)\nlibrary(viridis)\nlibrary(scales)\nlibrary(RColorBrewer) \n\ncrash_data &lt;- readRDS(\"dataset/cleaned_dataset.rds\")\n\nglimpse(crash_data)\n\ncolnames(crash_data)\n\nsummary(crash_data)\n\ncolSums(is.na(crash_data))\n\nhead(crash_data)\n\nstr(crash_data)\n\n\ncrash_data &lt;- crash_data %&gt;%\n  mutate(Hour = hour(Crash.Date.Time))\n\nggplot(crash_data, aes(x = Hour)) +\n  geom_histogram(binwidth = 1, fill = \"steelblue\", alpha = 0.7) +\n  theme_minimal() +\n  labs(title = \"Distribution of Crashes by Hour of Day\",\n       x = \"Hour (24-hour format)\",\n       y = \"Number of Crashes\")\n\ndistraction_fault &lt;- crash_data %&gt;%\n  filter(Driver.Distracted.By != \"UNKNOWN\") %&gt;%\n  group_by(Driver.Distracted.By, Driver.At.Fault) %&gt;%\n  summarise(count = n(), .groups = 'drop') %&gt;%\n  pivot_wider(names_from = Driver.At.Fault, values_from = count, values_fill = 0)\n\ncrash_data %&gt;%\n  filter(Vehicle.Year &gt; 1950 & Vehicle.Year &lt; 2024) %&gt;%  # Remove unlikely years\n  mutate(Vehicle.Age = 2024 - Vehicle.Year) %&gt;%\n  ggplot(aes(x = Vehicle.Age)) +\n  geom_histogram(binwidth = 1, fill = \"darkred\", alpha = 0.7) +\n  theme_minimal() +\n  labs(title = \"Age Distribution of Vehicles Involved in Crashes\",\n       x = \"Vehicle Age (Years)\",\n       y = \"Count\")\n\nggplot(crash_data, aes(x = Longitude, y = Latitude)) +\n  geom_hex(bins = 30) +\n  scale_fill_viridis_c() +\n  theme_minimal() +\n  labs(title = \"Geographic Distribution of Crashes\",\n       fill = \"Crash Count\")\n\ntop_makes &lt;- crash_data %&gt;%\n  group_by(Vehicle.Make) %&gt;%\n  summarise(\n    crash_count = n(),\n    avg_speed_limit = mean(Speed.Limit)\n  ) %&gt;%\n  arrange(desc(crash_count)) %&gt;%\n  head(10)\n\nmovement_analysis &lt;- crash_data %&gt;%\n  group_by(Vehicle.Movement) %&gt;%\n  summarise(\n    crash_count = n(),\n    avg_speed = mean(Speed.Limit),\n    .groups = 'drop'\n  ) %&gt;%\n  arrange(desc(crash_count))\n\nlicense_state_analysis &lt;- crash_data %&gt;%\n  filter(Drivers.License.State != \"\") %&gt;%\n  group_by(Drivers.License.State) %&gt;%\n  summarise(\n    crash_count = n(),\n    at_fault_count = sum(Driver.At.Fault == \"Yes\"),\n    fault_rate = mean(Driver.At.Fault == \"Yes\"),\n    .groups = 'drop'\n  ) %&gt;%\n  arrange(desc(crash_count))\n\nImage 1 shows the temporal distribution of crashes throughout the day, revealing two distinct peak periods: a morning rush hour spike around 8-9 AM with approximately 65 crashes, and a more sustained afternoon/evening peak between 2-6 PM reaching up to 75 crashes per hour. The lowest crash frequencies occur during the early morning hours (3-5 AM) with fewer than 10 crashes, demonstrating the strong correlation between traffic volume and crash occurrence.\nImage 2 presents the age distribution of vehicles involved in crashes, showing a right-skewed distribution with a peak for vehicles between 5-7 years old (approximately 70 crashes). The frequency gradually declines for older vehicles, with a notable drop after 15 years and very few vehicles over 30 years old involved in crashes, likely reflecting both the general age distribution of vehicles on the road and potentially the retirement of older, less safe vehicles.\nImage 3 shows the geographic distribution of crashes using a hexagonal heatmap, with crash density indicated by color intensity. The visualization reveals several high-concentration areas (shown in yellow and green) clustered around latitude 39.1-39.2 and longitude -77.2, suggesting potential crash hotspots that might correspond to high-traffic intersections or challenging road configurations. The pattern appears to follow major transportation corridors, with crash density generally decreasing toward the periphery of the mapped area.\n\nvehicle_year_damage &lt;- crash_data %&gt;%\n  filter(Vehicle.Year &gt; 1950 & Vehicle.Year &lt; 2024) %&gt;%\n  group_by(Vehicle.Year) %&gt;%\n  summarise(\n    crash_count = n(),\n    severe_damage_rate = mean(Vehicle.Damage.Extent == \"DISABLING\"),\n    .groups = 'drop'\n  )\n\nggplot(vehicle_year_damage, aes(x = Vehicle.Year, y = severe_damage_rate)) +\n  geom_point() +\n  geom_smooth(method = \"loess\") +\n  theme_minimal() +\n  labs(title = \"Vehicle Age vs Severe Damage Rate\",\n       x = \"Vehicle Year\",\n       y = \"Rate of Severe Damage\")\n\nThe scatter plot reveals an interesting relationship between vehicle age and severe damage rates in crashes from 1990 to 2020. There’s a noticeable downward trend in severe damage rates from older to newer vehicles, with vehicles from the early 1990s showing the highest rates (around 60-65%) and a significant variability as indicated by some points reaching above 90%. The trend line shows a steady decline until approximately 2010, where it reaches its lowest point of about 35% severe damage rate, suggesting improvements in vehicle safety technology and construction over this period. However, there’s a slight uptick in severe damage rates for the newest vehicles (2015-2020), though this comes with wider confidence intervals as shown by the expanding grey band, possibly due to fewer data points or other contributing factors. The scattered points show considerable variation around the trend line, indicating that while vehicle age is a factor in crash severity, other variables likely play important roles in determining crash outcomes.\n\n# spatial clustering\n# distance-based clusters of crashes\nlibrary(stats)\ncoords &lt;- crash_data %&gt;%\n  select(Longitude, Latitude) %&gt;%\n  as.matrix()\n\n# hierarchical clustering\ndist_matrix &lt;- dist(coords)\nclusters &lt;- hclust(dist_matrix, method = \"complete\")\ncrash_data$cluster &lt;- cutree(clusters, k = 5)\n\nggplot(crash_data, aes(x = Longitude, y = Latitude, color = factor(cluster))) +\n  geom_point(alpha = 0.6) +\n  theme_minimal() +\n  labs(title = \"Spatial Clusters of Crashes\",\n       color = \"Cluster\")\n\nThis scatter plot shows the spatial distribution of crashes across different geographical coordinates, with clusters indicated by different colors. The data reveals distinct spatial patterns with five clearly separated clusters. The largest concentrations appear in clusters 1 (coral), 3 (green), and 4 (blue), forming a diagonal pattern from northeast to southwest. Cluster 3, located around longitude -77.3 and latitude 39.2, shows the highest density of crashes, while clusters 1 and 4 appear more spread out. Two smaller clusters are also visible: cluster 2 (olive) consists of a single point in the southwest corner, and cluster 5 (pink) shows a sparse distribution of crashes in the western portion of the map. The clear separation between clusters suggests that these crashes may be concentrated around specific road features, intersections, or high-traffic areas that could benefit from targeted safety interventions.\n\n# route type\nroute_analysis &lt;- crash_data %&gt;%\n  filter(Route.Type != \"\") %&gt;%\n  group_by(Route.Type) %&gt;%\n  summarise(\n    crash_count = n(),\n    injury_rate = mean(ACRS.Report.Type == \"Injury Crash\"),\n    avg_speed = mean(Speed.Limit),\n    most_common_collision = names(which.max(table(Collision.Type))),\n    .groups = 'drop'\n  )\n\nroute_analysis_long &lt;- route_analysis %&gt;%\n  # normalize crash_count and avg_speed to 0-1 scale for better comparison\n  mutate(\n    crash_count_norm = (crash_count - min(crash_count)) / (max(crash_count) - min(crash_count)),\n    avg_speed_norm = (avg_speed - min(avg_speed)) / (max(avg_speed) - min(avg_speed))\n  ) %&gt;%\n  # convert to long format for faceting\n  pivot_longer(\n    cols = c(crash_count_norm, injury_rate, avg_speed_norm),\n    names_to = \"metric\",\n    values_to = \"value\"\n  ) %&gt;%\n  mutate(\n    metric = case_when(\n      metric == \"crash_count_norm\" ~ \"Crash Count (Normalized)\",\n      metric == \"injury_rate\" ~ \"Injury Rate\",\n      metric == \"avg_speed_norm\" ~ \"Average Speed (Normalized)\"\n    )\n  )\n\nggplot(route_analysis_long, aes(x = reorder(Route.Type, value), y = value)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\", alpha = 0.8) +\n  geom_text(aes(label = sprintf(\"%.2f\", value)), \n            hjust = -0.1, \n            size = 3) +\n  facet_wrap(~metric, scales = \"free_y\", ncol = 1) +\n  coord_flip() +\n  theme_minimal() +\n  theme(\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank(),\n    axis.title.y = element_blank(),\n    strip.text = element_text(face = \"bold\", size = 10),\n    plot.title = element_text(face = \"bold\", size = 12),\n    plot.subtitle = element_text(size = 9, color = \"gray50\")\n  ) +\n  labs(\n    title = \"Route Type Analysis: Metrics Comparison\",\n    subtitle = \"Comparing crash frequency, injury rates, and average speed limits across different route types\",\n    y = \"Value\"\n  )\n\nWe can see some comparison of road safety metrics across different route types using normalized values. The data reveals some striking patterns: while Interstate routes have the highest normalized average speed (1.0) and ramps follow with relatively high speeds (0.61), Maryland State routes dominate in terms of crash frequency with a normalized count of 1.0, followed closely by County routes at 0.86, while other route types show significantly lower crash counts. Interestingly, when it comes to injury rates, Other Public Roadways show the highest rate at 0.44, followed by Maryland State routes at 0.38, suggesting that while State routes have more crashes overall, the likelihood of injury is actually higher on public roadways. This could indicate that while high-speed routes like Interstates have safety measures that effectively prevent injuries despite their speed, lower-speed but less controlled environments might pose a higher risk for severe outcomes when crashes do occur.\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(scales)\n\n# hourly summary\ncrash_summary &lt;- crash_data %&gt;%\n  mutate(Hour = hour(Crash.Date.Time)) %&gt;%\n  group_by(Hour) %&gt;%\n  summarise(\n    Total_Crashes = n(),\n    Not_Distracted = sum(Driver.Distracted.By == \"NOT DISTRACTED\"),\n    Distracted = sum(Driver.Distracted.By != \"NOT DISTRACTED\" & \n                    Driver.Distracted.By != \"UNKNOWN\"),\n    Unknown = sum(Driver.Distracted.By == \"UNKNOWN\")\n  ) %&gt;%\n  # convert to long format for line plotting\n  pivot_longer(\n    cols = c(Not_Distracted, Distracted, Unknown),\n    names_to = \"Distraction_Status\",\n    values_to = \"Count\"\n  )\n\nggplot(crash_summary, aes(x = Hour, y = Count, color = Distraction_Status)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 3) +\n  geom_text(data = crash_summary %&gt;%\n              group_by(Distraction_Status) %&gt;%\n              slice_max(Count, n = 1),\n            aes(label = Count),\n            vjust = -0.5,\n            size = 3.5) +\n  scale_color_manual(\n    values = c(\"Not_Distracted\" = \"#66C2A5\",\n              \"Distracted\" = \"#FC8D62\",\n              \"Unknown\" = \"#8DA0CB\"),\n    labels = c(\"Not Distracted\", \"Distracted\", \"Unknown Status\")\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23, 2),\n    labels = sprintf(\"%02d:00\", seq(0, 23, 2))\n  ) +\n  scale_y_continuous(\n    breaks = seq(0, max(crash_summary$Count), by = 10),\n    expand = c(0, 2)\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(color = \"gray30\", size = 11),\n    legend.position = \"right\",\n    legend.title = element_text(face = \"bold\"),\n    legend.text = element_text(size = 10),\n    axis.title = element_text(face = \"bold\"),\n    axis.text = element_text(size = 9),\n    panel.grid.minor = element_blank(),\n    legend.box.background = element_rect(color = \"gray80\", fill = \"white\")\n  ) +\n  labs(\n    title = \"Hourly Distribution of Crashes by Driver Distraction Status\",\n    subtitle = \"Showing patterns of distracted vs. non-distracted driving crashes\",\n    x = \"Time of Day\",\n    y = \"Number of Crashes\",\n    color = \"Driver\\nStatus\"\n  )\n\npeak_hours &lt;- crash_summary %&gt;%\n  group_by(Hour) %&gt;%\n  summarise(\n    Total = sum(Count),\n    .groups = 'drop'\n  ) %&gt;%\n  arrange(desc(Total)) %&gt;%\n  head(5)\n\nprint(\"Peak Crash Hours:\")\nprint(peak_hours)\n\nThis time series plot reveals a pattern in the hourly distribution of crashes categorized by driver distraction status. The data shows distinct peaks in distracted driving crashes, with the highest spike of 45 crashes occurring around 15:00 (3 PM), followed by significant peaks at 8:00 AM and 12:00 PM, suggesting a strong correlation with typical rush hour and lunch break periods. In contrast, non-distracted crashes and those with unknown status show more moderate fluctuations, with their highest peaks reaching only about half the magnitude of distracted crashes (24 and 22 crashes respectively) around 15:00-16:00. The overnight period from 22:00 to 04:00 shows consistently low crash numbers across all categories, indicating that despite lower traffic volumes, the proportion of distracted driving crashes remains relatively stable during these hours.\n\ngeo_movement_analysis &lt;- crash_data %&gt;%\n  # filter out any invalid coordinates and empty movement patterns\n  filter(!is.na(Latitude), !is.na(Longitude), \n         Vehicle.Movement != \"\", \n         Vehicle.Movement != \"UNKNOWN\") %&gt;%\n  mutate(\n    Movement_Category = case_when(\n      Vehicle.Movement %in% c(\"MOVING CONSTANT SPEED\", \"ACCELERATING\", \"SLOWING OR STOPPING\") ~ \"Normal Traffic Flow\",\n      Vehicle.Movement %in% c(\"PARKING\", \"PARKED\", \"BACKING\") ~ \"Parking Related\",\n      Vehicle.Movement %in% c(\"MAKING LEFT TURN\", \"MAKING RIGHT TURN\", \"U-TURN\") ~ \"Turning Movements\",\n      TRUE ~ \"Other Movements\"\n    )\n  )\n\nggplot(geo_movement_analysis) +\n  geom_hex(aes(x = Longitude, y = Latitude, fill = ..count..), \n           bins = 30) +\n  scale_fill_viridis(option = \"plasma\", name = \"Crash Count\") +\n  facet_wrap(~Movement_Category) +\n  theme_minimal() +\n  theme(\n    legend.position = \"right\",\n    plot.title = element_text(face = \"bold\"),\n    plot.subtitle = element_text(color = \"grey30\"),\n    strip.text = element_text(face = \"bold\")\n  ) +\n  labs(\n    title = \"Geographic Distribution of Crashes by Movement Type\",\n    subtitle = \"Spatial patterns of different vehicle movements leading to crashes\",\n    x = \"Longitude\",\n    y = \"Latitude\"\n  )\n\nhotspot_analysis &lt;- geo_movement_analysis %&gt;%\n  group_by(Movement_Category) %&gt;%\n  summarise(\n    crash_count = n(),\n    avg_speed_limit = mean(Speed.Limit),\n    injury_crashes = sum(ACRS.Report.Type == \"Injury Crash\"),\n    injury_rate = injury_crashes / n(),\n    .groups = 'drop'\n  ) %&gt;%\n  arrange(desc(crash_count))\n\nprint(hotspot_analysis)\n\nmovement_time_analysis &lt;- geo_movement_analysis %&gt;%\n  mutate(Hour = hour(Crash.Date.Time)) %&gt;%\n  group_by(Movement_Category, Hour) %&gt;%\n  summarise(\n    crash_count = n(),\n    .groups = 'drop'\n  )\n\nggplot(movement_time_analysis, \n       aes(x = Hour, y = crash_count, color = Movement_Category)) +\n  geom_line(size = 1) +\n  geom_point(size = 2) +\n  scale_x_continuous(breaks = 0:23) +\n  scale_color_viridis_d() +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    axis.text.x = element_text(angle = 45)\n  ) +\n  labs(\n    title = \"Crash Patterns Throughout the Day by Movement Type\",\n    x = \"Hour of Day\",\n    y = \"Number of Crashes\",\n    color = \"Movement Type\"\n  )\n\nThe temporal distribution of crashes by movement type shows a clear dominance of normal traffic flow incidents throughout the day, with two prominent peaks: one during the morning rush hour around 9:00 (39 crashes) and an even higher peak during the evening rush hour at 18:00 (45 crashes). Other movement types (turning movements, parking-related, and other movements) maintain relatively consistent but lower crash frequencies, typically ranging between 5-15 crashes per hour, with subtle increases during daylight hours. The pattern suggests that while regular traffic flow poses the highest crash risk, especially during peak commuting hours, the other movement types contribute a persistent but lower baseline of crash incidents, with turning movements showing slightly higher numbers than parking-related incidents during business hours, particularly between 13:00-17:00.\n\nvehicle_analysis &lt;- crash_data %&gt;%\n  # filter out invalid years and create vehicle age\n  filter(Vehicle.Year &gt; 1950 & Vehicle.Year &lt; 2024) %&gt;%\n  mutate(\n    Vehicle.Age = 2024 - Vehicle.Year,\n    Age.Category = case_when(\n      Vehicle.Age &lt;= 5 ~ \"New (0-5 years)\",\n      Vehicle.Age &lt;= 10 ~ \"Recent (6-10 years)\",\n      Vehicle.Age &lt;= 15 ~ \"Mature (11-15 years)\",\n      TRUE ~ \"Older (15+ years)\"\n    ),\n    Age.Category = factor(Age.Category, \n                         levels = c(\"New (0-5 years)\", \n                                  \"Recent (6-10 years)\", \n                                  \"Mature (11-15 years)\", \n                                  \"Older (15+ years)\"))\n  )\n\nggplot(vehicle_analysis, aes(x = Age.Category, fill = Vehicle.Damage.Extent)) +\n  geom_bar(position = \"fill\") +\n  scale_fill_viridis_d() +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"right\",\n    plot.title = element_text(face = \"bold\")\n  ) +\n  labs(\n    title = \"Vehicle Age Category vs. Damage Extent\",\n    subtitle = \"Distribution of damage severity across different vehicle age groups\",\n    x = \"Vehicle Age Category\",\n    y = \"Proportion of Crashes\",\n    fill = \"Damage Extent\"\n  )\n\ntop_manufacturers &lt;- vehicle_analysis %&gt;%\n  group_by(Vehicle.Make) %&gt;%\n  summarise(\n    total_crashes = n(),\n    severe_crashes = sum(Vehicle.Damage.Extent %in% c(\"DISABLING\", \"SEVERE\")),\n    severe_crash_rate = severe_crashes / total_crashes,\n    avg_vehicle_age = mean(Vehicle.Age),\n    fault_rate = mean(Driver.At.Fault == \"Yes\", na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  filter(total_crashes &gt;= 20) %&gt;%  # Filter for manufacturers with significant data\n  arrange(desc(total_crashes))\n\nggplot(head(top_manufacturers, 10), \n       aes(x = reorder(Vehicle.Make, total_crashes), \n           y = total_crashes)) +\n  geom_col(aes(fill = severe_crash_rate)) +\n  geom_text(aes(label = sprintf(\"%.1f%%\", severe_crash_rate * 100)),\n            hjust = -0.1,\n            size = 3) +\n  coord_flip() +\n  scale_fill_gradient(low = \"lightblue\", high = \"darkred\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    legend.position = \"right\"\n  ) +\n  labs(\n    title = \"Top 10 Vehicle Makes Involved in Crashes\",\n    subtitle = \"Showing total crashes and severe crash rate percentage\",\n    x = \"Vehicle Make\",\n    y = \"Total Number of Crashes\",\n    fill = \"Severe Crash Rate\"\n  )\n\nprint(\"Summary of Top Manufacturer Patterns\")\nmanufacturer_patterns &lt;- top_manufacturers %&gt;%\n  select(Vehicle.Make, total_crashes, severe_crash_rate, avg_vehicle_age, fault_rate) %&gt;%\n  mutate(\n    severe_crash_rate = percent(severe_crash_rate, accuracy = 0.1),\n    fault_rate = percent(fault_rate, accuracy = 0.1),\n    avg_vehicle_age = round(avg_vehicle_age, 1)\n  ) %&gt;%\n  arrange(desc(total_crashes))\n\nprint(manufacturer_patterns)\n\nWhile Toyota and Honda lead in total crash numbers with around 150 crashes each, they maintain relatively moderate severe crash rates of 41% and 40.7% respectively. Interestingly, some manufacturers with fewer total crashes show higher severity rates - KIA stands out with the highest severe crash rate at 48.3%, followed by Nissan at 46.9%, despite having significantly fewer total crashes. Chevrolet and Dodge, with the lowest crash counts among the top 10, also show the lowest severe crash rates at 25.9% and 24.0% respectively.\nFrom “Vehicle Age Category vs. Damage Extent”, we can see a clear relationship between vehicle age and damage severity in crashes. The proportion of disabling and destroyed vehicles notably increases with vehicle age, being most pronounced in the Older (15+ years) category. While superficial damage remains relatively consistent across age groups at around 25-30%, functional damage decreases with vehicle age, particularly in the oldest category. This suggests that older vehicles are more susceptible to severe damage in crashes, possibly due to their aging structural integrity and lack of modern safety features.\n\nvehicle_analysis &lt;- crash_data %&gt;%\n  filter(Vehicle.Year &gt; 1950 & Vehicle.Year &lt; 2024) %&gt;%\n  mutate(\n    Vehicle.Age = 2024 - Vehicle.Year,\n    Age.Category = case_when(\n      Vehicle.Age &lt;= 5 ~ \"New (0-5 years)\",\n      Vehicle.Age &lt;= 10 ~ \"Recent (6-10 years)\",\n      Vehicle.Age &lt;= 15 ~ \"Mature (11-15 years)\",\n      TRUE ~ \"Older (15+ years)\"\n    ),\n    Age.Category = factor(Age.Category, \n                         levels = c(\"New (0-5 years)\", \n                                  \"Recent (6-10 years)\", \n                                  \"Mature (11-15 years)\", \n                                  \"Older (15+ years)\"))\n  )\n\nggplot(vehicle_analysis, \n       aes(x = Age.Category, \n           fill = Vehicle.Damage.Extent)) +\n  geom_bar(position = \"dodge\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"right\",\n    plot.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    title = \"Vehicle Age Category vs. Damage Extent\",\n    subtitle = \"Number of crashes by damage type for each vehicle age group\",\n    x = \"Vehicle Age Category\",\n    y = \"Number of Crashes\",\n    fill = \"Damage Extent\"\n  ) +\n  geom_text(\n    aes(label = after_stat(count)),\n    stat = \"count\",\n    position = position_dodge(width = 0.9),\n    vjust = -0.5,\n    size = 3\n  )\n\nThis bar chart reveals the absolute numbers of crashes by damage type across vehicle age categories, providing a more detailed view of crash severity patterns. Recent vehicles (6-10 years) experience the highest number of disabling crashes at 116 incidents, followed by new vehicles (0-5 years) with 93 disabling crashes. Functional damage shows a declining trend with vehicle age, from 84 cases in recent vehicles to 40 cases in older vehicles (15+ years). Superficial damage follows a similar decreasing pattern, with the highest numbers in newer vehicles (67 cases) declining to 41 cases in older vehicles. Interestingly, while destroyed vehicles remain relatively low across all categories, there’s a slight increase in the older vehicle category with 17 cases, suggesting that while older vehicles may have fewer crashes overall, they’re more likely to be completely destroyed when they do crash.\n\ncrash_timing &lt;- crash_data %&gt;%\n  mutate(\n    Hour = hour(Crash.Date.Time),\n    Time_Block = case_when(\n      Hour &gt;= 5 & Hour &lt; 9 ~ \"Morning Peak (5-8)\",\n      Hour &gt;= 9 & Hour &lt; 16 ~ \"Mid-Day (9-15)\",\n      Hour &gt;= 16 & Hour &lt; 19 ~ \"Evening Peak (16-18)\",\n      Hour &gt;= 19 & Hour &lt; 22 ~ \"Evening (19-21)\",\n      TRUE ~ \"Night (22-4)\"\n    ),\n    Time_Block = factor(Time_Block, \n                       levels = c(\"Morning Peak (5-8)\", \n                                \"Mid-Day (9-15)\",\n                                \"Evening Peak (16-18)\", \n                                \"Evening (19-21)\",\n                                \"Night (22-4)\")),\n    Has_Injury = ACRS.Report.Type == \"Injury Crash\",\n    Light = factor(Light, levels = c(\"DAYLIGHT\", \n                                   \"DARK LIGHTS ON\",\n                                   \"DARK NO LIGHTS\",\n                                   \"DAWN\",\n                                   \"DUSK\",\n                                   \"DARK -- UNKNOWN LIGHTING\",\n                                   \"UNKNOWN\",\n                                   \"OTHER\",\n                                   \"N/A\"))\n  )\n\nggplot(crash_timing, aes(x = Hour)) +\n  geom_line(stat = \"count\", aes(color = Light), size = 1) +\n  scale_x_continuous(breaks = seq(0, 23, 2)) +\n  scale_color_brewer(palette = \"Set2\") +\n  scale_y_continuous(expand = c(0, 0)) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 0),\n    legend.position = \"right\",\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(color = \"gray30\", size = 11),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    legend.title = element_text(face = \"bold\"),\n    axis.title = element_text(face = \"bold\")\n  ) +\n  labs(\n    title = \"Crash Distribution Throughout the Day\",\n    subtitle = \"Showing crash frequency and lighting conditions for each hour\",\n    x = \"Hour of Day (24-hour format)\",\n    y = \"Number of Crashes\",\n    color = \"Light\\nCondition\"\n  )\n\nThe line graph shows the temporal distribution of crashes across different lighting conditions throughout a 24-hour period, revealing some interesting patterns in crash frequency. Daylight crashes show two prominent peaks: one during the morning rush hour around 8:00 (approximately 62 crashes) and a higher peak in the early afternoon around 14:00 (approximately 75 crashes), followed by a sharp decline as daylight fades. As expected, “Dark Lights On” crashes become prevalent during evening hours, showing a moderate peak around 19:00-20:00 (about 28 crashes), while maintaining relatively consistent numbers throughout the nighttime hours. Other lighting conditions, including dawn, dusk, and dark with no lights, show notably lower crash frequencies but exhibit small spikes during their respective natural occurrence periods - dawn crashes peak around 6:00-7:00, and dusk-related incidents show a minor increase around 17:00-18:00, corresponding to typical twilight hours.\n\ncrash_summary &lt;- crash_data %&gt;%\n  mutate(Hour = hour(Crash.Date.Time)) %&gt;%\n  group_by(Hour) %&gt;%\n  summarise(\n    Total_Crashes = n(),\n    Daylight_Crashes = sum(Light == \"DAYLIGHT\"),\n    Dark_Lights_On = sum(Light == \"DARK LIGHTS ON\"),\n    Dark_No_Lights = sum(Light == \"DARK NO LIGHTS\")\n  ) %&gt;%\n  pivot_longer(\n    cols = c(Daylight_Crashes, Dark_Lights_On, Dark_No_Lights),\n    names_to = \"Condition\",\n    values_to = \"Count\"\n  ) %&gt;%\n  mutate(\n    Condition = case_when(\n      Condition == \"Daylight_Crashes\" ~ \"Daylight\",\n      Condition == \"Dark_Lights_On\" ~ \"Dark (Lights On)\",\n      Condition == \"Dark_No_Lights\" ~ \"Dark (No Lights)\"\n    )\n  )\n\nggplot(crash_summary, aes(x = Hour, y = Count, color = Condition)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 3) +\n  scale_color_manual(\n    values = c(\"Daylight\" = \"#4E79A7\",\n              \"Dark (Lights On)\" = \"#F28E2B\",\n              \"Dark (No Lights)\" = \"#59A14F\")\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23, 2),\n    labels = sprintf(\"%02d:00\", seq(0, 23, 2))\n  ) +\n  scale_y_continuous(\n    breaks = seq(0, 80, 10),\n    expand = c(0, 2)\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(color = \"gray30\", size = 11),\n    legend.position = \"right\",\n    legend.title = element_text(face = \"bold\"),\n    legend.text = element_text(size = 10),\n    axis.title = element_text(face = \"bold\"),\n    axis.text = element_text(size = 9),\n    panel.grid.minor = element_blank(),\n    legend.box.background = element_rect(color = \"gray80\", fill = \"white\")\n  ) +\n  labs(\n    title = \"Hourly Distribution of Crashes by Light Condition\",\n    subtitle = \"Showing the three main lighting conditions throughout the day\",\n    x = \"Time of Day\",\n    y = \"Number of Crashes\",\n    color = \"Light Condition\"\n  )\n\nThe hourly distribution of crashes by light condition reveals distinct patterns that align with natural daylight cycles and peak traffic hours. Daylight crashes show two prominent peaks: a morning rush hour spike around 8:00 with approximately 62 crashes, and an even higher afternoon peak at 14:00 (2 PM) with about 75 crashes. As daylight fades in the evening hours after 16:00 (4 PM), there’s a clear transition where “Dark (Lights On)” crashes become predominant, reaching their peak of around 28 crashes during the evening commute hours. Crashes in “Dark (No Lights)” conditions remain consistently low throughout the 24-hour period, never exceeding 5 crashes per hour, suggesting that while these conditions are potentially hazardous, they occur less frequently or drivers may be more cautious in unlit conditions.\n\ndriver_analysis &lt;- crash_data %&gt;%\n  filter(\n    Driver.Distracted.By != \"UNKNOWN\",\n    Driver.Distracted.By != \"N/A\",\n    Vehicle.Damage.Extent != \"UNKNOWN\",\n    Vehicle.Damage.Extent != \"N/A\",\n    Driver.At.Fault != \"UNKNOWN\"\n  ) %&gt;%\n  mutate(\n    Distraction_Type = case_when(\n      Driver.Distracted.By == \"NOT DISTRACTED\" ~ \"Not Distracted\",\n      str_detect(Driver.Distracted.By, \"CELL|PHONE|TEXT\") ~ \"Phone Usage\",\n      str_detect(Driver.Distracted.By, \"EATING|DRINKING\") ~ \"Eating/Drinking\",\n      TRUE ~ \"Other Distraction\"\n    ),\n    Damage_Level = case_when(\n      Vehicle.Damage.Extent %in% c(\"DESTROYED\", \"DISABLING\") ~ \"Severe\",\n      Vehicle.Damage.Extent == \"FUNCTIONAL\" ~ \"Moderate\",\n      Vehicle.Damage.Extent %in% c(\"SUPERFICIAL\", \"NO DAMAGE\") ~ \"Minor\",\n      TRUE ~ \"Other\"\n    ),\n    At_Fault = Driver.At.Fault == \"Yes\"\n  )\n\ndistraction_summary &lt;- driver_analysis %&gt;%\n  group_by(Distraction_Type, Damage_Level) %&gt;%\n  summarise(\n    Total_Cases = n(),\n    Fault_Rate = mean(At_Fault),\n    .groups = 'drop'\n  ) %&gt;%\n  mutate(\n    Distraction_Type = factor(Distraction_Type, \n                            levels = c(\"Not Distracted\", \"Phone Usage\", \n                                     \"Eating/Drinking\", \"Other Distraction\")),\n    Damage_Level = factor(Damage_Level, \n                         levels = c(\"Minor\", \"Moderate\", \"Severe\"))\n  )\n\nggplot(distraction_summary, \n       aes(x = Distraction_Type, y = Total_Cases, fill = Damage_Level)) +\n  geom_col(position = \"dodge\", width = 0.7) +\n  geom_text(aes(label = sprintf(\"%.0f%%\", Fault_Rate * 100)),\n            position = position_dodge(width = 0.7),\n            vjust = -0.5,\n            size = 3.5,\n            fontface = \"bold\") +\n  scale_fill_manual(\n    values = c(\"Minor\" = \"#66C2A5\",\n              \"Moderate\" = \"#FC8D62\",\n              \"Severe\" = \"#8DA0CB\")\n  ) +\n  scale_y_continuous(\n    expand = expansion(mult = c(0, 0.1)),\n    breaks = seq(0, max(distraction_summary$Total_Cases), by = 50)\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(color = \"gray30\", size = 11),\n    axis.title = element_text(face = \"bold\"),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank(),\n    legend.position = \"right\",\n    legend.box.background = element_rect(color = \"gray80\", fill = \"white\")\n  ) +\n  labs(\n    title = \"Driver Distraction and Crash Severity Patterns\",\n    subtitle = \"Numbers above bars show percentage of at-fault cases\",\n    x = \"Type of Distraction\",\n    y = \"Number of Crashes\",\n    fill = \"Damage\\nSeverity\"\n  )\n\ndistraction_stats &lt;- driver_analysis %&gt;%\n  group_by(Distraction_Type) %&gt;%\n  summarise(\n    Total_Crashes = n(),\n    Fault_Rate = mean(At_Fault),\n    Severe_Damage_Rate = mean(Damage_Level == \"Severe\"),\n    .groups = 'drop'\n  ) %&gt;%\n  arrange(desc(Total_Crashes))\n\nprint(distraction_stats)\n\nThis bar chart reveals patterns in crash severity across different types of driver distraction, with percentages indicating at-fault cases for each category. Non-distracted driving shows a relatively balanced distribution of crash severities, but with notably lower at-fault rates (21-33%) compared to distracted driving scenarios. The most striking finding is that all distracted driving categories - phone usage, eating/drinking, and other distractions - show extremely high at-fault rates of 87-100%. Of particular concern is the “Other Distraction” category, which not only shows high numbers of crashes across all severity levels but also maintains extremely high at-fault rates (94-96%). Phone usage and eating/drinking, while showing fewer total incidents, result in 100% at-fault severe crashes, highlighting the serious dangers of these specific distraction types.\n\ngeo_analysis &lt;- crash_data %&gt;%\n  mutate(\n    Hour = hour(Crash.Date.Time),\n    Time_Category = case_when(\n      Hour &gt;= 5 & Hour &lt; 12 ~ \"Morning (5-11)\",\n      Hour &gt;= 12 & Hour &lt; 17 ~ \"Afternoon (12-16)\",\n      Hour &gt;= 17 & Hour &lt; 22 ~ \"Evening (17-21)\",\n      TRUE ~ \"Night (22-4)\"\n    ),\n    Is_Injury = ACRS.Report.Type == \"Injury Crash\"\n  ) %&gt;%\n  filter(!is.na(Latitude), !is.na(Longitude))\n\nggplot(geo_analysis, aes(x = Longitude, y = Latitude)) +\n  facet_wrap(~Time_Category, ncol = 2) +\n  geom_hex(bins = 20, aes(fill = ..count..)) +\n  scale_fill_viridis(\n    option = \"plasma\",\n    name = \"Number\\nof Crashes\",\n    labels = scales::number_format()\n  ) +\n  coord_cartesian(\n    xlim = c(min(geo_analysis$Longitude) + 0.05, \n             max(geo_analysis$Longitude) - 0.05),\n    ylim = c(min(geo_analysis$Latitude) + 0.05, \n             max(geo_analysis$Latitude) - 0.05)\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(color = \"gray30\", size = 11),\n    strip.text = element_text(face = \"bold\", size = 11),\n    legend.title = element_text(face = \"bold\"),\n    axis.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank(),\n    legend.position = \"right\",\n    legend.box.background = element_rect(color = \"gray80\", fill = \"white\")\n  ) +\n  labs(\n    title = \"Geographic Distribution of Crashes by Time of Day\",\n    subtitle = \"Hexagonal heatmap showing crash concentration patterns\",\n    x = \"Longitude\",\n    y = \"Latitude\"\n  )\n\narea_summary &lt;- geo_analysis %&gt;%\n  mutate(\n    Area = case_when(\n      Longitude &gt; median(Longitude) & Latitude &gt; median(Latitude) ~ \"Northeast\",\n      Longitude &lt;= median(Longitude) & Latitude &gt; median(Latitude) ~ \"Northwest\",\n      Longitude &gt; median(Longitude) & Latitude &lt;= median(Latitude) ~ \"Southeast\",\n      TRUE ~ \"Southwest\"\n    )\n  ) %&gt;%\n  group_by(Area, Time_Category) %&gt;%\n  summarise(\n    Total_Crashes = n(),\n    Injury_Rate = mean(Is_Injury),\n    .groups = 'drop'\n  )\n\nggplot(area_summary, \n       aes(x = Area, y = Total_Crashes, fill = Time_Category)) +\n  geom_col(position = \"dodge\", width = 0.7) +\n  geom_text(aes(label = sprintf(\"%.1f%%\", Injury_Rate * 100)),\n            position = position_dodge(width = 0.7),\n            vjust = -0.5,\n            size = 3.5) +\n  scale_fill_brewer(palette = \"Set2\") +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.title = element_text(face = \"bold\"),\n    legend.position = \"right\"\n  ) +\n  labs(\n    title = \"Crash Distribution by Area and Time\",\n    subtitle = \"Numbers show injury rates for each category\",\n    x = \"Area\",\n    y = \"Number of Crashes\",\n    fill = \"Time of Day\"\n  )\n\nFrom this, we try to show the crash patterns across different geographic areas and times of day, with injury rates displayed for each category. The Northwest area shows the highest overall crash numbers and notably high injury rates across all time periods, with evening crashes having a particularly high injury rate of 41.8%. The Southeast region experiences its highest crash volumes in the afternoon with a 24.5% injury rate, but shows a concerning spike in injury rates during night hours at 29.7%. Throughout all areas, night-time crashes (22-4) consistently show lower total numbers but varying injury rates, from 10% in the Southwest to 29.7% in the Southeast, suggesting that different areas face distinct night-time driving challenges. The Southwest area, while having fewer total crashes, maintains relatively high injury rates during afternoon (32.5%) and evening (37%) periods, indicating a need for targeted safety measures during these times.\n\nmovement_analysis &lt;- crash_data %&gt;%\n  filter(\n    Vehicle.Movement != \"\",\n    Vehicle.Movement != \"UNKNOWN\",\n    Weather != \"\",\n    Weather != \"UNKNOWN\",\n    Collision.Type != \"\",\n    Collision.Type != \"UNKNOWN\"\n  ) %&gt;%\n  mutate(\n    Movement_Type = case_when(\n      Vehicle.Movement %in% c(\"MAKING LEFT TURN\", \"MAKING RIGHT TURN\", \"U-TURN\") ~ \"Turning\",\n      Vehicle.Movement %in% c(\"MOVING CONSTANT SPEED\", \"ACCELERATING\") ~ \"Moving Forward\",\n      Vehicle.Movement %in% c(\"SLOWING OR STOPPING\", \"STOPPED IN TRAFFIC\") ~ \"Slowing/Stopped\",\n      Vehicle.Movement %in% c(\"PARKING\", \"PARKED\", \"BACKING\") ~ \"Parking Related\",\n      TRUE ~ \"Other\"\n    ),\n    Weather_Type = case_when(\n      Weather == \"CLEAR\" ~ \"Clear\",\n      Weather %in% c(\"CLOUDY\", \"FOGGY\") ~ \"Cloudy/Foggy\",\n      Weather %in% c(\"RAIN\", \"SLEET\", \"SNOW\") ~ \"Precipitation\",\n      TRUE ~ \"Other\"\n    )\n  )\n\nmovement_summary &lt;- movement_analysis %&gt;%\n  group_by(Movement_Type, Weather_Type) %&gt;%\n  summarise(\n    Crash_Count = n(),\n    Most_Common_Collision = names(which.max(table(Collision.Type))),\n    Collision_Percentage = max(table(Collision.Type)) / n(),\n    .groups = 'drop'\n  ) %&gt;%\n  group_by(Weather_Type) %&gt;%\n  mutate(\n    Weather_Total = sum(Crash_Count),\n    Percentage = Crash_Count / Weather_Total\n  ) %&gt;%\n  ungroup()\n\nggplot(movement_summary, \n       aes(x = Weather_Type, y = Crash_Count, fill = Movement_Type)) +\n  geom_col(position = \"fill\", width = 0.7) +\n  geom_text(aes(label = sprintf(\"%.1f%%\", Percentage * 100)),\n            position = position_fill(vjust = 0.5),\n            color = \"white\",\n            size = 3.5,\n            fontface = \"bold\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_y_continuous(labels = scales::percent) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(color = \"gray30\", size = 11),\n    axis.title = element_text(face = \"bold\"),\n    legend.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank(),\n    legend.position = \"right\",\n    legend.box.background = element_rect(color = \"gray80\", fill = \"white\")\n  ) +\n  labs(\n    title = \"Vehicle Movement Patterns Under Different Weather Conditions\",\n    subtitle = \"Showing the distribution of vehicle movements and their relative frequencies\",\n    x = \"Weather Condition\",\n    y = \"Proportion of Crashes\",\n    fill = \"Movement\\nType\"\n  )\n\ncollision_summary &lt;- movement_analysis %&gt;%\n  group_by(Movement_Type, Collision.Type) %&gt;%\n  summarise(\n    Count = n(),\n    .groups = 'drop'\n  ) %&gt;%\n  group_by(Movement_Type) %&gt;%\n  slice_max(Count, n = 2) %&gt;%\n  mutate(Percentage = Count / sum(Count))\n\nggplot(collision_summary,\n       aes(y = reorder(Movement_Type, Count), \n           x = Count, \n           fill = Collision.Type)) +\n  geom_col(position = \"dodge\") +\n  geom_text(aes(label = sprintf(\"%.0f%%\", Percentage * 100)),\n            position = position_dodge(width = 0.9),\n            hjust = -0.2,\n            size = 3.5) +\n  scale_fill_brewer(palette = \"Set3\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    axis.title = element_text(face = \"bold\"),\n    legend.title = element_text(face = \"bold\")\n  ) +\n  labs(\n    title = \"Most Common Collision Types by Vehicle Movement\",\n    subtitle = \"Numbers show percentage within each movement type\",\n    x = \"Number of Crashes\",\n    y = \"Movement Type\",\n    fill = \"Collision Type\"\n  )\n\nImage 1 reveals that moving forward is the predominant vehicle movement pattern across all weather conditions, but its proportion notably increases during precipitation (60%) compared to clear conditions (45.6%). During cloudy/foggy conditions, there’s a marked increase in slowing/stopped vehicles (20.9%) compared to clear weather (10.4%), suggesting drivers are more cautious in reduced visibility. Turning movements remain relatively consistent across weather conditions but show a slight increase during precipitation (20%) compared to other conditions.\nImage 2 shows distinct collision patterns for each movement type. Most notably, slowing/stopped vehicles are overwhelmingly involved in rear-end collisions (88%), highlighting the risks of sudden stops. For turning movements, straight movement angle collisions dominate (63%), followed by head-on left turn accidents (37%). Moving forward vehicles show a more balanced distribution between straight movement angle collisions (59%) and same direction rear-end crashes (41%), while parking-related incidents are predominantly classified as “other” types (88%), suggesting unique accident dynamics in parking scenarios.\n\nspeed_analysis &lt;- crash_data %&gt;%\n  mutate(\n    Hour = hour(Crash.Date.Time),\n    Time_Block = case_when(\n      Hour &gt;= 5 & Hour &lt; 10 ~ \"Morning Rush (5-9)\",\n      Hour &gt;= 10 & Hour &lt; 16 ~ \"Mid-Day (10-15)\",\n      Hour &gt;= 16 & Hour &lt; 20 ~ \"Evening Rush (16-19)\",\n      Hour &gt;= 20 | Hour &lt; 5 ~ \"Night (20-4)\"\n    ),\n    Speed_Category = case_when(\n      Speed.Limit &lt;= 25 ~ \"≤25 mph\",\n      Speed.Limit &lt;= 35 ~ \"26-35 mph\",\n      Speed.Limit &lt;= 45 ~ \"36-45 mph\",\n      TRUE ~ \"&gt;45 mph\"\n    ),\n    Speed_Category = factor(Speed_Category, \n                          levels = c(\"≤25 mph\", \"26-35 mph\", \n                                   \"36-45 mph\", \"&gt;45 mph\")),\n    Has_Injury = ACRS.Report.Type == \"Injury Crash\"\n  ) %&gt;%\n  filter(Speed.Limit &gt; 0)\n\nproportions_data &lt;- speed_analysis %&gt;%\n  group_by(Time_Block, Speed_Category, Has_Injury) %&gt;%\n  summarise(count = n(), .groups = 'drop') %&gt;%\n  group_by(Time_Block, Speed_Category) %&gt;%\n  mutate(\n    total = sum(count),\n    proportion = count/total,\n    label = sprintf(\"%.1f%%\", proportion * 100)\n  )\n\nggplot(proportions_data, \n       aes(x = Speed_Category, y = proportion, fill = Has_Injury)) +\n  facet_wrap(~Time_Block, ncol = 2) +\n  geom_col(position = \"stack\") +\n  geom_text(aes(label = label),\n            position = position_stack(vjust = 0.5),\n            size = 3.5,\n            color = \"white\") +\n  scale_fill_manual(\n    values = c(\"FALSE\" = \"#66C2A5\", \"TRUE\" = \"#FC8D62\"),\n    labels = c(\"No Injury\", \"Injury\"),\n    name = \"Crash\\nOutcome\"\n  ) +\n  scale_y_continuous(labels = scales::percent) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(color = \"gray30\", size = 11),\n    strip.text = element_text(face = \"bold\", size = 11),\n    axis.title = element_text(face = \"bold\"),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank(),\n    legend.position = \"right\",\n    legend.box.background = element_rect(color = \"gray80\", fill = \"white\")\n  ) +\n  labs(\n    title = \"Injury Rates by Speed Limit and Time of Day\",\n    subtitle = \"Showing the proportion of injury crashes across different conditions\",\n    x = \"Speed Limit Category\",\n    y = \"Proportion of Crashes\"\n  )\n\nspeed_summary &lt;- speed_analysis %&gt;%\n  group_by(Speed_Category) %&gt;%\n  summarise(\n    Total_Category_Crashes = n(),\n    Avg_Injury_Rate = mean(Has_Injury)\n  ) %&gt;%\n  arrange(desc(Total_Category_Crashes))\n\nggplot(speed_summary,\n       aes(x = Speed_Category, y = Avg_Injury_Rate)) +\n  geom_col(aes(fill = Avg_Injury_Rate), width = 0.7) +\n  geom_text(aes(label = sprintf(\"n=%d\", Total_Category_Crashes)),\n            vjust = -0.5,\n            size = 3.5) +\n  geom_text(aes(label = sprintf(\"%.1f%%\", Avg_Injury_Rate * 100)),\n            vjust = 1.5,\n            color = \"white\",\n            size = 3.5,\n            fontface = \"bold\") +\n  scale_fill_viridis(option = \"plasma\", guide = \"none\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    axis.title = element_text(face = \"bold\"),\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  ) +\n  labs(\n    title = \"Overall Injury Rates by Speed Limit\",\n    subtitle = \"Numbers above bars show total crash count\",\n    x = \"Speed Limit Category\",\n    y = \"Average Injury Rate\"\n  )\n\nprint(\"Summary Statistics by Speed Category:\")\nprint(speed_summary)\n\nImage 1 shows a somewhat complex relationship between injury rates, speed limits, and time of day. During evening rush hour (16-19), roads with 36-45 mph limits show the highest injury rate at 54.7%, while mid-day (10-15) demonstrates lower injury rates across all speed categories, with ≤25 mph zones having the lowest at 17.9%. Interestingly, high-speed zones (&gt;45 mph) show varying injury patterns, from 0% injuries during evening rush to 44.4% during night hours (20-4), suggesting that timing significantly influences crash severity in these zones.\nImage 2 provides a broader perspective on overall injury rates by speed limit, revealing that the 36-45 mph zones have both the highest injury rate (45.3%) and a substantial number of crashes (n=192). Despite expectations, the highest speed category (&gt;45 mph) shows a relatively low injury rate of 21.6%, though this may be influenced by its smaller sample size (n=51). The 26-35 mph zones represent the highest crash frequency (n=460) with a moderate injury rate of 37.2%, while the lowest speed zones maintain the lowest injury rate at 20.2% across a significant number of crashes (n=262).\n\ncollision_analysis &lt;- crash_data %&gt;%\n  filter(\n    Traffic.Control != \"UNKNOWN\",\n    Traffic.Control != \"N/A\",\n    Weather != \"UNKNOWN\",\n    Collision.Type != \"UNKNOWN\",\n    Collision.Type != \"OTHER\"\n  ) %&gt;%\n  mutate(\n    Control_Type = case_when(\n      str_detect(Traffic.Control, \"SIGNAL\") ~ \"Traffic Signal\",\n      str_detect(Traffic.Control, \"STOP\") ~ \"Stop Sign\",\n      str_detect(Traffic.Control, \"YIELD\") ~ \"Yield Sign\",\n      Traffic.Control == \"NO CONTROLS\" ~ \"No Controls\",\n      TRUE ~ \"Other Controls\"\n    ),\n    Weather_Condition = case_when(\n      Weather == \"CLEAR\" ~ \"Clear\",\n      Weather == \"CLOUDY\" ~ \"Cloudy\",\n      Weather %in% c(\"RAIN\", \"SNOW\", \"SLEET\") ~ \"Precipitation\",\n      TRUE ~ \"Other\"\n    ),\n    Collision_Category = case_when(\n      str_detect(Collision.Type, \"REAR-END\") ~ \"Rear End\",\n      str_detect(Collision.Type, \"ANGLE\") ~ \"Angle\",\n      str_detect(Collision.Type, \"SIDESWIPE\") ~ \"Sideswipe\",\n      str_detect(Collision.Type, \"HEAD ON\") ~ \"Head On\",\n      TRUE ~ \"Other\"\n    )\n  )\n\ncontrol_summary &lt;- collision_analysis %&gt;%\n  group_by(Control_Type, Weather_Condition) %&gt;%\n  summarise(\n    Total_Crashes = n(),\n    Injury_Rate = mean(ACRS.Report.Type == \"Injury Crash\"),\n    .groups = 'drop'\n  ) %&gt;%\n  group_by(Weather_Condition) %&gt;%\n  mutate(\n    Proportion = Total_Crashes / sum(Total_Crashes)\n  )\n\nplot1 &lt;- ggplot(control_summary, \n       aes(x = Weather_Condition, y = Proportion, fill = Control_Type)) +\n  geom_col(position = position_dodge(width = 0.9), width = 0.8) +\n  geom_text(aes(label = sprintf(\"%.1f%%\", Proportion * 100)),\n            position = position_dodge(width = 0.9),\n            vjust = -0.5,\n            size = 3.5,\n            fontface = \"bold\") +\n  geom_text(aes(label = sprintf(\"IR: %.1f%%\", Injury_Rate * 100)),\n            position = position_dodge(width = 0.9),\n            vjust = 1.5,\n            size = 3,\n            color = \"darkgray\") +\n  scale_fill_manual(values = c(\n    \"No Controls\" = \"#2E86C1\",\n    \"Other Controls\" = \"#F1C40F\",\n    \"Stop Sign\" = \"#E67E22\",\n    \"Traffic Signal\" = \"#27AE60\",\n    \"Yield Sign\" = \"#8E44AD\"\n  )) +\n  scale_y_continuous(\n    labels = scales::percent,\n    limits = c(0, 0.8),\n    breaks = seq(0, 0.8, 0.1)\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(color = \"gray30\", size = 12),\n    axis.title = element_text(face = \"bold\", size = 12),\n    axis.text = element_text(size = 10),\n    legend.title = element_text(face = \"bold\", size = 12),\n    legend.text = element_text(size = 10),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    legend.position = \"right\",\n    legend.box.background = element_rect(color = \"gray80\", fill = \"white\", linewidth = 0.5),\n    plot.margin = margin(t = 20, r = 20, b = 20, l = 20)\n  ) +\n  labs(\n    title = \"Traffic Control Effectiveness by Weather Condition\",\n    subtitle = \"Showing crash distribution (%) and injury rates (IR)\",\n    x = \"Weather Condition\",\n    y = \"Percentage of Crashes\",\n    fill = \"Traffic Control\\nType\"\n  )\n\ncollision_type_summary &lt;- collision_analysis %&gt;%\n  group_by(Control_Type, Collision_Category) %&gt;%\n  summarise(\n    Count = n(),\n    Injury_Count = sum(ACRS.Report.Type == \"Injury Crash\"),\n    Injury_Rate = Injury_Count / Count,\n    .groups = 'drop'\n  ) %&gt;%\n  group_by(Control_Type) %&gt;%\n  mutate(Proportion = Count / sum(Count))\n\nplot2 &lt;- ggplot(collision_type_summary, \n       aes(x = Control_Type, y = Count, fill = Collision_Category)) +\n  geom_col(position = position_dodge(width = 0.8), width = 0.7) +\n  geom_text(aes(label = sprintf(\"%.1f%%\", Injury_Rate * 100)),\n            position = position_dodge(width = 0.8),\n            vjust = -0.5,\n            size = 3.5,\n            fontface = \"bold\") +\n  scale_fill_manual(values = c(\n    \"Angle\" = \"#3498DB\",\n    \"Head On\" = \"#E74C3C\",\n    \"Rear End\" = \"#2ECC71\",\n    \"Sideswipe\" = \"#F1C40F\",\n    \"Other\" = \"#95A5A6\"\n  )) +\n  scale_y_continuous(\n    breaks = seq(0, 200, 25),\n    expand = expansion(mult = c(0, 0.15))\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(color = \"gray30\", size = 12),\n    axis.title = element_text(face = \"bold\", size = 12),\n    axis.text = element_text(size = 10),\n    axis.text.x = element_text(angle = 45, hjust = 1, face = \"bold\"),\n    legend.title = element_text(face = \"bold\", size = 12),\n    legend.text = element_text(size = 10),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    legend.position = \"right\",\n    legend.box.background = element_rect(color = \"gray80\", fill = \"white\", linewidth = 0.5),\n    plot.margin = margin(t = 20, r = 20, b = 40, l = 20)\n  ) +\n  labs(\n    title = \"Collision Types and Injury Rates by Traffic Control\",\n    subtitle = \"Numbers above bars show injury rates (%)\",\n    x = \"Traffic Control Type\",\n    y = \"Number of Crashes\",\n    fill = \"Type of\\nCollision\"\n  )\n\nprint(\"Summary by Traffic Control Type:\")\nprint(control_summary %&gt;% \n      group_by(Control_Type) %&gt;%\n      summarise(\n        Total_Crashes = sum(Total_Crashes),\n        Avg_Injury_Rate = mean(Injury_Rate),\n        .groups = 'drop'\n      ) %&gt;%\n      arrange(desc(Total_Crashes)))\n\nprint(plot1)\nprint(plot2)\n\nImage 1 reveals that traffic signals are the predominant control type across all weather conditions, but their effectiveness varies significantly. During precipitation, traffic signals account for 66.7% of crashes with a 50% injury rate, while no-control areas show a concerning 100% injury rate despite fewer crashes (33.3%). In clear conditions, traffic signals handle 48.3% of crashes with a 35.8% injury rate, while no-control areas see 38.7% of crashes with a 32% injury rate, suggesting better performance in good weather.\nImage 2 demonstrates that traffic signals experience the highest volume of crashes but show varying injury rates by collision type - angle collisions have a 48.9% injury rate while head-on collisions show 46.8%. Stop signs, while having fewer crashes, show a high injury rate for angle collisions (53.4%). Areas with no controls show relatively balanced injury rates across collision types (35-39%), except for sideswipes at 21.7%. Notably, yield signs have limited data but show a 50% injury rate, suggesting they may warrant closer monitoring for safety effectiveness.\n\nroad_analysis &lt;- crash_data %&gt;%\n  filter(\n    Route.Type != \"\",\n    !is.na(Speed.Limit),\n    Speed.Limit &gt; 0,\n    ACRS.Report.Type != \"UNKNOWN\"\n  ) %&gt;%\n  mutate(\n    Road_Category = case_when(\n      str_detect(Route.Type, \"Interstate\") ~ \"Interstate\",\n      str_detect(Route.Type, \"State|Maryland\") ~ \"State Road\",\n      str_detect(Route.Type, \"County\") ~ \"County Road\",\n      str_detect(Route.Type, \"Municipal\") ~ \"Municipal Road\",\n      TRUE ~ \"Other\"\n    ),\n    Speed_Category = case_when(\n      Speed.Limit &lt;= 25 ~ \"Low (≤25)\",\n      Speed.Limit &lt;= 35 ~ \"Medium (26-35)\",\n      Speed.Limit &lt;= 45 ~ \"High (36-45)\",\n      TRUE ~ \"Very High (&gt;45)\"\n    ),\n    Speed_Category = factor(Speed_Category, \n                          levels = c(\"Low (≤25)\", \"Medium (26-35)\", \n                                   \"High (36-45)\", \"Very High (&gt;45)\")),\n    Severity = case_when(\n      ACRS.Report.Type == \"Property Damage Crash\" ~ \"Property Damage\",\n      ACRS.Report.Type == \"Injury Crash\" ~ \"Injury\",\n      TRUE ~ \"Other\"\n    )\n  )\n\nroad_summary &lt;- road_analysis %&gt;%\n  group_by(Road_Category, Speed_Category) %&gt;%\n  summarise(\n    Total_Crashes = n(),\n    Injury_Count = sum(Severity == \"Injury\"),\n    Injury_Rate = Injury_Count / Total_Crashes,\n    Avg_Speed = mean(Speed.Limit),\n    .groups = 'drop'\n  )\n\nplot1 &lt;- ggplot(road_summary, \n       aes(x = Speed_Category, y = Total_Crashes, fill = Road_Category)) +\n  geom_col(position = \"dodge\", width = 0.8) +\n  geom_text(aes(label = Total_Crashes),\n            position = position_dodge(width = 0.8),\n            vjust = -0.5,\n            size = 3.5,\n            fontface = \"bold\") +\n  geom_text(aes(label = sprintf(\"IR: %.1f%%\", Injury_Rate * 100)),\n            position = position_dodge(width = 0.8),\n            vjust = 1.5,\n            size = 3,\n            color = \"darkgray\") +\n  scale_fill_manual(values = c(\n    \"Interstate\" = \"#2980B9\",\n    \"State Road\" = \"#27AE60\",\n    \"County Road\" = \"#F1C40F\",\n    \"Municipal Road\" = \"#E67E22\",\n    \"Other\" = \"#95A5A6\"\n  )) +\n  scale_y_continuous(\n    expand = expansion(mult = c(0, 0.2)),\n    breaks = seq(0, max(road_summary$Total_Crashes), by = 25)\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(color = \"gray30\", size = 12),\n    axis.title = element_text(face = \"bold\", size = 12),\n    axis.text = element_text(size = 10),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.title = element_text(face = \"bold\", size = 12),\n    legend.text = element_text(size = 10),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    legend.position = \"right\",\n    legend.box.background = element_rect(color = \"gray80\", fill = \"white\", linewidth = 0.5),\n    plot.margin = margin(t = 20, r = 20, b = 40, l = 20)\n  ) +\n  labs(\n    title = \"Crash Distribution by Road Type and Speed Limit\",\n    subtitle = \"Showing crash counts and injury rates (IR)\",\n    x = \"Speed Limit Category\",\n    y = \"Number of Crashes\",\n    fill = \"Road Type\"\n  )\n\ntime_road_analysis &lt;- road_analysis %&gt;%\n  mutate(\n    Hour = hour(Crash.Date.Time),\n    Time_Category = case_when(\n      Hour &gt;= 5 & Hour &lt; 10 ~ \"Morning Rush (5-9)\",\n      Hour &gt;= 10 & Hour &lt; 16 ~ \"Mid-Day (10-15)\",\n      Hour &gt;= 16 & Hour &lt; 20 ~ \"Evening Rush (16-19)\",\n      TRUE ~ \"Night (20-4)\"\n    )\n  ) %&gt;%\n  group_by(Road_Category, Time_Category) %&gt;%\n  summarise(\n    Crashes = n(),\n    Injury_Rate = mean(Severity == \"Injury\"),\n    .groups = 'drop'\n  )\n\nplot2 &lt;- ggplot(time_road_analysis, \n       aes(x = Road_Category, y = Crashes, fill = Time_Category)) +\n  geom_col(position = \"dodge\", width = 0.8) +\n  geom_text(aes(label = Crashes),\n            position = position_dodge(width = 0.8),\n            vjust = -0.5,\n            size = 3.5,\n            fontface = \"bold\") +\n  geom_text(aes(label = sprintf(\"%.1f%%\", Injury_Rate * 100)),\n            position = position_dodge(width = 0.8),\n            vjust = 1.5,\n            size = 3,\n            color = \"darkgray\") +\n  scale_fill_manual(values = c(\n    \"Morning Rush (5-9)\" = \"#3498DB\",\n    \"Mid-Day (10-15)\" = \"#2ECC71\",\n    \"Evening Rush (16-19)\" = \"#F1C40F\",\n    \"Night (20-4)\" = \"#95A5A6\"\n  )) +\n  scale_y_continuous(\n    expand = expansion(mult = c(0, 0.2)),\n    breaks = seq(0, max(time_road_analysis$Crashes), by = 25)\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(color = \"gray30\", size = 12),\n    axis.title = element_text(face = \"bold\", size = 12),\n    axis.text = element_text(size = 10),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.title = element_text(face = \"bold\", size = 12),\n    legend.text = element_text(size = 10),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    legend.position = \"right\",\n    legend.box.background = element_rect(color = \"gray80\", fill = \"white\", linewidth = 0.5),\n    plot.margin = margin(t = 20, r = 20, b = 40, l = 20)\n  ) +\n  labs(\n    title = \"Crash Distribution by Road Type and Time of Day\",\n    subtitle = \"Showing crash counts and injury rates (%)\",\n    x = \"Road Type\",\n    y = \"Number of Crashes\",\n    fill = \"Time Period\"\n  )\n\nprint(\"Summary by Road Type:\")\nprint(road_analysis %&gt;% \n      group_by(Road_Category) %&gt;%\n      summarise(\n        Total_Crashes = n(),\n        Avg_Injury_Rate = mean(Severity == \"Injury\"),\n        Avg_Speed_Limit = mean(Speed.Limit),\n        .groups = 'drop'\n      ) %&gt;%\n      arrange(desc(Total_Crashes)))\n\nprint(plot1)\nprint(plot2)\n\nImage 1 shows that medium-speed roads (26-35 mph) experience the highest number of crashes, with State Roads leading at 241 crashes and a 36.1% injury rate, followed by County Roads with 179 crashes and a 39.1% injury rate. High-speed roads (36-45 mph) show elevated injury rates, particularly on State Roads (43.5%) and County Roads (49.1%), despite lower crash volumes. Interestingly, very high-speed roads (&gt;45 mph) show relatively few crashes but maintain significant injury rates, with State Roads showing a 30.8% injury rate across 26 incidents.\nImage 2 reveals distinct temporal patterns across road types, with State Roads experiencing the highest crash volumes during mid-day (153 crashes, 35.3% injury rate) and evening rush (113 crashes, 38.9% injury rate). County Roads show a similar pattern but with lower volumes, peaking during mid-day (124 crashes, 29% injury rate). Municipal Roads and Interstates have considerably fewer crashes but show varying injury rates across time periods, with Municipal Roads experiencing higher injury rates during morning rush hours (21.4%). Night-time crashes show generally lower volumes across all road types but maintain concerning injury rates, particularly on State Roads (29.7% across 74 crashes)."
  },
  {
    "objectID": "posts/2024-11-11-blog-post/blog-05.html",
    "href": "posts/2024-11-11-blog-post/blog-05.html",
    "title": "Blog Post 5 - Additional Datasets",
    "section": "",
    "text": "Link: Pavement Condition Index 2019\nThe dataset I am combining is Montgomery County’s Pavement Condition Index (PCI), which assesses the condition of 5,200 lane miles of roadways. PCI uses a scale from 0-100 to represent road conditions, where lower values indicate poorer conditions (e.g., PCI of 30), and higher values indicate better conditions (e.g., PCI of 80).\nI am matching the geographic locations of crashes with the corresponding PCI values for those road segments, allowing analysis of whether areas with poorer road conditions have higher crash rates. I have identified key variables needed to join the two datasets, focusing on road-related variables, such as road name, in the crash dataset.\nThe challenge I encountered is that the road information between the two datasets doesn’t perfectly align. For example, one dataset has broader location descriptions than the other, making the merge challenging. My next steps are to refine the matching variables to improve accuracy and then conduct a more detailed analysis to examine the relationship between crash rates and PCI values."
  },
  {
    "objectID": "posts/2024-11-11-blog-post/blog-05.html#pavement-condition-index",
    "href": "posts/2024-11-11-blog-post/blog-05.html#pavement-condition-index",
    "title": "Blog Post 5 - Additional Datasets",
    "section": "",
    "text": "Link: Pavement Condition Index 2019\nThe dataset I am combining is Montgomery County’s Pavement Condition Index (PCI), which assesses the condition of 5,200 lane miles of roadways. PCI uses a scale from 0-100 to represent road conditions, where lower values indicate poorer conditions (e.g., PCI of 30), and higher values indicate better conditions (e.g., PCI of 80).\nI am matching the geographic locations of crashes with the corresponding PCI values for those road segments, allowing analysis of whether areas with poorer road conditions have higher crash rates. I have identified key variables needed to join the two datasets, focusing on road-related variables, such as road name, in the crash dataset.\nThe challenge I encountered is that the road information between the two datasets doesn’t perfectly align. For example, one dataset has broader location descriptions than the other, making the merge challenging. My next steps are to refine the matching variables to improve accuracy and then conduct a more detailed analysis to examine the relationship between crash rates and PCI values."
  },
  {
    "objectID": "posts/2024-11-11-blog-post/blog-05.html#annual-highway-mileage-reports",
    "href": "posts/2024-11-11-blog-post/blog-05.html#annual-highway-mileage-reports",
    "title": "Blog Post 5 - Additional Datasets",
    "section": "Annual Highway Mileage Reports",
    "text": "Annual Highway Mileage Reports\nLink: Annual Highway Mileage Reports\nThe primary dataset, Crash Reporting - Drivers Data, provides detailed information about traffic collisions on county and local roadways, including driver behaviors, environmental conditions, and collision outcomes. To enhance our analysis, we’re incorporating the Annual Highway Mileage Reports from the Maryland Department of Transportation State Highway Administration, which offers complementary information about road infrastructure, usage patterns, and road classifications.\nThe Highway Mileage Reports contain valuable metrics such as annual vehicle miles traveled, functional road classifications, and lane mileage data, which can provide context to our crash analysis. By combining these datasets, we can investigate whether certain road types experience higher crash rates, if traffic volume correlates with accident frequency, and how road infrastructure improvements might impact safety outcomes.\nThe process of combining these datasets requires careful consideration of common linking elements. We’re using geographic location as our primary joining key, specifically focusing on Montgomery County data. The crash data provides specific location information through latitude, longitude, and road names, while the highway mileage data is organized by county and road functional classifications. This allows us to analyze crash patterns in relation to road types and traffic volumes.\nHowever, we’ve encountered several challenges in the data integration process. First, the temporal alignment of the datasets requires attention, as we’re working with crash data from 2015-2023 and need to match it with corresponding annual highway reports. Second, the granularity of data differs between sources - crash data is incident-specific, while highway data is aggregated at the county level. We’re addressing this by creating appropriate aggregation levels that maintain analytical value while ensuring meaningful comparisons.\nOur next steps involve developing more sophisticated analysis methods to account for exposure rates (crashes per vehicle mile traveled) and examining how different road classifications correlate with crash severity. We’re also planning to incorporate additional variables such as lane width and surface type from the highway data to build a more comprehensive understanding of infrastructure-related safety factors."
  },
  {
    "objectID": "posts/2024-11-11-blog-post/blog-05.html#ems-responses---nemsis-maryland-state-data-set",
    "href": "posts/2024-11-11-blog-post/blog-05.html#ems-responses---nemsis-maryland-state-data-set",
    "title": "Blog Post 5 - Additional Datasets",
    "section": "EMS Responses - NEMSIS Maryland State Data Set",
    "text": "EMS Responses - NEMSIS Maryland State Data Set\nLink: EMS Responses - Maryland State Data Set\nTo enhance our crash analysis, we are integrating the NEMSIS Maryland State Data Set for EMS responses, which includes detailed information on response times, dispatch times, and patient transfers. By aligning this data with the Montgomery County crash dataset, we aim to assess how EMS response times vary by crash severity, location, and road conditions. This integration will allow us to identify patterns in EMS efficiency, highlighting areas where response times may be longer due to factors like poor road quality or high traffic. One challenge in combining these datasets is accurately matching crashes with EMS records, as location and time details differ slightly across sources. Our next steps involve refining these joins to better understand how EMS response correlates with injury severity and crash frequency in specific areas."
  },
  {
    "objectID": "posts/2024-11-11-blog-post/blog-05.html#marylands-traffic-volume-maps",
    "href": "posts/2024-11-11-blog-post/blog-05.html#marylands-traffic-volume-maps",
    "title": "Blog Post 5 - Additional Datasets",
    "section": "Maryland’s Traffic Volume Maps",
    "text": "Maryland’s Traffic Volume Maps\nLink: Maryland Traffic Volume Maps\nFor this analysis, we incorporated Maryland’s Traffic Volume Maps data, which provide Annual Average Daily Traffic (AADT) metrics across multiple roadways in Montgomery County and other regions. By integrating this dataset with our crash and road condition data, we aim to examine how traffic volume correlates with crash rates, particularly in areas with varying pavement conditions. This data enables a more nuanced analysis of risk factors, as higher traffic volumes may influence crash frequency and severity. A key challenge in this integration has been aligning traffic volume data with the specific crash locations due to slight mismatches in geographic identifiers between datasets. Our next steps will involve refining the matching process to ensure accurate associations, followed by statistical analyses to identify any significant relationships between traffic volume, road conditions, and crash incidences."
  },
  {
    "objectID": "posts/2024-11-11-blog-post/blog-05.html#crime-reporting-for-montgomery-county",
    "href": "posts/2024-11-11-blog-post/blog-05.html#crime-reporting-for-montgomery-county",
    "title": "Blog Post 5 - Additional Datasets",
    "section": "Crime Reporting for Montgomery County",
    "text": "Crime Reporting for Montgomery County\nLink: Crime Dataset\nTo add in tandem with our Crash Reporting for Montgomery County, MD. I found a data set based on crime data in the same county. This data set offers variables such as location, crime committed, dates, and street names. This could offer some additional relationships we can graph with linear regression to see how crime in the county is related to crashes.\nThis data set provides information from other counties as well, which we would need to filter out, additionally, it is not offered in csv format but we can reformat it and fit it into our project. Additionally, with thirty variables/columns, it may take more time to choose the right ones and investigate possible relationships without the main data set.\nThis information comes from an official US website, DATA.GOV, and was shared for the purpose of public access. This dataset is updated daily by Montgomery Polic Department."
  },
  {
    "objectID": "posts/2024-12-02-blog-post/blog-07.html",
    "href": "posts/2024-12-02-blog-post/blog-07.html",
    "title": "Blog Post 7 - Wrap-Up: Traffic Safety Analysis in Montgomery County",
    "section": "",
    "text": "Our analysis of traffic safety in Montgomery County has evolved into a comprehensive study that integrates multiple datasets to understand the intricate relationships between infrastructure, emergency response capabilities, and temporal patterns. Through meticulous analysis of crash data, pavement conditions, and emergency response metrics, we’ve developed a thesis that captures the complex nature of traffic safety in the county: Traffic safety in Montgomery County is influenced by the complex relationship of road infrastructure characteristics (including pavement quality and road type), emergency response capabilities, and temporal patterns, with certain combinations creating high-risk scenarios that affect both crash likelihood and emergency response outcomes."
  },
  {
    "objectID": "posts/2024-12-02-blog-post/blog-07.html#project-overview-and-thesis-development",
    "href": "posts/2024-12-02-blog-post/blog-07.html#project-overview-and-thesis-development",
    "title": "Blog Post 7 - Wrap-Up: Traffic Safety Analysis in Montgomery County",
    "section": "",
    "text": "Our analysis of traffic safety in Montgomery County has evolved into a comprehensive study that integrates multiple datasets to understand the intricate relationships between infrastructure, emergency response capabilities, and temporal patterns. Through meticulous analysis of crash data, pavement conditions, and emergency response metrics, we’ve developed a thesis that captures the complex nature of traffic safety in the county: Traffic safety in Montgomery County is influenced by the complex relationship of road infrastructure characteristics (including pavement quality and road type), emergency response capabilities, and temporal patterns, with certain combinations creating high-risk scenarios that affect both crash likelihood and emergency response outcomes."
  },
  {
    "objectID": "posts/2024-12-02-blog-post/blog-07.html#data-integration-and-analysis-progress",
    "href": "posts/2024-12-02-blog-post/blog-07.html#data-integration-and-analysis-progress",
    "title": "Blog Post 7 - Wrap-Up: Traffic Safety Analysis in Montgomery County",
    "section": "Data Integration and Analysis Progress",
    "text": "Data Integration and Analysis Progress\nThe depth of our analysis has been significantly enhanced by the integration of several critical datasets. We’ve incorporated the Pavement Condition Index (PCI), Annual Highway Mileage Reports, EMS Responses from NEMSIS Maryland State Data, Maryland Traffic Volume Maps, and Montgomery County Crime Data. This comprehensive data integration has allowed us to examine traffic safety from multiple angles, providing a more nuanced understanding of the factors contributing to crash incidents and their outcomes.\nOur analysis revealed several unexpected patterns that challenged our initial assumptions. Most notably, we discovered that the relationship between road quality and accident occurrence isn’t linear – most crashes occur on roads with moderate Pavement Condition Index ratings (30-80), rather than on the poorest quality roads as initially hypothesized. This finding suggests that other factors, such as traffic volume and speed limits, may play more significant roles in crash likelihood than pavement quality alone."
  },
  {
    "objectID": "posts/2024-12-02-blog-post/blog-07.html#statistical-analysis-and-findings",
    "href": "posts/2024-12-02-blog-post/blog-07.html#statistical-analysis-and-findings",
    "title": "Blog Post 7 - Wrap-Up: Traffic Safety Analysis in Montgomery County",
    "section": "Statistical Analysis and Findings",
    "text": "Statistical Analysis and Findings\nOur regression analysis of the Pavement Condition Index yielded interesting results. With an intercept of 11.09965 and a PCI coefficient of -0.04909 (statistically significant at p=0.00337), we found a weak but significant relationship between pavement quality and accident rates. However, the low adjusted R-squared value of 0.004102 suggests that pavement quality alone explains only a small portion of the variance in crash occurrences, pointing to the need for a more comprehensive model that incorporates additional variables.\nIn our investigation of potential correlations between crime rates and traffic safety, we found a very weak positive correlation (0.0326) between daily crime counts and crashes. While we observed slight variations in crash distribution patterns on high-crime days, the relationship wasn’t strong enough to suggest a meaningful connection between criminal activity and traffic safety incidents."
  },
  {
    "objectID": "posts/2024-12-02-blog-post/blog-07.html#enhanced-visualization",
    "href": "posts/2024-12-02-blog-post/blog-07.html#enhanced-visualization",
    "title": "Blog Post 7 - Wrap-Up: Traffic Safety Analysis in Montgomery County",
    "section": "Enhanced Visualization",
    "text": "Enhanced Visualization\nTo effectively communicate our findings, we’re developing a suite of sophisticated visualizations that will illustrate the complex relationships we’ve uncovered. Our infrastructure analysis visualizations will demonstrate the relationships between crash rates and various surface types, road widths, and functional classifications. We’re particularly focused on creating mileage-adjusted crash rate visualizations that will provide a more accurate representation of road safety across different route types.\nTemporal pattern analysis will be enhanced through visualizations that show year-over-year infrastructure performance trends and safety metrics over time. These visualizations will incorporate multiple variables to show how different factors interact across various time scales, from daily patterns to seasonal trends."
  },
  {
    "objectID": "posts/2024-12-02-blog-post/blog-07.html#statistical-methodology-refinement",
    "href": "posts/2024-12-02-blog-post/blog-07.html#statistical-methodology-refinement",
    "title": "Blog Post 7 - Wrap-Up: Traffic Safety Analysis in Montgomery County",
    "section": "Statistical Methodology Refinement",
    "text": "Statistical Methodology Refinement\nOur statistical approach has evolved to include more sophisticated models that better capture the complexity of traffic safety. We’re implementing negative binomial regression for crash frequency analysis, as this method better handles the count nature of crash data compared to traditional linear regression. Time series analysis is being applied to temporal patterns to identify seasonal and cyclical trends in crash occurrences.\nThe multiple linear regression model we’re developing incorporates several key predictors, including PCI, road width, and speed limits. We’re particularly interested in the interaction terms between PCI and speed limits, as preliminary analysis suggests that the impact of pavement quality on safety varies significantly at different speed levels."
  },
  {
    "objectID": "posts/2024-12-02-blog-post/blog-07.html#website-structure-and-presentation",
    "href": "posts/2024-12-02-blog-post/blog-07.html#website-structure-and-presentation",
    "title": "Blog Post 7 - Wrap-Up: Traffic Safety Analysis in Montgomery County",
    "section": "Website Structure and Presentation",
    "text": "Website Structure and Presentation\nWe are working on putting the website with an introduction that outlines our project scope and objectives, we move through sections on data cleaning and methodology, analysis and visualizations, geographic distribution analysis, temporal pattern examination, and infrastructure impact assessment. Each section builds upon the previous ones to create a narrative that supports our thesis while acknowledging the complexities inherent in traffic safety analysis."
  },
  {
    "objectID": "posts/2024-12-02-blog-post/blog-07.html#future-directions-and-implications",
    "href": "posts/2024-12-02-blog-post/blog-07.html#future-directions-and-implications",
    "title": "Blog Post 7 - Wrap-Up: Traffic Safety Analysis in Montgomery County",
    "section": "Future Directions and Implications",
    "text": "Future Directions and Implications\nAs we move toward completing this project, we’re focusing on refining our visualizations using advanced tools and techniques. We’re implementing interactive features that will allow users to explore the data in more detail, and we’re developing detailed annotations that will help guide readers through our findings. Hopefully, our findings could help inform policy decisions and infrastructure improvements in Montgomery County, potentially leading to more effective traffic safety measures."
  }
]