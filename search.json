[
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "This comes from the file data.qmd.\nYour first steps in this project will be to find data to work on.\nInitially, you will study one dataset but later you will need to combine that data with another dataset. For this reason, I recommend finding data that has some date and/or location components. These types of data are conducive to interesting visualizations and analysis and you can also combine this data with other data that also has a date or location variable. Data from the census, weather data, economic data, are all relatively easy to combine with other data with time/location components."
  },
  {
    "objectID": "data.html#what-makes-a-good-data-set",
    "href": "data.html#what-makes-a-good-data-set",
    "title": "Data",
    "section": "What makes a good data set?",
    "text": "What makes a good data set?\n\nData you are interested in and care about.\nData where there are a lot of potential questions that you can explore.\nA data set that isn’t completely cleaned already.\nMultiple sources for data that you can combine.\nSome type of time and/or location component."
  },
  {
    "objectID": "data.html#where-to-keep-data",
    "href": "data.html#where-to-keep-data",
    "title": "Data",
    "section": "Where to keep data?",
    "text": "Where to keep data?\nBelow 50mb: In dataset folder\nAbove 50mb: In dataset_ignore folder. This folder will be ignored by git so you’ll have to manually sync these files across your team.\n\nSharing your data\nFor small datasets (&lt;50mb), you can use the dataset folder that is tracked by github. Add the files just like you would any other file.\nIf you create a folder named data this will cause problems.\nFor larger datasets, you’ll need to create a new folder in the project root directory named dataset-ignore. This will be ignored by git (based off the .gitignore file in the project root directory) which will help you avoid issues with Github’s size limits. Your team will have to manually make sure the data files in dataset-ignore are synced across team members.\nYour load_and_clean_data.R file is how you will load and clean your data. Here is a an example of a very simple one.\n\nsource(\n  \"scripts/load_and_clean_data.R\",\n  echo = TRUE # Use echo=FALSE or omit it to avoid code output  \n)\n\n\n&gt; library(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n&gt; loan_data &lt;- read_csv(here::here(\"dataset\", \"loan_refusal.csv\"))\n\n\nRows: 20 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): bank\ndbl (4): min, white, himin, hiwhite\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n&gt; loan_data_clean &lt;- pivot_longer(loan_data, 2:5, names_to = \"group\", \n+     values_to = \"refusal_rate\")\n\n&gt; write_rds(loan_data_clean, file = here::here(\"dataset\", \n+     \"loan_refusal_clean.rds\"))\n\n\nYou should never use absolute paths (eg. /Users/danielsussman/path/to/project/ or C:\\MA415\\\\Final_Project\\).\nYou might consider using the here function from the here package to avoid path problems.\n\n\nLoad and clean data script\nThe idea behind this file is that someone coming to your website could largely replicate your analyses after running this script on the original data sets to clean them. This file might create a derivative data set that you then use for your subsequent analysis. Note that you don’t need to run this script from every post/page. Instead, you can load in the results of this script, which could be plain text files or .RData files. In your data page you’ll describe how these results were created. If you have a very large data set, you might save smaller data sets that you can use for exploration purposes. To link to this file, you can use [cleaning script](/scripts/load_and_clean_data.R) wich appears as cleaning script."
  },
  {
    "objectID": "data.html#rubric-on-this-page",
    "href": "data.html#rubric-on-this-page",
    "title": "Data",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nDescribe where/how to find data.\n\nYou must include a link to the original data source(s). Make sure to provide attribution to those who collected the data.\nWhy was the data collected/curated? Who put it together? (This is important, if you don’t know why it was collected then that might not be a good dataset to look at.\n\nDescribe the different data files used and what each variable means.\n\nIf you have many variables then only describe the most relevant ones and summarize the rest.\n\nDescribe any cleaning you had to do for your data.\n\nYou must include a link to your load_and_clean_data.R file.\nRrename variables and recode factors to make data more clear.\nAlso, describe any additional R packages you used outside of those covered in class.\nDescribe and show code for how you combined multiple data files and any cleaning that was necessary for that.\nSome repetition of what you do in your load_and_clean_data.R file is fine and encouraged if it helps explain what you did.\n\nOrganization, clarity, cleanliness of the page\n\nMake sure to remove excessive warnings, use clean easy-to-read code (without side scrolling), organize with sections, use bullets and other organization tools, etc.\nThis page should be self-contained."
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "This comes from the file analysis.qmd.\nWe describe here our detailed data analysis. This page will provide an overview of what questions you addressed, illustrations of relevant aspects of the data with tables and figures, and a statistical model that attempts to answer part of the question. You’ll also reflect on next steps and further analysis.\nThe audience for this page is someone like your class mates, so you can expect that they have some level of statistical and quantitative sophistication and understand ideas like linear and logistic regression, coefficients, confidence intervals, overfitting, etc.\nWhile the exact number of figures and tables will vary and depend on your analysis, you should target around 5 to 6. An overly long analysis could lead to losing points. If you want you can link back to your blog posts or create separate pages with more details.\nThe style of this paper should aim to be that of an academic paper. I don’t expect this to be of publication quality but you should keep that aim in mind. Avoid using “we” too frequently, for example “We also found that …”. Describe your methodology and your findings but don’t describe your whole process."
  },
  {
    "objectID": "analysis.html#note-on-attribution",
    "href": "analysis.html#note-on-attribution",
    "title": "Analysis",
    "section": "Note on Attribution",
    "text": "Note on Attribution\nIn general, you should try to provide links to relevant resources, especially those that helped you. You don’t have to link to every StackOverflow post you used but if there are explainers on aspects of the data or specific models that you found helpful, try to link to those. Also, try to link to other sources that might support (or refute) your analysis. These can just be regular hyperlinks. You don’t need a formal citation.\nIf you are directly quoting from a source, please make that clear. You can show quotes using &gt; like this\n&gt; To be or not to be.\n\nTo be or not to be."
  },
  {
    "objectID": "analysis.html#rubric-on-this-page",
    "href": "analysis.html#rubric-on-this-page",
    "title": "Analysis",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nIntroduce what motivates your Data Analysis (DA)\n\nWhich variables and relationships are you most interested in?\nWhat questions are you interested in answering?\nProvide context for the rest of the page. This will include figures/tables that illustrate aspects of the data of your question.\n\nModeling and Inference\n\nThe page will include some kind of formal statistical model. This could be a linear regression, logistic regression, or another modeling framework.\nExplain the ideas and techniques you used to choose the predictors for your model. (Think about including interaction terms and other transformations of your variables.)\nDescribe the results of your modelling and make sure to give a sense of the uncertainty in your estimates and conclusions.\n\nExplain the flaws and limitations of your analysis\n\nAre there some assumptions that you needed to make that might not hold? Is there other data that would help to answer your questions?\n\nClarity Figures\n\nAre your figures/tables/results easy to read, informative, without problems like overplotting, hard-to-read labels, etc?\nEach figure should provide a key insight. Too many figures or other data summaries can detract from this. (While not a hard limit, around 5 total figures is probably a good target.)\nDefault lm output and plots are typically not acceptable.\n\nClarity of Explanations\n\nHow well do you explain each figure/result?\nDo you provide interpretations that suggest further analysis or explanations for observed phenomenon?\n\nOrganization and cleanliness.\n\nMake sure to remove excessive warnings, hide most or all code, organize with sections or multiple pages, use bullets, etc.\nThis page should be self-contained, i.e. provide a description of the relevant data."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MA [46]15 Final Project",
    "section": "",
    "text": "Final Project due May 7, 2024 at 11:59pm.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEDA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 5: Additional Datasets\n\n\n\n\n\n\n\n\n\n\n\nNov 11, 2024\n\n\nGroup 1\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 3: EDA\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2024\n\n\nGroup 1\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 2/3: Data Clean\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2024\n\n\nGroup 1\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 2/3: Data Equity\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2024\n\n\nGroup 1\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 2/3: Data Equity\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2024\n\n\nGroup 1\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 2: Data Background\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2024\n\n\nGroup 1\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 1: Choosing a Dataset\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2024\n\n\nGroup 1\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n\n\n\n\n\n\nGetting started\n\n\n\n\n\n\n\n\nDirections to set up your website and create your first post. \n\n\n\n\n\nFeb 23, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n\n\n\n\n\n\nFirst Team Meeting\n\n\n\n\n\n\n\n\nThis post details the steps you’ll take for your first team meeting. \n\n\n\n\n\nFeb 21, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "dataset/cleaning_primah.html",
    "href": "dataset/cleaning_primah.html",
    "title": "Data",
    "section": "",
    "text": "library(lubridate)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(skimr)\n\n\ncrash_df &lt;- read.csv(\"dataset-ignore/Crash_Reporting_Drivers_Data.csv\")\n\n# remove columns with a high percentage of missing values\n\n# Define threshold for column selection\nthreshold &lt;- 0.8\n\n# Remove columns with a high percentage of missing values\ndata_clean &lt;- crash_df |&gt;\n  select_if(~ mean(is.na(.)) &lt; threshold)\n# convert case number column to character format\n\ndata_clean$Local.Case.Number &lt;- as.character(data_clean$Local.Case.Number)\n\n# convert date and time column to datetime format\n\ndata_clean$Crash.Date.Time &lt;- mdy_hms(data_clean$Crash.Date.Time)\n\n\n# fill missing values in route type column with the most common value\n\nmost_common_route &lt;- data_clean$Route_Type |&gt; \n\n  na.omit() |&gt;\n\n  table() |&gt; \n\n  which.max()\n\n\n\ndata_clean$Route_Type[is.na(data_clean$Route_Type)] &lt;- names(most_common_route)\n\n\n\n# remove duplicate rows\n\ndata_clean &lt;- data_clean |&gt; distinct()\n\nsubset_data &lt;- data_clean %&gt;%\n  slice(1:1000) # Adjust based on your requirements\n\n\n# check the summary of cleaned data\n\nsummary(data_clean)\n\nsaveRDS(subset_data,\"dataset/cleaned_dataset.rds\")"
  },
  {
    "objectID": "posts/2024-10-21-blog-post/blog-02.html",
    "href": "posts/2024-10-21-blog-post/blog-02.html",
    "title": "Blog Post 2: Data Background",
    "section": "",
    "text": "The data comes from the Automated Crash Reporting System (ACRS) of the Maryland State Police. It includes traffic collision reports from Montgomery County Police, Gaithersburg Police, Rockville Police, and the Maryland National Capital Park Police. The data was collected by the ACRS and local law enforcement agencies. The data was collected to document and analyze traffic collisions involving motor vehicle operators on county and local roads in Montgomery County. The purpose of this data collection might be to help enforcement agencies understand the circumstances of traffic collisions. It can be found on the original website, which is the Montgomery County government data site."
  },
  {
    "objectID": "posts/2024-10-21-blog-post/blog-02.html#where-does-the-data-come-from-who-collected-it-why-was-it-collected-are-you-able-to-find-the-data-from-the-original-source",
    "href": "posts/2024-10-21-blog-post/blog-02.html#where-does-the-data-come-from-who-collected-it-why-was-it-collected-are-you-able-to-find-the-data-from-the-original-source",
    "title": "Blog Post 2: Data Background",
    "section": "",
    "text": "The data comes from the Automated Crash Reporting System (ACRS) of the Maryland State Police. It includes traffic collision reports from Montgomery County Police, Gaithersburg Police, Rockville Police, and the Maryland National Capital Park Police. The data was collected by the ACRS and local law enforcement agencies. The data was collected to document and analyze traffic collisions involving motor vehicle operators on county and local roads in Montgomery County. The purpose of this data collection might be to help enforcement agencies understand the circumstances of traffic collisions. It can be found on the original website, which is the Montgomery County government data site."
  },
  {
    "objectID": "posts/2024-10-21-blog-post/blog-02.html#are-there-any-issues-you-can-see-with-how-the-data-was-collected-what-is-the-sample-population-are-there-reasons-to-think-the-sample-is-biased-in-some-way",
    "href": "posts/2024-10-21-blog-post/blog-02.html#are-there-any-issues-you-can-see-with-how-the-data-was-collected-what-is-the-sample-population-are-there-reasons-to-think-the-sample-is-biased-in-some-way",
    "title": "Blog Post 2: Data Background",
    "section": "Are there any issues you can see with how the data was collected? What is the sample population? Are there reasons to think the sample is biased in some way?",
    "text": "Are there any issues you can see with how the data was collected? What is the sample population? Are there reasons to think the sample is biased in some way?\nAccording to the information from the original government website providing the data, since the collision reports are based on preliminary information supplied to the Police Department by the reporting parties, there are some potential issues with the data collection process, including:\n\nInformation not yet verified by further investigation\nInformation that may include verified and unverified collision data\nPreliminary collision classifications may be changed at a later date based on further investigation\nInformation may include mechanical or human error\n\nThe sample population consists of motor vehicle operators involved in traffic collisions on county and local roadways within Montgomery County, Maryland. The dataset might be biased because it may not include minor collisions or unreported incidents."
  },
  {
    "objectID": "posts/2024-10-21-blog-post/blog-02.html#how-is-this-data-used-what-questions-have-others-ask-about-this-data",
    "href": "posts/2024-10-21-blog-post/blog-02.html#how-is-this-data-used-what-questions-have-others-ask-about-this-data",
    "title": "Blog Post 2: Data Background",
    "section": "How is this data used? What questions have others ask about this data?",
    "text": "How is this data used? What questions have others ask about this data?\nThe data is widely used by government agencies, law enforcement, transportation planners, and other stakeholders who might be looking into improving road safety and traffic management. By analyzing the collision patterns, they can identify areas that might be prone to accidents and take corrective measures. The data set also includes information on driver at-fault status, substance abuse, and distractions providing insights into common behavioral causes of collisions. This can inform stricter law enforcement measures. Officials may also use this data to assess the effectiveness of traffic regulations. This data can also be used by insurance companies for underwriting and to assess claims involving collisions. Some questions that others might ask about the data are:\n\nWhich roads or intersections are leading to the highest collision rates and why? This will help identify accident hotspots that need infrastructural improvements\nWhat is the correlation between weather conditions and the rate of accidents? This can inform them on whether or not they need more safety warnings during hazardous weather conditions that might help people navigate these conditions better and potentially lead to fewer collisions\nWhat time of the day do most accidents occur? Understanding this better can help change street lighting or traffic patterns to make driving at night easier\nAre there certain car models that have more accidents than others? This could lead to a ban on certain kinds of vehicles or even help manufacturers fix flaws"
  },
  {
    "objectID": "posts/2024-10-21-blog-post/blog-02.html#has-there-been-other-research-on-the-same-data-is-this-data-being-used-for-some-policy-decisions",
    "href": "posts/2024-10-21-blog-post/blog-02.html#has-there-been-other-research-on-the-same-data-is-this-data-being-used-for-some-policy-decisions",
    "title": "Blog Post 2: Data Background",
    "section": "Has there been other research on the same data? Is this data being used for some policy decisions?",
    "text": "Has there been other research on the same data? Is this data being used for some policy decisions?\nThis data provided by the state of New Jersey is being used by the New Jersey Department of Transportation (NJDOT) through a database called Safety Voyager. This tool allows them to utilize data like this set as well as similar datasets from other counties in the state, to visualize and analyze crash data. A dataset like ours is used in combination with the surrounding towns to identify areas of high risk. This data is also used to better safety improvement projects such as The Highway Safety Improvement Project (HSIP) which gathers this information from NJDOT to reduce crashes in crash-dense areas. Additionally, the New Jersey Transportation Planning Authority (NJTPA) also refers to information synthesized from our data to create overall safety examinations and better planning efforts. Along with NJDOT, HSIP, and NJTPA, the New Jersey Safety Outcomes and Data Warehouse (NJ-SHO) integrates crash data such as ours as well as health consequences to investigate public safety. Overall, it is mainly the state departments of New Jersey that are leading research projects with these crash reports, and creating programs and initiatives that will encourage future safety measures in the state."
  },
  {
    "objectID": "posts/2024-11-06-blog-post/blog-03.html",
    "href": "posts/2024-11-06-blog-post/blog-03.html",
    "title": "Blog Post 3: EDA",
    "section": "",
    "text": "The road with the highest number of crashes is unnamed, with just under 125 crashes recorded. The road with the second-highest number of crashes is Georgia Avenue, which has significantly fewer crashes, totaling just under 50\n\n# The top 10 roads with the highest number of crashes \nlibrary(dplyr)\nlibrary(ggplot2)\n\ncleaned_dataset &lt;- read.csv(\"/dataset/cleaned_dataset.rds\")\n\nroad_crash_data &lt;- cleaned_dataset %&gt;%\n  filter(!is.na(`Road.Name`)) %&gt;%\n  count(`Road.Name`, sort = TRUE) %&gt;%\n  top_n(10, n)\n\nggplot(road_crash_data, aes(x = reorder(`Road.Name`, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +\n  labs(title = \"Top 10 Roads with the Most Crashes\",x = \"Road Name\", y = \"Number of Crashes\") +\n  theme_minimal()\n\n\n# Distribution of Simplified Surface conditions by collision type\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\nsimplified_barplot_data &lt;- cleaned_dataset %&gt;%\n  mutate(Simplified_Surface = case_when(\n    Surface.Condition %in% c(\"Dry\", \"DRY\") ~ \"Dry\",\n    Surface.Condition %in% c(\"Wet\", \"WET\", \"Water (standing, moving)\", \"WATER(STANDING/MOVING)\") ~ \"Wet\",\n    Surface.Condition %in% c(\"Ice\", \"ICE\", \"Ice/Frost\", \"Snow\", \"SNOW\", \"Slush\", \"SLUSH\") ~ \"Snow/Ice\",\n    Surface.Condition %in% c(\"Mud, Dirt, Gravel\", \"MUD, DIRT, GRAVEL\") ~ \"Mud/Dirt\",\n    Surface.Condition %in% c(\"Sand\", \"SAND\", \"OIL\", \"Other\", \"OTHER\") ~ \"Other\",\n    TRUE ~ \"Unknown\"\n  )) %&gt;%\n  count(Collision.Type, Simplified_Surface) %&gt;%\n  group_by(Collision.Type) %&gt;%\n  mutate(percentage = n / sum(n))\n\nggplot(simplified_barplot_data, aes(x = Collision.Type, y = percentage, fill = Simplified_Surface)) +\n  geom_bar(stat = \"identity\", position = \"fill\") +\n  scale_y_continuous(labels = scales::percent) +\n  labs(title = \"Distribution of Simplified Surface Conditions by Collision Type\",\n       x = \"Collision Type\", y = \"Percentage\",\n       fill = \"Surface Condition\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nThere is no definitive way to predict the time of year crashes will occur, but the trend shows a notable spike in crashes from January to March in 2015 and 2017. Additionally, there is often an increase in crashes in the month of November.\n\n# Monthly trend of Crash COunts by Year\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(lubridate)\n\n\ncrash_data &lt;- cleaned_dataset %&gt;%\n  mutate(\n    Crash_Date = ymd_hms(Crash.Date.Time), \n    Year = year(Crash_Date),\n    Month = month(Crash_Date, label = TRUE)\n  ) %&gt;%\n  filter(!is.na(Crash_Date))  \n\nmonthly_data &lt;- crash_data %&gt;%\n  group_by(Year, Month) %&gt;%\n  summarise(Crash_Count = n(), .groups = 'drop')\n\nggplot(monthly_data, aes(x = Month, y = Crash_Count, group = Year)) +\n  geom_line() +\n  labs(\n    title = \"Monthly Trend of Crash Counts by Year\",\n    x = \"Month\", y = \"Number of Crashes\"\n  ) +\n  facet_wrap(~ Year, scales = \"free_y\") +\n  theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nOut of the top 10 roads with the most crashes the surface conditions are mostly dry.\n\n# Find the top 10 roads with the most crashes\ntop_road_crash_data &lt;- cleaned_dataset %&gt;%\n  filter(!is.na(`Road.Name`)) %&gt;%\n  count(`Road.Name`, sort = TRUE) %&gt;%\n  top_n(10, n)\n\n# Filter the cleaned dataset to only include these top 10 roads\ntop_road_conditions &lt;- cleaned_dataset %&gt;%\n  filter(`Road.Name` %in% top_road_crash_data$`Road.Name`) %&gt;%\n  mutate(Simplified_Surface = case_when(\n    Surface.Condition %in% c(\"Dry\", \"DRY\") ~ \"Dry\",\n    Surface.Condition %in% c(\"Wet\", \"WET\", \"Water (standing, moving)\", \"WATER(STANDING/MOVING)\") ~ \"Wet\",\n    Surface.Condition %in% c(\"Ice\", \"ICE\", \"Ice/Frost\", \"Snow\", \"SNOW\", \"Slush\", \"SLUSH\") ~ \"Snow/Ice\",\n    Surface.Condition %in% c(\"Mud, Dirt, Gravel\", \"MUD, DIRT, GRAVEL\") ~ \"Mud/Dirt\",\n    Surface.Condition %in% c(\"Sand\", \"SAND\", \"OIL\", \"Other\", \"OTHER\") ~ \"Other\",\n    TRUE ~ \"Unknown\"\n  )) %&gt;%\n  count(`Road.Name`, Simplified_Surface) %&gt;%\n  group_by(`Road.Name`) %&gt;%\n  mutate(percentage = n / sum(n))\n\n# Plot the distribution of surface conditions for the top 10 roads\nggplot(top_road_conditions, aes(x = reorder(`Road.Name`, -n), y = percentage, fill = Simplified_Surface)) +\n  geom_bar(stat = \"identity\", position = \"fill\") +\n  scale_y_continuous(labels = scales::percent) +\n  coord_flip() +\n  labs(\n    title = \"Surface Conditions for the Top 10 Roads with the Most Crashes\",\n    x = \"Road Name\", y = \"Percentage of Crashes\",\n    fill = \"Surface Condition\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "posts/2024-10-11-blog-post/blog-01.html",
    "href": "posts/2024-10-11-blog-post/blog-01.html",
    "title": "Blog Post 1: Choosing a Dataset",
    "section": "",
    "text": "Summaries: Choosing a Data Set\nNational Student Loan Data System: https://catalog.data.gov/dataset/national-student-loan-data-system-722b0 The National Student Loan data contains information about loans and grants awarded to students under Title IV of the Higher Education Act of 1965, which refers to federal financial aid programs for post secondary education. The data includes various loan categories like Direct Loans, Federal Family Education Loans (FFEL), and Perkins Loans. It spans the entire life cycle of these loans, from disbursement to closure. Most files contain between 10 to 25 columns, with key information such as loan types, disbursement amounts, repayment plans, deferment and forbearance details, and loan statuses. These columns typically include financial metrics like “Dollars Outstanding”, “Recipients”, and “Number of Loans”. The files also vary in terms of row counts; some are relatively small, like summary files with a few hundred rows, while detailed loan or quarterly activity reports contain thousands of rows (ranging from 3798 to 4057 rows). Some files also have additional formatting and metadata rows.\nThis data was originally collected to centralize and monitor federal student aid, helping policymakers, educational institutions, and borrowers manage financial aid efficiently. Main challenges might include cleaning and consolidating disparate files and handling large amounts of historical data, especially if some files are incomplete or inconsistent in format. Some key questions to consider might include loan default rates over time, trends in deferment or forbearance, and the impact of repayment plans on loan outcomes. Additionally, we can explore how loan activity varies across schools and regions, using the quarterly reports to identify trends in loan disbursements and borrower demographics. Another key question would be how the Public Service Loan Forgiveness and Teacher Loan Forgiveness programs are affecting long-term repayment patterns and loan forgiveness rate across various borrower groups.\nCrash Reporting-Drivers Data: https://catalog.data.gov/dataset/crash-reporting-drivers-data This dataset includes information on vehicles that have been involved in collisions on public and local roads within Montgomery County, Maryland. It includes 39 columns and 186,170 rows or entries, and we are able to clean the data as well as load it into the environment. It was collected by an Automated Reporting System (ARS) through the Police department and showed each collision recorded as well as the drivers involved. This data may include verified as well as unverified data and is non-federal.\nThe main questions we hope to address with this data set include data on road safety, driver behaviors, as well as accident patterns within Montgomery County. We could use response and predictor variables such as location, vehicle model, collision type, and time of day (light) to name a few. Some challenges we could face with this data are the number of entries it holds, as well as many NA entries that were unable to be filled by the ARS. This means cleaning this data could take much longer than anticipated. Additionally, it doesn’t provide variables such as Age or Race which could be helpful indicators to see patterns in possible criminal injustice. Overall this data set is very interesting but it may not be the best of the three for analyzing as well as finding prejudices.\nCrime Data from 2020 to Present: https://catalog.data.gov/dataset/crime-data-from-2020-to-present This dataset contains records of crime incidents in Los Angeles, California, from 2020 until 2024. The dataset was digitized from paper reports originally maintained by the LA Police Department and later transferred to an electronic format. It includes 22 columns and over 200,000 rows of data. Due to digitizing, there may be some discrepancies, such as missing or incomplete data, particularly in location fields. For privacy reasons, locations are only recorded up to the hundredth block rather than specific addresses.\nThe dataset contains over 200,000 rows and 22 columns, offering detailed information about various incidents, including crime type, date, time, location, and the involved parties. The Los Angeles Police Department originally collected this data as part of routine crime reporting. Initially maintained as paper records, it was later digitized and entered into an electronic database. This data collection aims to track crime incidents across the city for law enforcement purposes, urban planning, and policy analysis. It is also used to inform the public and local authorities about crime trends and patterns in Los Angeles.\nThe data can be loaded and cleaned, but its large size and missing entries from the transition from paper to digital format may make cleaning time-consuming. Special attention will be needed for missing values, especially in location and categorical fields, and date/time reformatting may be required. Key questions include identifying crime trends from 2020 to 2024, examining neighborhoods with higher crime rates, and finding correlations between crime types and factors like time of day or proximity to locations. Challenges include handling over 200,000 rows, missing location data, and limited precision due to privacy restrictions on location details. Ensuring data consistency after the transition is another potential issue."
  },
  {
    "objectID": "posts/2024-10-28-blog-post/data_equity.html",
    "href": "posts/2024-10-28-blog-post/data_equity.html",
    "title": "Blog Post 2/3: Data Equity",
    "section": "",
    "text": "#Think about how the principles for advancing equitable data practice are relevant to your data.For two or three of principles, write about how they are relevant and what adhering to the practice would entail for your data.Think about what might be some limitations of your analysis and the potential for abuse or misuse of the data.\nWe can utilize the principles for advancing equitable data practices by applying the principles of beneficence, respect for persons, and justice to the exploration of our data set.\nThese principles are relevant for a few reasons. First, looking at beneficience, we may consider how the findings in our data set are being used by local or federal governments, as well as how we are using the data. We can see from this principle what the best actions for the county to take in response to traffic violations would be. Additionally adhering to this practice for our group’s project may look like researching into how the government is using this dataset.\nFurthermore, we can look at the principle of justice. This is relevant to our data as we can see the members of this sample that are facing violations or get into accidents. We can see if they are more at risk than others, and provide some insight through our findings to see if there is any injustice within this county of Marylands response, wheher through law or bill.\nSome limitations of our analysis could be missing data, data not provided or included as a variable, and lack of information on the internet to see the local or federal governments lack of/ or response to this data. Some misuse of the data could be government surveillance in high at-risk areas or strict law enforcement. It could also look like policy changes without community input and in turn renforce community inequities."
  },
  {
    "objectID": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "href": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "title": "First Team Meeting",
    "section": "",
    "text": "These are the steps that you will take today to get started on your project. Today, you will just be brainstorming, and then next week, you’ll get started on the main aspects of the project.\n\nStart by introducing yourselves to each other. I also recommend creating a private channel on Microsoft Teams with all your team members. This will be a place that you can communicate and share ideas, code, problems, etc.\nDiscuss what aspects of the project each of you are more or less excited about. These include\n\nCollecting, cleaning, and munging data ,\nStatistical Modeling,\nVisualization,\nWriting about analyses, and\nManaging and reviewing team work.\n\nBased on this, discuss where you feel your strengths and weaknesses might be.\nNext, start brainstorming questions you hope to answer as part of this project. This question should in some way be addressing issues around racial disparities. The questions you come up with should be at the level of the question we started with when exploring the HMDA data. (“Are there differences in the ease of securing a loan based on the race of the applicant?”) You’ll revise your questions a lot over the course of the project. Come up with a few questions that your group might be interested in exploring.\nBased on these questions, start looking around for data that might help you analyze this. If you are looking at U.S. based data, data.gov is a good source and if you are looking internationally, I recommend checking out the World Bank. Also, try Googling for data. Include “data set” or “dataset” in your query. You might even include “CSV” or some other format. Using “data” by itself in your query often doesn’t work too well. Spend some time searching for data and try to come up with at least three possible data sets. (For your first blog post, you’ll write short proposals about each of them that I’ll give feedback on.)\nCome up with a team name. Next week, I’ll provide the Github Classroom assignment that will be where you work on your final project and you’ll have to have your team name finalized by then. Your project will be hosted online at the website with a URL like sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME.\n\nNext time, you’ll get your final project website set up and write your first blog post."
  },
  {
    "objectID": "posts/2023-10-15-getting-started/getting-started.html",
    "href": "posts/2023-10-15-getting-started/getting-started.html",
    "title": "Getting started",
    "section": "",
    "text": "Below, the items marked with [[OP]] should only be done by one person on the team.\n\nTo get started\n\n[[OP]] One person from the team should click the Github Classroom link on Teams.\n[[OP]] That person types in the group name for their group.\nThe rest of the team now clicks the Github Classroom link and selects their team from the dropdown list.\nFinally, each of you can clone the repository to your laptop like a normal assignment.\n\n\n\nSetting up the site\n\n[[OP]] Open the terminal and run quarto publish gh-pages.\n[[OP]] Select Yes to the prompt:  ? Publish site to https://sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME/ using gh-pages? (Y/n)\n[[OP]] Wait for the process to finish.\nOnce it is done, you can go to the URL it asked you about to see your site.\n\nNote: This is the process you will use every time you want to update your published site. Make sure to always follow the steps below for rendering, previewing, and committing your changes before doing these publish steps. Anyone can publish in the future.\n\n\nCustomize your site\n\n[[OP]] Open the _quarto.yml file and update the title to include your team name.\n[[OP]] Go to the about.qmd and remove the TF’s and professor’s names.\nadd your own along with a short introduction and a link to your Github user page.\n[[OP]] Render the site.\n[[OP]] Check and make sure you didn’t get any errors.\n[[OP]] Commit your changes and push.\n[[OP]] Repeat the steps under Setting up your site.\n\nOnce one person is done with this, each teammate in the group can, in turn, repeat steps 3-7. Before doing so, make sure to pull the changes from teammates before starting to make new changes. (We’ll talk soon about ways to organize your work and resolve conflicts.)\n\n\nStart your first post\n\nTo start your first post first, run remotes::install_github(\"sussmanbu/quartopost\") in your Console.\n[[OP]] Run quartopost::quartopost() (or click Addins-&gt;Create Quarto Post, or use C-Shift-P, type “Create Quarto” and press enter to run the command).\n\nNow you can start working on your post. You’ll want to render your post to see what it will look like on the site.\n\nEvery time you want to make a new post, you can repeat step 2 above.\nWhen you want to publish your progress, follow steps 4-7 from Customize your site.\n\nFinally, make sure to read through everything on this site which has the directions and rubric for the final project."
  },
  {
    "objectID": "posts/2024-10-28-blog-post/data_clean.html",
    "href": "posts/2024-10-28-blog-post/data_clean.html",
    "title": "Blog Post 2/3: Data Clean",
    "section": "",
    "text": "In our recent data cleaning work, we started by removing columns that had a high percentage of missing values, specifically those with over 80% missing data, to ensure the reliability of our analysis. We then transformed the format of columns “Local Case Number” to a character format to preserve specific formatting details, and the “Crash Date and Time” to a datetime format for better temporal analysis.\nAdditionally, we addressed missing entries in the “Route Type” column by filling them with the most frequently occurring value, ensuring consistency in our categorical data. We also removed all duplicate entries to maintain the uniqueness of the dataset."
  },
  {
    "objectID": "posts/2024-10-28/data_equity.html",
    "href": "posts/2024-10-28/data_equity.html",
    "title": "Blog Post 2/3: Data Equity",
    "section": "",
    "text": "#Think about how the principles for advancing equitable data practice are relevant to your data.For two or three of principles, write about how they are relevant and what adhering to the practice would entail for your data.Think about what might be some limitations of your analysis and the potential for abuse or misuse of the data.\nWe can utilize the principles for advancing equitable data practices by applying the principles of beneficence, respect for persons, and justice to the exploration of our data set.\nThese principles are relevant for a few reasons. First, looking at beneficience, we may consider how the findings in our data set are being used by local or federal governments, as well as how we are using the data. We can see from this principle what the best actions for the county to take in response to traffic violations would be. Additionally adhering to this practice for our group’s project may look like researching into how the government is using this dataset.\nFurthermore, we can look at the principle of justice. This is relevant to our data as we can see the members of this sample that are facing violations or get into accidents. We can see if they are more at risk than others, and provide some insight through our findings to see if there is any injustice within this county of Marylands response, wheher through law or bill.\nSome limitations of our analysis could be missing data, data not provided or included as a variable, and lack of information on the internet to see the local or federal governments lack of/ or response to this data. Some misuse of the data could be government surveillance in high at-risk areas or strict law enforcement. It could also look like policy changes without community input and in turn renforce community inequities."
  },
  {
    "objectID": "posts/2024-11-11-blog-post/blog-05.html",
    "href": "posts/2024-11-11-blog-post/blog-05.html",
    "title": "Blog Post 5: Additional Datasets",
    "section": "",
    "text": "Link: Pavement Condition Index 2019\nThe dataset I am combining is Montgomery County’s Pavement Condition Index (PCI), which assesses the condition of 5,200 lane miles of roadways. PCI uses a scale from 0-100 to represent road conditions, where lower values indicate poorer conditions (e.g., PCI of 30), and higher values indicate better conditions (e.g., PCI of 80).\nI am matching the geographic locations of crashes with the corresponding PCI values for those road segments, allowing analysis of whether areas with poorer road conditions have higher crash rates. I have identified key variables needed to join the two datasets, focusing on road-related variables, such as road name, in the crash dataset.\nThe challenge I encountered is that the road information between the two datasets doesn’t perfectly align. For example, one dataset has broader location descriptions than the other, making the merge challenging. My next steps are to refine the matching variables to improve accuracy and then conduct a more detailed analysis to examine the relationship between crash rates and PCI values."
  },
  {
    "objectID": "posts/2024-11-11-blog-post/blog-05.html#pavement-condition-index",
    "href": "posts/2024-11-11-blog-post/blog-05.html#pavement-condition-index",
    "title": "Blog Post 5: Additional Datasets",
    "section": "",
    "text": "Link: Pavement Condition Index 2019\nThe dataset I am combining is Montgomery County’s Pavement Condition Index (PCI), which assesses the condition of 5,200 lane miles of roadways. PCI uses a scale from 0-100 to represent road conditions, where lower values indicate poorer conditions (e.g., PCI of 30), and higher values indicate better conditions (e.g., PCI of 80).\nI am matching the geographic locations of crashes with the corresponding PCI values for those road segments, allowing analysis of whether areas with poorer road conditions have higher crash rates. I have identified key variables needed to join the two datasets, focusing on road-related variables, such as road name, in the crash dataset.\nThe challenge I encountered is that the road information between the two datasets doesn’t perfectly align. For example, one dataset has broader location descriptions than the other, making the merge challenging. My next steps are to refine the matching variables to improve accuracy and then conduct a more detailed analysis to examine the relationship between crash rates and PCI values."
  },
  {
    "objectID": "posts/2024-11-11-blog-post/blog-05.html#annual-highway-mileage-reports",
    "href": "posts/2024-11-11-blog-post/blog-05.html#annual-highway-mileage-reports",
    "title": "Blog Post 5: Additional Datasets",
    "section": "Annual Highway Mileage Reports",
    "text": "Annual Highway Mileage Reports\nLink: Annual Highway Mileage Reports\nThe primary dataset, Crash Reporting - Drivers Data, provides detailed information about traffic collisions on county and local roadways, including driver behaviors, environmental conditions, and collision outcomes. To enhance our analysis, we’re incorporating the Annual Highway Mileage Reports from the Maryland Department of Transportation State Highway Administration, which offers complementary information about road infrastructure, usage patterns, and road classifications.\nThe Highway Mileage Reports contain valuable metrics such as annual vehicle miles traveled, functional road classifications, and lane mileage data, which can provide context to our crash analysis. By combining these datasets, we can investigate whether certain road types experience higher crash rates, if traffic volume correlates with accident frequency, and how road infrastructure improvements might impact safety outcomes.\nThe process of combining these datasets requires careful consideration of common linking elements. We’re using geographic location as our primary joining key, specifically focusing on Montgomery County data. The crash data provides specific location information through latitude, longitude, and road names, while the highway mileage data is organized by county and road functional classifications. This allows us to analyze crash patterns in relation to road types and traffic volumes.\nHowever, we’ve encountered several challenges in the data integration process. First, the temporal alignment of the datasets requires attention, as we’re working with crash data from 2015-2023 and need to match it with corresponding annual highway reports. Second, the granularity of data differs between sources - crash data is incident-specific, while highway data is aggregated at the county level. We’re addressing this by creating appropriate aggregation levels that maintain analytical value while ensuring meaningful comparisons.\nOur next steps involve developing more sophisticated analysis methods to account for exposure rates (crashes per vehicle mile traveled) and examining how different road classifications correlate with crash severity. We’re also planning to incorporate additional variables such as lane width and surface type from the highway data to build a more comprehensive understanding of infrastructure-related safety factors."
  },
  {
    "objectID": "posts/2024-11-11-blog-post/blog-05.html#ems-responses---nemsis-maryland-state-data-set",
    "href": "posts/2024-11-11-blog-post/blog-05.html#ems-responses---nemsis-maryland-state-data-set",
    "title": "Blog Post 5: Additional Datasets",
    "section": "EMS Responses - NEMSIS Maryland State Data Set",
    "text": "EMS Responses - NEMSIS Maryland State Data Set\nLink: EMS Responses - Maryland State Data Set\nTo enhance our crash analysis, we are integrating the NEMSIS Maryland State Data Set for EMS responses, which includes detailed information on response times, dispatch times, and patient transfers. By aligning this data with the Montgomery County crash dataset, we aim to assess how EMS response times vary by crash severity, location, and road conditions. This integration will allow us to identify patterns in EMS efficiency, highlighting areas where response times may be longer due to factors like poor road quality or high traffic. One challenge in combining these datasets is accurately matching crashes with EMS records, as location and time details differ slightly across sources. Our next steps involve refining these joins to better understand how EMS response correlates with injury severity and crash frequency in specific areas."
  },
  {
    "objectID": "posts/2024-11-11-blog-post/blog-05.html#marylands-traffic-volume-maps",
    "href": "posts/2024-11-11-blog-post/blog-05.html#marylands-traffic-volume-maps",
    "title": "Blog Post 5: Additional Datasets",
    "section": "Maryland’s Traffic Volume Maps",
    "text": "Maryland’s Traffic Volume Maps\nLink: Maryland Traffic Volume Maps\nFor this analysis, we incorporated Maryland’s Traffic Volume Maps data, which provide Annual Average Daily Traffic (AADT) metrics across multiple roadways in Montgomery County and other regions. By integrating this dataset with our crash and road condition data, we aim to examine how traffic volume correlates with crash rates, particularly in areas with varying pavement conditions. This data enables a more nuanced analysis of risk factors, as higher traffic volumes may influence crash frequency and severity. A key challenge in this integration has been aligning traffic volume data with the specific crash locations due to slight mismatches in geographic identifiers between datasets. Our next steps will involve refining the matching process to ensure accurate associations, followed by statistical analyses to identify any significant relationships between traffic volume, road conditions, and crash incidences."
  },
  {
    "objectID": "posts/2024-11-11-blog-post/blog-05.html#crash-reporting-for-montgomery-county",
    "href": "posts/2024-11-11-blog-post/blog-05.html#crash-reporting-for-montgomery-county",
    "title": "Blog Post 5: Additional Datasets",
    "section": "Crash Reporting for Montgomery County",
    "text": "Crash Reporting for Montgomery County\nLink: Crime Dataset\nTo add in tandem with our Crash Reporting for Montgomery County, MD. I found a data set based on crime data in the same county. This data set offers variables such as location, crime committed, dates, and street names. This could offer some additional relationships we can graph with linear regression to see how crime in the county is related to crashes.\nThis data set provides information from other counties as well, which we would need to filter out, additionally, it is not offered in csv format but we can reformat it and fit it into our project. Additionally, with thirty variables/columns, it may take more time to choose the right ones and investigate possible relationships without the main data set.\nThis information comes from an official US website, DATA.GOV, and was shared for the purpose of public access. This dataset is updated daily by Montgomery Polic Department."
  },
  {
    "objectID": "posts/2024-11-06-blog-post/blog-03-eda-xiang.html",
    "href": "posts/2024-11-06-blog-post/blog-03-eda-xiang.html",
    "title": "EDA",
    "section": "",
    "text": "Setting up the environment and loading the data.\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(viridis)\nlibrary(sf)\nlibrary(viridis)\nlibrary(scales)\nlibrary(RColorBrewer) \n\ncrash_data &lt;- readRDS(\"/Users/xfu/bu-courses-repo/cas-ma-415/ma-4615-project/posts/2024-11-06-blog-post/cleaned_dataset.rds\")\n\nglimpse(crash_data)\n\ncolnames(crash_data)\n\nsummary(crash_data)\n\ncolSums(is.na(crash_data))\n\nhead(crash_data)\n\nstr(crash_data)\n\n\ncrash_data &lt;- crash_data %&gt;%\n  mutate(Hour = hour(Crash.Date.Time))\n\nggplot(crash_data, aes(x = Hour)) +\n  geom_histogram(binwidth = 1, fill = \"steelblue\", alpha = 0.7) +\n  theme_minimal() +\n  labs(title = \"Distribution of Crashes by Hour of Day\",\n       x = \"Hour (24-hour format)\",\n       y = \"Number of Crashes\")\n\ndistraction_fault &lt;- crash_data %&gt;%\n  filter(Driver.Distracted.By != \"UNKNOWN\") %&gt;%\n  group_by(Driver.Distracted.By, Driver.At.Fault) %&gt;%\n  summarise(count = n(), .groups = 'drop') %&gt;%\n  pivot_wider(names_from = Driver.At.Fault, values_from = count, values_fill = 0)\n\ncrash_data %&gt;%\n  filter(Vehicle.Year &gt; 1950 & Vehicle.Year &lt; 2024) %&gt;%  # Remove unlikely years\n  mutate(Vehicle.Age = 2024 - Vehicle.Year) %&gt;%\n  ggplot(aes(x = Vehicle.Age)) +\n  geom_histogram(binwidth = 1, fill = \"darkred\", alpha = 0.7) +\n  theme_minimal() +\n  labs(title = \"Age Distribution of Vehicles Involved in Crashes\",\n       x = \"Vehicle Age (Years)\",\n       y = \"Count\")\n\nggplot(crash_data, aes(x = Longitude, y = Latitude)) +\n  geom_hex(bins = 30) +\n  scale_fill_viridis_c() +\n  theme_minimal() +\n  labs(title = \"Geographic Distribution of Crashes\",\n       fill = \"Crash Count\")\n\ntop_makes &lt;- crash_data %&gt;%\n  group_by(Vehicle.Make) %&gt;%\n  summarise(\n    crash_count = n(),\n    avg_speed_limit = mean(Speed.Limit)\n  ) %&gt;%\n  arrange(desc(crash_count)) %&gt;%\n  head(10)\n\nmovement_analysis &lt;- crash_data %&gt;%\n  group_by(Vehicle.Movement) %&gt;%\n  summarise(\n    crash_count = n(),\n    avg_speed = mean(Speed.Limit),\n    .groups = 'drop'\n  ) %&gt;%\n  arrange(desc(crash_count))\n\nlicense_state_analysis &lt;- crash_data %&gt;%\n  filter(Drivers.License.State != \"\") %&gt;%\n  group_by(Drivers.License.State) %&gt;%\n  summarise(\n    crash_count = n(),\n    at_fault_count = sum(Driver.At.Fault == \"Yes\"),\n    fault_rate = mean(Driver.At.Fault == \"Yes\"),\n    .groups = 'drop'\n  ) %&gt;%\n  arrange(desc(crash_count))\n\nImage 1 shows the temporal distribution of crashes throughout the day, revealing two distinct peak periods: a morning rush hour spike around 8-9 AM with approximately 65 crashes, and a more sustained afternoon/evening peak between 2-6 PM reaching up to 75 crashes per hour. The lowest crash frequencies occur during the early morning hours (3-5 AM) with fewer than 10 crashes, demonstrating the strong correlation between traffic volume and crash occurrence.\nImage 2 presents the age distribution of vehicles involved in crashes, showing a right-skewed distribution with a peak for vehicles between 5-7 years old (approximately 70 crashes). The frequency gradually declines for older vehicles, with a notable drop after 15 years and very few vehicles over 30 years old involved in crashes, likely reflecting both the general age distribution of vehicles on the road and potentially the retirement of older, less safe vehicles.\nImage 3 shows the geographic distribution of crashes using a hexagonal heatmap, with crash density indicated by color intensity. The visualization reveals several high-concentration areas (shown in yellow and green) clustered around latitude 39.1-39.2 and longitude -77.2, suggesting potential crash hotspots that might correspond to high-traffic intersections or challenging road configurations. The pattern appears to follow major transportation corridors, with crash density generally decreasing toward the periphery of the mapped area.\n\nvehicle_year_damage &lt;- crash_data %&gt;%\n  filter(Vehicle.Year &gt; 1950 & Vehicle.Year &lt; 2024) %&gt;%\n  group_by(Vehicle.Year) %&gt;%\n  summarise(\n    crash_count = n(),\n    severe_damage_rate = mean(Vehicle.Damage.Extent == \"DISABLING\"),\n    .groups = 'drop'\n  )\n\nggplot(vehicle_year_damage, aes(x = Vehicle.Year, y = severe_damage_rate)) +\n  geom_point() +\n  geom_smooth(method = \"loess\") +\n  theme_minimal() +\n  labs(title = \"Vehicle Age vs Severe Damage Rate\",\n       x = \"Vehicle Year\",\n       y = \"Rate of Severe Damage\")\n\nThe scatter plot reveals an interesting relationship between vehicle age and severe damage rates in crashes from 1990 to 2020. There’s a noticeable downward trend in severe damage rates from older to newer vehicles, with vehicles from the early 1990s showing the highest rates (around 60-65%) and a significant variability as indicated by some points reaching above 90%. The trend line shows a steady decline until approximately 2010, where it reaches its lowest point of about 35% severe damage rate, suggesting improvements in vehicle safety technology and construction over this period. However, there’s a slight uptick in severe damage rates for the newest vehicles (2015-2020), though this comes with wider confidence intervals as shown by the expanding grey band, possibly due to fewer data points or other contributing factors. The scattered points show considerable variation around the trend line, indicating that while vehicle age is a factor in crash severity, other variables likely play important roles in determining crash outcomes.\n\n# spatial clustering\n# distance-based clusters of crashes\nlibrary(stats)\ncoords &lt;- crash_data %&gt;%\n  select(Longitude, Latitude) %&gt;%\n  as.matrix()\n\n# hierarchical clustering\ndist_matrix &lt;- dist(coords)\nclusters &lt;- hclust(dist_matrix, method = \"complete\")\ncrash_data$cluster &lt;- cutree(clusters, k = 5)\n\nggplot(crash_data, aes(x = Longitude, y = Latitude, color = factor(cluster))) +\n  geom_point(alpha = 0.6) +\n  theme_minimal() +\n  labs(title = \"Spatial Clusters of Crashes\",\n       color = \"Cluster\")\n\nThis scatter plot shows the spatial distribution of crashes across different geographical coordinates, with clusters indicated by different colors. The data reveals distinct spatial patterns with five clearly separated clusters. The largest concentrations appear in clusters 1 (coral), 3 (green), and 4 (blue), forming a diagonal pattern from northeast to southwest. Cluster 3, located around longitude -77.3 and latitude 39.2, shows the highest density of crashes, while clusters 1 and 4 appear more spread out. Two smaller clusters are also visible: cluster 2 (olive) consists of a single point in the southwest corner, and cluster 5 (pink) shows a sparse distribution of crashes in the western portion of the map. The clear separation between clusters suggests that these crashes may be concentrated around specific road features, intersections, or high-traffic areas that could benefit from targeted safety interventions.\n\n# route type\nroute_analysis &lt;- crash_data %&gt;%\n  filter(Route.Type != \"\") %&gt;%\n  group_by(Route.Type) %&gt;%\n  summarise(\n    crash_count = n(),\n    injury_rate = mean(ACRS.Report.Type == \"Injury Crash\"),\n    avg_speed = mean(Speed.Limit),\n    most_common_collision = names(which.max(table(Collision.Type))),\n    .groups = 'drop'\n  )\n\nroute_analysis_long &lt;- route_analysis %&gt;%\n  # normalize crash_count and avg_speed to 0-1 scale for better comparison\n  mutate(\n    crash_count_norm = (crash_count - min(crash_count)) / (max(crash_count) - min(crash_count)),\n    avg_speed_norm = (avg_speed - min(avg_speed)) / (max(avg_speed) - min(avg_speed))\n  ) %&gt;%\n  # convert to long format for faceting\n  pivot_longer(\n    cols = c(crash_count_norm, injury_rate, avg_speed_norm),\n    names_to = \"metric\",\n    values_to = \"value\"\n  ) %&gt;%\n  mutate(\n    metric = case_when(\n      metric == \"crash_count_norm\" ~ \"Crash Count (Normalized)\",\n      metric == \"injury_rate\" ~ \"Injury Rate\",\n      metric == \"avg_speed_norm\" ~ \"Average Speed (Normalized)\"\n    )\n  )\n\nggplot(route_analysis_long, aes(x = reorder(Route.Type, value), y = value)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\", alpha = 0.8) +\n  geom_text(aes(label = sprintf(\"%.2f\", value)), \n            hjust = -0.1, \n            size = 3) +\n  facet_wrap(~metric, scales = \"free_y\", ncol = 1) +\n  coord_flip() +\n  theme_minimal() +\n  theme(\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank(),\n    axis.title.y = element_blank(),\n    strip.text = element_text(face = \"bold\", size = 10),\n    plot.title = element_text(face = \"bold\", size = 12),\n    plot.subtitle = element_text(size = 9, color = \"gray50\")\n  ) +\n  labs(\n    title = \"Route Type Analysis: Metrics Comparison\",\n    subtitle = \"Comparing crash frequency, injury rates, and average speed limits across different route types\",\n    y = \"Value\"\n  )\n\nWe can see some comparison of road safety metrics across different route types using normalized values. The data reveals some striking patterns: while Interstate routes have the highest normalized average speed (1.0) and ramps follow with relatively high speeds (0.61), Maryland State routes dominate in terms of crash frequency with a normalized count of 1.0, followed closely by County routes at 0.86, while other route types show significantly lower crash counts. Interestingly, when it comes to injury rates, Other Public Roadways show the highest rate at 0.44, followed by Maryland State routes at 0.38, suggesting that while State routes have more crashes overall, the likelihood of injury is actually higher on public roadways. This could indicate that while high-speed routes like Interstates have safety measures that effectively prevent injuries despite their speed, lower-speed but less controlled environments might pose a higher risk for severe outcomes when crashes do occur.\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(scales)\n\n# hourly summary\ncrash_summary &lt;- crash_data %&gt;%\n  mutate(Hour = hour(Crash.Date.Time)) %&gt;%\n  group_by(Hour) %&gt;%\n  summarise(\n    Total_Crashes = n(),\n    Not_Distracted = sum(Driver.Distracted.By == \"NOT DISTRACTED\"),\n    Distracted = sum(Driver.Distracted.By != \"NOT DISTRACTED\" & \n                    Driver.Distracted.By != \"UNKNOWN\"),\n    Unknown = sum(Driver.Distracted.By == \"UNKNOWN\")\n  ) %&gt;%\n  # convert to long format for line plotting\n  pivot_longer(\n    cols = c(Not_Distracted, Distracted, Unknown),\n    names_to = \"Distraction_Status\",\n    values_to = \"Count\"\n  )\n\nggplot(crash_summary, aes(x = Hour, y = Count, color = Distraction_Status)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 3) +\n  geom_text(data = crash_summary %&gt;%\n              group_by(Distraction_Status) %&gt;%\n              slice_max(Count, n = 1),\n            aes(label = Count),\n            vjust = -0.5,\n            size = 3.5) +\n  scale_color_manual(\n    values = c(\"Not_Distracted\" = \"#66C2A5\",\n              \"Distracted\" = \"#FC8D62\",\n              \"Unknown\" = \"#8DA0CB\"),\n    labels = c(\"Not Distracted\", \"Distracted\", \"Unknown Status\")\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23, 2),\n    labels = sprintf(\"%02d:00\", seq(0, 23, 2))\n  ) +\n  scale_y_continuous(\n    breaks = seq(0, max(crash_summary$Count), by = 10),\n    expand = c(0, 2)\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(color = \"gray30\", size = 11),\n    legend.position = \"right\",\n    legend.title = element_text(face = \"bold\"),\n    legend.text = element_text(size = 10),\n    axis.title = element_text(face = \"bold\"),\n    axis.text = element_text(size = 9),\n    panel.grid.minor = element_blank(),\n    legend.box.background = element_rect(color = \"gray80\", fill = \"white\")\n  ) +\n  labs(\n    title = \"Hourly Distribution of Crashes by Driver Distraction Status\",\n    subtitle = \"Showing patterns of distracted vs. non-distracted driving crashes\",\n    x = \"Time of Day\",\n    y = \"Number of Crashes\",\n    color = \"Driver\\nStatus\"\n  )\n\npeak_hours &lt;- crash_summary %&gt;%\n  group_by(Hour) %&gt;%\n  summarise(\n    Total = sum(Count),\n    .groups = 'drop'\n  ) %&gt;%\n  arrange(desc(Total)) %&gt;%\n  head(5)\n\nprint(\"Peak Crash Hours:\")\nprint(peak_hours)\n\nThis time series plot reveals a pattern in the hourly distribution of crashes categorized by driver distraction status. The data shows distinct peaks in distracted driving crashes, with the highest spike of 45 crashes occurring around 15:00 (3 PM), followed by significant peaks at 8:00 AM and 12:00 PM, suggesting a strong correlation with typical rush hour and lunch break periods. In contrast, non-distracted crashes and those with unknown status show more moderate fluctuations, with their highest peaks reaching only about half the magnitude of distracted crashes (24 and 22 crashes respectively) around 15:00-16:00. The overnight period from 22:00 to 04:00 shows consistently low crash numbers across all categories, indicating that despite lower traffic volumes, the proportion of distracted driving crashes remains relatively stable during these hours.\n\ngeo_movement_analysis &lt;- crash_data %&gt;%\n  # filter out any invalid coordinates and empty movement patterns\n  filter(!is.na(Latitude), !is.na(Longitude), \n         Vehicle.Movement != \"\", \n         Vehicle.Movement != \"UNKNOWN\") %&gt;%\n  mutate(\n    Movement_Category = case_when(\n      Vehicle.Movement %in% c(\"MOVING CONSTANT SPEED\", \"ACCELERATING\", \"SLOWING OR STOPPING\") ~ \"Normal Traffic Flow\",\n      Vehicle.Movement %in% c(\"PARKING\", \"PARKED\", \"BACKING\") ~ \"Parking Related\",\n      Vehicle.Movement %in% c(\"MAKING LEFT TURN\", \"MAKING RIGHT TURN\", \"U-TURN\") ~ \"Turning Movements\",\n      TRUE ~ \"Other Movements\"\n    )\n  )\n\nggplot(geo_movement_analysis) +\n  geom_hex(aes(x = Longitude, y = Latitude, fill = ..count..), \n           bins = 30) +\n  scale_fill_viridis(option = \"plasma\", name = \"Crash Count\") +\n  facet_wrap(~Movement_Category) +\n  theme_minimal() +\n  theme(\n    legend.position = \"right\",\n    plot.title = element_text(face = \"bold\"),\n    plot.subtitle = element_text(color = \"grey30\"),\n    strip.text = element_text(face = \"bold\")\n  ) +\n  labs(\n    title = \"Geographic Distribution of Crashes by Movement Type\",\n    subtitle = \"Spatial patterns of different vehicle movements leading to crashes\",\n    x = \"Longitude\",\n    y = \"Latitude\"\n  )\n\nhotspot_analysis &lt;- geo_movement_analysis %&gt;%\n  group_by(Movement_Category) %&gt;%\n  summarise(\n    crash_count = n(),\n    avg_speed_limit = mean(Speed.Limit),\n    injury_crashes = sum(ACRS.Report.Type == \"Injury Crash\"),\n    injury_rate = injury_crashes / n(),\n    .groups = 'drop'\n  ) %&gt;%\n  arrange(desc(crash_count))\n\nprint(hotspot_analysis)\n\nmovement_time_analysis &lt;- geo_movement_analysis %&gt;%\n  mutate(Hour = hour(Crash.Date.Time)) %&gt;%\n  group_by(Movement_Category, Hour) %&gt;%\n  summarise(\n    crash_count = n(),\n    .groups = 'drop'\n  )\n\nggplot(movement_time_analysis, \n       aes(x = Hour, y = crash_count, color = Movement_Category)) +\n  geom_line(size = 1) +\n  geom_point(size = 2) +\n  scale_x_continuous(breaks = 0:23) +\n  scale_color_viridis_d() +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    axis.text.x = element_text(angle = 45)\n  ) +\n  labs(\n    title = \"Crash Patterns Throughout the Day by Movement Type\",\n    x = \"Hour of Day\",\n    y = \"Number of Crashes\",\n    color = \"Movement Type\"\n  )\n\nThe temporal distribution of crashes by movement type shows a clear dominance of normal traffic flow incidents throughout the day, with two prominent peaks: one during the morning rush hour around 9:00 (39 crashes) and an even higher peak during the evening rush hour at 18:00 (45 crashes). Other movement types (turning movements, parking-related, and other movements) maintain relatively consistent but lower crash frequencies, typically ranging between 5-15 crashes per hour, with subtle increases during daylight hours. The pattern suggests that while regular traffic flow poses the highest crash risk, especially during peak commuting hours, the other movement types contribute a persistent but lower baseline of crash incidents, with turning movements showing slightly higher numbers than parking-related incidents during business hours, particularly between 13:00-17:00.\n\nvehicle_analysis &lt;- crash_data %&gt;%\n  # filter out invalid years and create vehicle age\n  filter(Vehicle.Year &gt; 1950 & Vehicle.Year &lt; 2024) %&gt;%\n  mutate(\n    Vehicle.Age = 2024 - Vehicle.Year,\n    Age.Category = case_when(\n      Vehicle.Age &lt;= 5 ~ \"New (0-5 years)\",\n      Vehicle.Age &lt;= 10 ~ \"Recent (6-10 years)\",\n      Vehicle.Age &lt;= 15 ~ \"Mature (11-15 years)\",\n      TRUE ~ \"Older (15+ years)\"\n    ),\n    Age.Category = factor(Age.Category, \n                         levels = c(\"New (0-5 years)\", \n                                  \"Recent (6-10 years)\", \n                                  \"Mature (11-15 years)\", \n                                  \"Older (15+ years)\"))\n  )\n\nggplot(vehicle_analysis, aes(x = Age.Category, fill = Vehicle.Damage.Extent)) +\n  geom_bar(position = \"fill\") +\n  scale_fill_viridis_d() +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"right\",\n    plot.title = element_text(face = \"bold\")\n  ) +\n  labs(\n    title = \"Vehicle Age Category vs. Damage Extent\",\n    subtitle = \"Distribution of damage severity across different vehicle age groups\",\n    x = \"Vehicle Age Category\",\n    y = \"Proportion of Crashes\",\n    fill = \"Damage Extent\"\n  )\n\ntop_manufacturers &lt;- vehicle_analysis %&gt;%\n  group_by(Vehicle.Make) %&gt;%\n  summarise(\n    total_crashes = n(),\n    severe_crashes = sum(Vehicle.Damage.Extent %in% c(\"DISABLING\", \"SEVERE\")),\n    severe_crash_rate = severe_crashes / total_crashes,\n    avg_vehicle_age = mean(Vehicle.Age),\n    fault_rate = mean(Driver.At.Fault == \"Yes\", na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  filter(total_crashes &gt;= 20) %&gt;%  # Filter for manufacturers with significant data\n  arrange(desc(total_crashes))\n\nggplot(head(top_manufacturers, 10), \n       aes(x = reorder(Vehicle.Make, total_crashes), \n           y = total_crashes)) +\n  geom_col(aes(fill = severe_crash_rate)) +\n  geom_text(aes(label = sprintf(\"%.1f%%\", severe_crash_rate * 100)),\n            hjust = -0.1,\n            size = 3) +\n  coord_flip() +\n  scale_fill_gradient(low = \"lightblue\", high = \"darkred\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    legend.position = \"right\"\n  ) +\n  labs(\n    title = \"Top 10 Vehicle Makes Involved in Crashes\",\n    subtitle = \"Showing total crashes and severe crash rate percentage\",\n    x = \"Vehicle Make\",\n    y = \"Total Number of Crashes\",\n    fill = \"Severe Crash Rate\"\n  )\n\nprint(\"Summary of Top Manufacturer Patterns\")\nmanufacturer_patterns &lt;- top_manufacturers %&gt;%\n  select(Vehicle.Make, total_crashes, severe_crash_rate, avg_vehicle_age, fault_rate) %&gt;%\n  mutate(\n    severe_crash_rate = percent(severe_crash_rate, accuracy = 0.1),\n    fault_rate = percent(fault_rate, accuracy = 0.1),\n    avg_vehicle_age = round(avg_vehicle_age, 1)\n  ) %&gt;%\n  arrange(desc(total_crashes))\n\nprint(manufacturer_patterns)\n\nWhile Toyota and Honda lead in total crash numbers with around 150 crashes each, they maintain relatively moderate severe crash rates of 41% and 40.7% respectively. Interestingly, some manufacturers with fewer total crashes show higher severity rates - KIA stands out with the highest severe crash rate at 48.3%, followed by Nissan at 46.9%, despite having significantly fewer total crashes. Chevrolet and Dodge, with the lowest crash counts among the top 10, also show the lowest severe crash rates at 25.9% and 24.0% respectively.\nFrom “Vehicle Age Category vs. Damage Extent”, we can see a clear relationship between vehicle age and damage severity in crashes. The proportion of disabling and destroyed vehicles notably increases with vehicle age, being most pronounced in the Older (15+ years) category. While superficial damage remains relatively consistent across age groups at around 25-30%, functional damage decreases with vehicle age, particularly in the oldest category. This suggests that older vehicles are more susceptible to severe damage in crashes, possibly due to their aging structural integrity and lack of modern safety features.\n\nvehicle_analysis &lt;- crash_data %&gt;%\n  filter(Vehicle.Year &gt; 1950 & Vehicle.Year &lt; 2024) %&gt;%\n  mutate(\n    Vehicle.Age = 2024 - Vehicle.Year,\n    Age.Category = case_when(\n      Vehicle.Age &lt;= 5 ~ \"New (0-5 years)\",\n      Vehicle.Age &lt;= 10 ~ \"Recent (6-10 years)\",\n      Vehicle.Age &lt;= 15 ~ \"Mature (11-15 years)\",\n      TRUE ~ \"Older (15+ years)\"\n    ),\n    Age.Category = factor(Age.Category, \n                         levels = c(\"New (0-5 years)\", \n                                  \"Recent (6-10 years)\", \n                                  \"Mature (11-15 years)\", \n                                  \"Older (15+ years)\"))\n  )\n\nggplot(vehicle_analysis, \n       aes(x = Age.Category, \n           fill = Vehicle.Damage.Extent)) +\n  geom_bar(position = \"dodge\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"right\",\n    plot.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    title = \"Vehicle Age Category vs. Damage Extent\",\n    subtitle = \"Number of crashes by damage type for each vehicle age group\",\n    x = \"Vehicle Age Category\",\n    y = \"Number of Crashes\",\n    fill = \"Damage Extent\"\n  ) +\n  geom_text(\n    aes(label = after_stat(count)),\n    stat = \"count\",\n    position = position_dodge(width = 0.9),\n    vjust = -0.5,\n    size = 3\n  )\n\nThis bar chart reveals the absolute numbers of crashes by damage type across vehicle age categories, providing a more detailed view of crash severity patterns. Recent vehicles (6-10 years) experience the highest number of disabling crashes at 116 incidents, followed by new vehicles (0-5 years) with 93 disabling crashes. Functional damage shows a declining trend with vehicle age, from 84 cases in recent vehicles to 40 cases in older vehicles (15+ years). Superficial damage follows a similar decreasing pattern, with the highest numbers in newer vehicles (67 cases) declining to 41 cases in older vehicles. Interestingly, while destroyed vehicles remain relatively low across all categories, there’s a slight increase in the older vehicle category with 17 cases, suggesting that while older vehicles may have fewer crashes overall, they’re more likely to be completely destroyed when they do crash.\n\ncrash_timing &lt;- crash_data %&gt;%\n  mutate(\n    Hour = hour(Crash.Date.Time),\n    Time_Block = case_when(\n      Hour &gt;= 5 & Hour &lt; 9 ~ \"Morning Peak (5-8)\",\n      Hour &gt;= 9 & Hour &lt; 16 ~ \"Mid-Day (9-15)\",\n      Hour &gt;= 16 & Hour &lt; 19 ~ \"Evening Peak (16-18)\",\n      Hour &gt;= 19 & Hour &lt; 22 ~ \"Evening (19-21)\",\n      TRUE ~ \"Night (22-4)\"\n    ),\n    Time_Block = factor(Time_Block, \n                       levels = c(\"Morning Peak (5-8)\", \n                                \"Mid-Day (9-15)\",\n                                \"Evening Peak (16-18)\", \n                                \"Evening (19-21)\",\n                                \"Night (22-4)\")),\n    Has_Injury = ACRS.Report.Type == \"Injury Crash\",\n    Light = factor(Light, levels = c(\"DAYLIGHT\", \n                                   \"DARK LIGHTS ON\",\n                                   \"DARK NO LIGHTS\",\n                                   \"DAWN\",\n                                   \"DUSK\",\n                                   \"DARK -- UNKNOWN LIGHTING\",\n                                   \"UNKNOWN\",\n                                   \"OTHER\",\n                                   \"N/A\"))\n  )\n\nggplot(crash_timing, aes(x = Hour)) +\n  geom_line(stat = \"count\", aes(color = Light), size = 1) +\n  scale_x_continuous(breaks = seq(0, 23, 2)) +\n  scale_color_brewer(palette = \"Set2\") +\n  scale_y_continuous(expand = c(0, 0)) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 0),\n    legend.position = \"right\",\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(color = \"gray30\", size = 11),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    legend.title = element_text(face = \"bold\"),\n    axis.title = element_text(face = \"bold\")\n  ) +\n  labs(\n    title = \"Crash Distribution Throughout the Day\",\n    subtitle = \"Showing crash frequency and lighting conditions for each hour\",\n    x = \"Hour of Day (24-hour format)\",\n    y = \"Number of Crashes\",\n    color = \"Light\\nCondition\"\n  )\n\nThe line graph shows the temporal distribution of crashes across different lighting conditions throughout a 24-hour period, revealing some interesting patterns in crash frequency. Daylight crashes show two prominent peaks: one during the morning rush hour around 8:00 (approximately 62 crashes) and a higher peak in the early afternoon around 14:00 (approximately 75 crashes), followed by a sharp decline as daylight fades. As expected, “Dark Lights On” crashes become prevalent during evening hours, showing a moderate peak around 19:00-20:00 (about 28 crashes), while maintaining relatively consistent numbers throughout the nighttime hours. Other lighting conditions, including dawn, dusk, and dark with no lights, show notably lower crash frequencies but exhibit small spikes during their respective natural occurrence periods - dawn crashes peak around 6:00-7:00, and dusk-related incidents show a minor increase around 17:00-18:00, corresponding to typical twilight hours.\n\ncrash_summary &lt;- crash_data %&gt;%\n  mutate(Hour = hour(Crash.Date.Time)) %&gt;%\n  group_by(Hour) %&gt;%\n  summarise(\n    Total_Crashes = n(),\n    Daylight_Crashes = sum(Light == \"DAYLIGHT\"),\n    Dark_Lights_On = sum(Light == \"DARK LIGHTS ON\"),\n    Dark_No_Lights = sum(Light == \"DARK NO LIGHTS\")\n  ) %&gt;%\n  pivot_longer(\n    cols = c(Daylight_Crashes, Dark_Lights_On, Dark_No_Lights),\n    names_to = \"Condition\",\n    values_to = \"Count\"\n  ) %&gt;%\n  mutate(\n    Condition = case_when(\n      Condition == \"Daylight_Crashes\" ~ \"Daylight\",\n      Condition == \"Dark_Lights_On\" ~ \"Dark (Lights On)\",\n      Condition == \"Dark_No_Lights\" ~ \"Dark (No Lights)\"\n    )\n  )\n\nggplot(crash_summary, aes(x = Hour, y = Count, color = Condition)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 3) +\n  scale_color_manual(\n    values = c(\"Daylight\" = \"#4E79A7\",\n              \"Dark (Lights On)\" = \"#F28E2B\",\n              \"Dark (No Lights)\" = \"#59A14F\")\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23, 2),\n    labels = sprintf(\"%02d:00\", seq(0, 23, 2))\n  ) +\n  scale_y_continuous(\n    breaks = seq(0, 80, 10),\n    expand = c(0, 2)\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(color = \"gray30\", size = 11),\n    legend.position = \"right\",\n    legend.title = element_text(face = \"bold\"),\n    legend.text = element_text(size = 10),\n    axis.title = element_text(face = \"bold\"),\n    axis.text = element_text(size = 9),\n    panel.grid.minor = element_blank(),\n    legend.box.background = element_rect(color = \"gray80\", fill = \"white\")\n  ) +\n  labs(\n    title = \"Hourly Distribution of Crashes by Light Condition\",\n    subtitle = \"Showing the three main lighting conditions throughout the day\",\n    x = \"Time of Day\",\n    y = \"Number of Crashes\",\n    color = \"Light Condition\"\n  )\n\nThe hourly distribution of crashes by light condition reveals distinct patterns that align with natural daylight cycles and peak traffic hours. Daylight crashes show two prominent peaks: a morning rush hour spike around 8:00 with approximately 62 crashes, and an even higher afternoon peak at 14:00 (2 PM) with about 75 crashes. As daylight fades in the evening hours after 16:00 (4 PM), there’s a clear transition where “Dark (Lights On)” crashes become predominant, reaching their peak of around 28 crashes during the evening commute hours. Crashes in “Dark (No Lights)” conditions remain consistently low throughout the 24-hour period, never exceeding 5 crashes per hour, suggesting that while these conditions are potentially hazardous, they occur less frequently or drivers may be more cautious in unlit conditions.\n\ndriver_analysis &lt;- crash_data %&gt;%\n  filter(\n    Driver.Distracted.By != \"UNKNOWN\",\n    Driver.Distracted.By != \"N/A\",\n    Vehicle.Damage.Extent != \"UNKNOWN\",\n    Vehicle.Damage.Extent != \"N/A\",\n    Driver.At.Fault != \"UNKNOWN\"\n  ) %&gt;%\n  mutate(\n    Distraction_Type = case_when(\n      Driver.Distracted.By == \"NOT DISTRACTED\" ~ \"Not Distracted\",\n      str_detect(Driver.Distracted.By, \"CELL|PHONE|TEXT\") ~ \"Phone Usage\",\n      str_detect(Driver.Distracted.By, \"EATING|DRINKING\") ~ \"Eating/Drinking\",\n      TRUE ~ \"Other Distraction\"\n    ),\n    Damage_Level = case_when(\n      Vehicle.Damage.Extent %in% c(\"DESTROYED\", \"DISABLING\") ~ \"Severe\",\n      Vehicle.Damage.Extent == \"FUNCTIONAL\" ~ \"Moderate\",\n      Vehicle.Damage.Extent %in% c(\"SUPERFICIAL\", \"NO DAMAGE\") ~ \"Minor\",\n      TRUE ~ \"Other\"\n    ),\n    At_Fault = Driver.At.Fault == \"Yes\"\n  )\n\ndistraction_summary &lt;- driver_analysis %&gt;%\n  group_by(Distraction_Type, Damage_Level) %&gt;%\n  summarise(\n    Total_Cases = n(),\n    Fault_Rate = mean(At_Fault),\n    .groups = 'drop'\n  ) %&gt;%\n  mutate(\n    Distraction_Type = factor(Distraction_Type, \n                            levels = c(\"Not Distracted\", \"Phone Usage\", \n                                     \"Eating/Drinking\", \"Other Distraction\")),\n    Damage_Level = factor(Damage_Level, \n                         levels = c(\"Minor\", \"Moderate\", \"Severe\"))\n  )\n\nggplot(distraction_summary, \n       aes(x = Distraction_Type, y = Total_Cases, fill = Damage_Level)) +\n  geom_col(position = \"dodge\", width = 0.7) +\n  geom_text(aes(label = sprintf(\"%.0f%%\", Fault_Rate * 100)),\n            position = position_dodge(width = 0.7),\n            vjust = -0.5,\n            size = 3.5,\n            fontface = \"bold\") +\n  scale_fill_manual(\n    values = c(\"Minor\" = \"#66C2A5\",\n              \"Moderate\" = \"#FC8D62\",\n              \"Severe\" = \"#8DA0CB\")\n  ) +\n  scale_y_continuous(\n    expand = expansion(mult = c(0, 0.1)),\n    breaks = seq(0, max(distraction_summary$Total_Cases), by = 50)\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(color = \"gray30\", size = 11),\n    axis.title = element_text(face = \"bold\"),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank(),\n    legend.position = \"right\",\n    legend.box.background = element_rect(color = \"gray80\", fill = \"white\")\n  ) +\n  labs(\n    title = \"Driver Distraction and Crash Severity Patterns\",\n    subtitle = \"Numbers above bars show percentage of at-fault cases\",\n    x = \"Type of Distraction\",\n    y = \"Number of Crashes\",\n    fill = \"Damage\\nSeverity\"\n  )\n\ndistraction_stats &lt;- driver_analysis %&gt;%\n  group_by(Distraction_Type) %&gt;%\n  summarise(\n    Total_Crashes = n(),\n    Fault_Rate = mean(At_Fault),\n    Severe_Damage_Rate = mean(Damage_Level == \"Severe\"),\n    .groups = 'drop'\n  ) %&gt;%\n  arrange(desc(Total_Crashes))\n\nprint(distraction_stats)\n\nThis bar chart reveals patterns in crash severity across different types of driver distraction, with percentages indicating at-fault cases for each category. Non-distracted driving shows a relatively balanced distribution of crash severities, but with notably lower at-fault rates (21-33%) compared to distracted driving scenarios. The most striking finding is that all distracted driving categories - phone usage, eating/drinking, and other distractions - show extremely high at-fault rates of 87-100%. Of particular concern is the “Other Distraction” category, which not only shows high numbers of crashes across all severity levels but also maintains extremely high at-fault rates (94-96%). Phone usage and eating/drinking, while showing fewer total incidents, result in 100% at-fault severe crashes, highlighting the serious dangers of these specific distraction types.\n\ngeo_analysis &lt;- crash_data %&gt;%\n  mutate(\n    Hour = hour(Crash.Date.Time),\n    Time_Category = case_when(\n      Hour &gt;= 5 & Hour &lt; 12 ~ \"Morning (5-11)\",\n      Hour &gt;= 12 & Hour &lt; 17 ~ \"Afternoon (12-16)\",\n      Hour &gt;= 17 & Hour &lt; 22 ~ \"Evening (17-21)\",\n      TRUE ~ \"Night (22-4)\"\n    ),\n    Is_Injury = ACRS.Report.Type == \"Injury Crash\"\n  ) %&gt;%\n  filter(!is.na(Latitude), !is.na(Longitude))\n\nggplot(geo_analysis, aes(x = Longitude, y = Latitude)) +\n  facet_wrap(~Time_Category, ncol = 2) +\n  geom_hex(bins = 20, aes(fill = ..count..)) +\n  scale_fill_viridis(\n    option = \"plasma\",\n    name = \"Number\\nof Crashes\",\n    labels = scales::number_format()\n  ) +\n  coord_cartesian(\n    xlim = c(min(geo_analysis$Longitude) + 0.05, \n             max(geo_analysis$Longitude) - 0.05),\n    ylim = c(min(geo_analysis$Latitude) + 0.05, \n             max(geo_analysis$Latitude) - 0.05)\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(color = \"gray30\", size = 11),\n    strip.text = element_text(face = \"bold\", size = 11),\n    legend.title = element_text(face = \"bold\"),\n    axis.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank(),\n    legend.position = \"right\",\n    legend.box.background = element_rect(color = \"gray80\", fill = \"white\")\n  ) +\n  labs(\n    title = \"Geographic Distribution of Crashes by Time of Day\",\n    subtitle = \"Hexagonal heatmap showing crash concentration patterns\",\n    x = \"Longitude\",\n    y = \"Latitude\"\n  )\n\narea_summary &lt;- geo_analysis %&gt;%\n  mutate(\n    Area = case_when(\n      Longitude &gt; median(Longitude) & Latitude &gt; median(Latitude) ~ \"Northeast\",\n      Longitude &lt;= median(Longitude) & Latitude &gt; median(Latitude) ~ \"Northwest\",\n      Longitude &gt; median(Longitude) & Latitude &lt;= median(Latitude) ~ \"Southeast\",\n      TRUE ~ \"Southwest\"\n    )\n  ) %&gt;%\n  group_by(Area, Time_Category) %&gt;%\n  summarise(\n    Total_Crashes = n(),\n    Injury_Rate = mean(Is_Injury),\n    .groups = 'drop'\n  )\n\nggplot(area_summary, \n       aes(x = Area, y = Total_Crashes, fill = Time_Category)) +\n  geom_col(position = \"dodge\", width = 0.7) +\n  geom_text(aes(label = sprintf(\"%.1f%%\", Injury_Rate * 100)),\n            position = position_dodge(width = 0.7),\n            vjust = -0.5,\n            size = 3.5) +\n  scale_fill_brewer(palette = \"Set2\") +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.title = element_text(face = \"bold\"),\n    legend.position = \"right\"\n  ) +\n  labs(\n    title = \"Crash Distribution by Area and Time\",\n    subtitle = \"Numbers show injury rates for each category\",\n    x = \"Area\",\n    y = \"Number of Crashes\",\n    fill = \"Time of Day\"\n  )\n\nFrom this, we try to show the crash patterns across different geographic areas and times of day, with injury rates displayed for each category. The Northwest area shows the highest overall crash numbers and notably high injury rates across all time periods, with evening crashes having a particularly high injury rate of 41.8%. The Southeast region experiences its highest crash volumes in the afternoon with a 24.5% injury rate, but shows a concerning spike in injury rates during night hours at 29.7%. Throughout all areas, night-time crashes (22-4) consistently show lower total numbers but varying injury rates, from 10% in the Southwest to 29.7% in the Southeast, suggesting that different areas face distinct night-time driving challenges. The Southwest area, while having fewer total crashes, maintains relatively high injury rates during afternoon (32.5%) and evening (37%) periods, indicating a need for targeted safety measures during these times.\n\nmovement_analysis &lt;- crash_data %&gt;%\n  filter(\n    Vehicle.Movement != \"\",\n    Vehicle.Movement != \"UNKNOWN\",\n    Weather != \"\",\n    Weather != \"UNKNOWN\",\n    Collision.Type != \"\",\n    Collision.Type != \"UNKNOWN\"\n  ) %&gt;%\n  mutate(\n    Movement_Type = case_when(\n      Vehicle.Movement %in% c(\"MAKING LEFT TURN\", \"MAKING RIGHT TURN\", \"U-TURN\") ~ \"Turning\",\n      Vehicle.Movement %in% c(\"MOVING CONSTANT SPEED\", \"ACCELERATING\") ~ \"Moving Forward\",\n      Vehicle.Movement %in% c(\"SLOWING OR STOPPING\", \"STOPPED IN TRAFFIC\") ~ \"Slowing/Stopped\",\n      Vehicle.Movement %in% c(\"PARKING\", \"PARKED\", \"BACKING\") ~ \"Parking Related\",\n      TRUE ~ \"Other\"\n    ),\n    Weather_Type = case_when(\n      Weather == \"CLEAR\" ~ \"Clear\",\n      Weather %in% c(\"CLOUDY\", \"FOGGY\") ~ \"Cloudy/Foggy\",\n      Weather %in% c(\"RAIN\", \"SLEET\", \"SNOW\") ~ \"Precipitation\",\n      TRUE ~ \"Other\"\n    )\n  )\n\nmovement_summary &lt;- movement_analysis %&gt;%\n  group_by(Movement_Type, Weather_Type) %&gt;%\n  summarise(\n    Crash_Count = n(),\n    Most_Common_Collision = names(which.max(table(Collision.Type))),\n    Collision_Percentage = max(table(Collision.Type)) / n(),\n    .groups = 'drop'\n  ) %&gt;%\n  group_by(Weather_Type) %&gt;%\n  mutate(\n    Weather_Total = sum(Crash_Count),\n    Percentage = Crash_Count / Weather_Total\n  ) %&gt;%\n  ungroup()\n\nggplot(movement_summary, \n       aes(x = Weather_Type, y = Crash_Count, fill = Movement_Type)) +\n  geom_col(position = \"fill\", width = 0.7) +\n  geom_text(aes(label = sprintf(\"%.1f%%\", Percentage * 100)),\n            position = position_fill(vjust = 0.5),\n            color = \"white\",\n            size = 3.5,\n            fontface = \"bold\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_y_continuous(labels = scales::percent) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(color = \"gray30\", size = 11),\n    axis.title = element_text(face = \"bold\"),\n    legend.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank(),\n    legend.position = \"right\",\n    legend.box.background = element_rect(color = \"gray80\", fill = \"white\")\n  ) +\n  labs(\n    title = \"Vehicle Movement Patterns Under Different Weather Conditions\",\n    subtitle = \"Showing the distribution of vehicle movements and their relative frequencies\",\n    x = \"Weather Condition\",\n    y = \"Proportion of Crashes\",\n    fill = \"Movement\\nType\"\n  )\n\ncollision_summary &lt;- movement_analysis %&gt;%\n  group_by(Movement_Type, Collision.Type) %&gt;%\n  summarise(\n    Count = n(),\n    .groups = 'drop'\n  ) %&gt;%\n  group_by(Movement_Type) %&gt;%\n  slice_max(Count, n = 2) %&gt;%\n  mutate(Percentage = Count / sum(Count))\n\nggplot(collision_summary,\n       aes(y = reorder(Movement_Type, Count), \n           x = Count, \n           fill = Collision.Type)) +\n  geom_col(position = \"dodge\") +\n  geom_text(aes(label = sprintf(\"%.0f%%\", Percentage * 100)),\n            position = position_dodge(width = 0.9),\n            hjust = -0.2,\n            size = 3.5) +\n  scale_fill_brewer(palette = \"Set3\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    axis.title = element_text(face = \"bold\"),\n    legend.title = element_text(face = \"bold\")\n  ) +\n  labs(\n    title = \"Most Common Collision Types by Vehicle Movement\",\n    subtitle = \"Numbers show percentage within each movement type\",\n    x = \"Number of Crashes\",\n    y = \"Movement Type\",\n    fill = \"Collision Type\"\n  )\n\nImage 1 reveals that moving forward is the predominant vehicle movement pattern across all weather conditions, but its proportion notably increases during precipitation (60%) compared to clear conditions (45.6%). During cloudy/foggy conditions, there’s a marked increase in slowing/stopped vehicles (20.9%) compared to clear weather (10.4%), suggesting drivers are more cautious in reduced visibility. Turning movements remain relatively consistent across weather conditions but show a slight increase during precipitation (20%) compared to other conditions.\nImage 2 shows distinct collision patterns for each movement type. Most notably, slowing/stopped vehicles are overwhelmingly involved in rear-end collisions (88%), highlighting the risks of sudden stops. For turning movements, straight movement angle collisions dominate (63%), followed by head-on left turn accidents (37%). Moving forward vehicles show a more balanced distribution between straight movement angle collisions (59%) and same direction rear-end crashes (41%), while parking-related incidents are predominantly classified as “other” types (88%), suggesting unique accident dynamics in parking scenarios.\n\nspeed_analysis &lt;- crash_data %&gt;%\n  mutate(\n    Hour = hour(Crash.Date.Time),\n    Time_Block = case_when(\n      Hour &gt;= 5 & Hour &lt; 10 ~ \"Morning Rush (5-9)\",\n      Hour &gt;= 10 & Hour &lt; 16 ~ \"Mid-Day (10-15)\",\n      Hour &gt;= 16 & Hour &lt; 20 ~ \"Evening Rush (16-19)\",\n      Hour &gt;= 20 | Hour &lt; 5 ~ \"Night (20-4)\"\n    ),\n    Speed_Category = case_when(\n      Speed.Limit &lt;= 25 ~ \"≤25 mph\",\n      Speed.Limit &lt;= 35 ~ \"26-35 mph\",\n      Speed.Limit &lt;= 45 ~ \"36-45 mph\",\n      TRUE ~ \"&gt;45 mph\"\n    ),\n    Speed_Category = factor(Speed_Category, \n                          levels = c(\"≤25 mph\", \"26-35 mph\", \n                                   \"36-45 mph\", \"&gt;45 mph\")),\n    Has_Injury = ACRS.Report.Type == \"Injury Crash\"\n  ) %&gt;%\n  filter(Speed.Limit &gt; 0)\n\nproportions_data &lt;- speed_analysis %&gt;%\n  group_by(Time_Block, Speed_Category, Has_Injury) %&gt;%\n  summarise(count = n(), .groups = 'drop') %&gt;%\n  group_by(Time_Block, Speed_Category) %&gt;%\n  mutate(\n    total = sum(count),\n    proportion = count/total,\n    label = sprintf(\"%.1f%%\", proportion * 100)\n  )\n\nggplot(proportions_data, \n       aes(x = Speed_Category, y = proportion, fill = Has_Injury)) +\n  facet_wrap(~Time_Block, ncol = 2) +\n  geom_col(position = \"stack\") +\n  geom_text(aes(label = label),\n            position = position_stack(vjust = 0.5),\n            size = 3.5,\n            color = \"white\") +\n  scale_fill_manual(\n    values = c(\"FALSE\" = \"#66C2A5\", \"TRUE\" = \"#FC8D62\"),\n    labels = c(\"No Injury\", \"Injury\"),\n    name = \"Crash\\nOutcome\"\n  ) +\n  scale_y_continuous(labels = scales::percent) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(color = \"gray30\", size = 11),\n    strip.text = element_text(face = \"bold\", size = 11),\n    axis.title = element_text(face = \"bold\"),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank(),\n    legend.position = \"right\",\n    legend.box.background = element_rect(color = \"gray80\", fill = \"white\")\n  ) +\n  labs(\n    title = \"Injury Rates by Speed Limit and Time of Day\",\n    subtitle = \"Showing the proportion of injury crashes across different conditions\",\n    x = \"Speed Limit Category\",\n    y = \"Proportion of Crashes\"\n  )\n\nspeed_summary &lt;- speed_analysis %&gt;%\n  group_by(Speed_Category) %&gt;%\n  summarise(\n    Total_Category_Crashes = n(),\n    Avg_Injury_Rate = mean(Has_Injury)\n  ) %&gt;%\n  arrange(desc(Total_Category_Crashes))\n\nggplot(speed_summary,\n       aes(x = Speed_Category, y = Avg_Injury_Rate)) +\n  geom_col(aes(fill = Avg_Injury_Rate), width = 0.7) +\n  geom_text(aes(label = sprintf(\"n=%d\", Total_Category_Crashes)),\n            vjust = -0.5,\n            size = 3.5) +\n  geom_text(aes(label = sprintf(\"%.1f%%\", Avg_Injury_Rate * 100)),\n            vjust = 1.5,\n            color = \"white\",\n            size = 3.5,\n            fontface = \"bold\") +\n  scale_fill_viridis(option = \"plasma\", guide = \"none\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    axis.title = element_text(face = \"bold\"),\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  ) +\n  labs(\n    title = \"Overall Injury Rates by Speed Limit\",\n    subtitle = \"Numbers above bars show total crash count\",\n    x = \"Speed Limit Category\",\n    y = \"Average Injury Rate\"\n  )\n\nprint(\"Summary Statistics by Speed Category:\")\nprint(speed_summary)\n\nImage 1 shows a somewhat complex relationship between injury rates, speed limits, and time of day. During evening rush hour (16-19), roads with 36-45 mph limits show the highest injury rate at 54.7%, while mid-day (10-15) demonstrates lower injury rates across all speed categories, with ≤25 mph zones having the lowest at 17.9%. Interestingly, high-speed zones (&gt;45 mph) show varying injury patterns, from 0% injuries during evening rush to 44.4% during night hours (20-4), suggesting that timing significantly influences crash severity in these zones.\nImage 2 provides a broader perspective on overall injury rates by speed limit, revealing that the 36-45 mph zones have both the highest injury rate (45.3%) and a substantial number of crashes (n=192). Despite expectations, the highest speed category (&gt;45 mph) shows a relatively low injury rate of 21.6%, though this may be influenced by its smaller sample size (n=51). The 26-35 mph zones represent the highest crash frequency (n=460) with a moderate injury rate of 37.2%, while the lowest speed zones maintain the lowest injury rate at 20.2% across a significant number of crashes (n=262).\n\ncollision_analysis &lt;- crash_data %&gt;%\n  filter(\n    Traffic.Control != \"UNKNOWN\",\n    Traffic.Control != \"N/A\",\n    Weather != \"UNKNOWN\",\n    Collision.Type != \"UNKNOWN\",\n    Collision.Type != \"OTHER\"\n  ) %&gt;%\n  mutate(\n    Control_Type = case_when(\n      str_detect(Traffic.Control, \"SIGNAL\") ~ \"Traffic Signal\",\n      str_detect(Traffic.Control, \"STOP\") ~ \"Stop Sign\",\n      str_detect(Traffic.Control, \"YIELD\") ~ \"Yield Sign\",\n      Traffic.Control == \"NO CONTROLS\" ~ \"No Controls\",\n      TRUE ~ \"Other Controls\"\n    ),\n    Weather_Condition = case_when(\n      Weather == \"CLEAR\" ~ \"Clear\",\n      Weather == \"CLOUDY\" ~ \"Cloudy\",\n      Weather %in% c(\"RAIN\", \"SNOW\", \"SLEET\") ~ \"Precipitation\",\n      TRUE ~ \"Other\"\n    ),\n    Collision_Category = case_when(\n      str_detect(Collision.Type, \"REAR-END\") ~ \"Rear End\",\n      str_detect(Collision.Type, \"ANGLE\") ~ \"Angle\",\n      str_detect(Collision.Type, \"SIDESWIPE\") ~ \"Sideswipe\",\n      str_detect(Collision.Type, \"HEAD ON\") ~ \"Head On\",\n      TRUE ~ \"Other\"\n    )\n  )\n\ncontrol_summary &lt;- collision_analysis %&gt;%\n  group_by(Control_Type, Weather_Condition) %&gt;%\n  summarise(\n    Total_Crashes = n(),\n    Injury_Rate = mean(ACRS.Report.Type == \"Injury Crash\"),\n    .groups = 'drop'\n  ) %&gt;%\n  group_by(Weather_Condition) %&gt;%\n  mutate(\n    Proportion = Total_Crashes / sum(Total_Crashes)\n  )\n\nplot1 &lt;- ggplot(control_summary, \n       aes(x = Weather_Condition, y = Proportion, fill = Control_Type)) +\n  geom_col(position = position_dodge(width = 0.9), width = 0.8) +\n  geom_text(aes(label = sprintf(\"%.1f%%\", Proportion * 100)),\n            position = position_dodge(width = 0.9),\n            vjust = -0.5,\n            size = 3.5,\n            fontface = \"bold\") +\n  geom_text(aes(label = sprintf(\"IR: %.1f%%\", Injury_Rate * 100)),\n            position = position_dodge(width = 0.9),\n            vjust = 1.5,\n            size = 3,\n            color = \"darkgray\") +\n  scale_fill_manual(values = c(\n    \"No Controls\" = \"#2E86C1\",\n    \"Other Controls\" = \"#F1C40F\",\n    \"Stop Sign\" = \"#E67E22\",\n    \"Traffic Signal\" = \"#27AE60\",\n    \"Yield Sign\" = \"#8E44AD\"\n  )) +\n  scale_y_continuous(\n    labels = scales::percent,\n    limits = c(0, 0.8),\n    breaks = seq(0, 0.8, 0.1)\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(color = \"gray30\", size = 12),\n    axis.title = element_text(face = \"bold\", size = 12),\n    axis.text = element_text(size = 10),\n    legend.title = element_text(face = \"bold\", size = 12),\n    legend.text = element_text(size = 10),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    legend.position = \"right\",\n    legend.box.background = element_rect(color = \"gray80\", fill = \"white\", linewidth = 0.5),\n    plot.margin = margin(t = 20, r = 20, b = 20, l = 20)\n  ) +\n  labs(\n    title = \"Traffic Control Effectiveness by Weather Condition\",\n    subtitle = \"Showing crash distribution (%) and injury rates (IR)\",\n    x = \"Weather Condition\",\n    y = \"Percentage of Crashes\",\n    fill = \"Traffic Control\\nType\"\n  )\n\ncollision_type_summary &lt;- collision_analysis %&gt;%\n  group_by(Control_Type, Collision_Category) %&gt;%\n  summarise(\n    Count = n(),\n    Injury_Count = sum(ACRS.Report.Type == \"Injury Crash\"),\n    Injury_Rate = Injury_Count / Count,\n    .groups = 'drop'\n  ) %&gt;%\n  group_by(Control_Type) %&gt;%\n  mutate(Proportion = Count / sum(Count))\n\nplot2 &lt;- ggplot(collision_type_summary, \n       aes(x = Control_Type, y = Count, fill = Collision_Category)) +\n  geom_col(position = position_dodge(width = 0.8), width = 0.7) +\n  geom_text(aes(label = sprintf(\"%.1f%%\", Injury_Rate * 100)),\n            position = position_dodge(width = 0.8),\n            vjust = -0.5,\n            size = 3.5,\n            fontface = \"bold\") +\n  scale_fill_manual(values = c(\n    \"Angle\" = \"#3498DB\",\n    \"Head On\" = \"#E74C3C\",\n    \"Rear End\" = \"#2ECC71\",\n    \"Sideswipe\" = \"#F1C40F\",\n    \"Other\" = \"#95A5A6\"\n  )) +\n  scale_y_continuous(\n    breaks = seq(0, 200, 25),\n    expand = expansion(mult = c(0, 0.15))\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(color = \"gray30\", size = 12),\n    axis.title = element_text(face = \"bold\", size = 12),\n    axis.text = element_text(size = 10),\n    axis.text.x = element_text(angle = 45, hjust = 1, face = \"bold\"),\n    legend.title = element_text(face = \"bold\", size = 12),\n    legend.text = element_text(size = 10),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    legend.position = \"right\",\n    legend.box.background = element_rect(color = \"gray80\", fill = \"white\", linewidth = 0.5),\n    plot.margin = margin(t = 20, r = 20, b = 40, l = 20)\n  ) +\n  labs(\n    title = \"Collision Types and Injury Rates by Traffic Control\",\n    subtitle = \"Numbers above bars show injury rates (%)\",\n    x = \"Traffic Control Type\",\n    y = \"Number of Crashes\",\n    fill = \"Type of\\nCollision\"\n  )\n\nprint(\"Summary by Traffic Control Type:\")\nprint(control_summary %&gt;% \n      group_by(Control_Type) %&gt;%\n      summarise(\n        Total_Crashes = sum(Total_Crashes),\n        Avg_Injury_Rate = mean(Injury_Rate),\n        .groups = 'drop'\n      ) %&gt;%\n      arrange(desc(Total_Crashes)))\n\nprint(plot1)\nprint(plot2)\n\nImage 1 reveals that traffic signals are the predominant control type across all weather conditions, but their effectiveness varies significantly. During precipitation, traffic signals account for 66.7% of crashes with a 50% injury rate, while no-control areas show a concerning 100% injury rate despite fewer crashes (33.3%). In clear conditions, traffic signals handle 48.3% of crashes with a 35.8% injury rate, while no-control areas see 38.7% of crashes with a 32% injury rate, suggesting better performance in good weather.\nImage 2 demonstrates that traffic signals experience the highest volume of crashes but show varying injury rates by collision type - angle collisions have a 48.9% injury rate while head-on collisions show 46.8%. Stop signs, while having fewer crashes, show a high injury rate for angle collisions (53.4%). Areas with no controls show relatively balanced injury rates across collision types (35-39%), except for sideswipes at 21.7%. Notably, yield signs have limited data but show a 50% injury rate, suggesting they may warrant closer monitoring for safety effectiveness.\n\nroad_analysis &lt;- crash_data %&gt;%\n  filter(\n    Route.Type != \"\",\n    !is.na(Speed.Limit),\n    Speed.Limit &gt; 0,\n    ACRS.Report.Type != \"UNKNOWN\"\n  ) %&gt;%\n  mutate(\n    Road_Category = case_when(\n      str_detect(Route.Type, \"Interstate\") ~ \"Interstate\",\n      str_detect(Route.Type, \"State|Maryland\") ~ \"State Road\",\n      str_detect(Route.Type, \"County\") ~ \"County Road\",\n      str_detect(Route.Type, \"Municipal\") ~ \"Municipal Road\",\n      TRUE ~ \"Other\"\n    ),\n    Speed_Category = case_when(\n      Speed.Limit &lt;= 25 ~ \"Low (≤25)\",\n      Speed.Limit &lt;= 35 ~ \"Medium (26-35)\",\n      Speed.Limit &lt;= 45 ~ \"High (36-45)\",\n      TRUE ~ \"Very High (&gt;45)\"\n    ),\n    Speed_Category = factor(Speed_Category, \n                          levels = c(\"Low (≤25)\", \"Medium (26-35)\", \n                                   \"High (36-45)\", \"Very High (&gt;45)\")),\n    Severity = case_when(\n      ACRS.Report.Type == \"Property Damage Crash\" ~ \"Property Damage\",\n      ACRS.Report.Type == \"Injury Crash\" ~ \"Injury\",\n      TRUE ~ \"Other\"\n    )\n  )\n\nroad_summary &lt;- road_analysis %&gt;%\n  group_by(Road_Category, Speed_Category) %&gt;%\n  summarise(\n    Total_Crashes = n(),\n    Injury_Count = sum(Severity == \"Injury\"),\n    Injury_Rate = Injury_Count / Total_Crashes,\n    Avg_Speed = mean(Speed.Limit),\n    .groups = 'drop'\n  )\n\nplot1 &lt;- ggplot(road_summary, \n       aes(x = Speed_Category, y = Total_Crashes, fill = Road_Category)) +\n  geom_col(position = \"dodge\", width = 0.8) +\n  geom_text(aes(label = Total_Crashes),\n            position = position_dodge(width = 0.8),\n            vjust = -0.5,\n            size = 3.5,\n            fontface = \"bold\") +\n  geom_text(aes(label = sprintf(\"IR: %.1f%%\", Injury_Rate * 100)),\n            position = position_dodge(width = 0.8),\n            vjust = 1.5,\n            size = 3,\n            color = \"darkgray\") +\n  scale_fill_manual(values = c(\n    \"Interstate\" = \"#2980B9\",\n    \"State Road\" = \"#27AE60\",\n    \"County Road\" = \"#F1C40F\",\n    \"Municipal Road\" = \"#E67E22\",\n    \"Other\" = \"#95A5A6\"\n  )) +\n  scale_y_continuous(\n    expand = expansion(mult = c(0, 0.2)),\n    breaks = seq(0, max(road_summary$Total_Crashes), by = 25)\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(color = \"gray30\", size = 12),\n    axis.title = element_text(face = \"bold\", size = 12),\n    axis.text = element_text(size = 10),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.title = element_text(face = \"bold\", size = 12),\n    legend.text = element_text(size = 10),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    legend.position = \"right\",\n    legend.box.background = element_rect(color = \"gray80\", fill = \"white\", linewidth = 0.5),\n    plot.margin = margin(t = 20, r = 20, b = 40, l = 20)\n  ) +\n  labs(\n    title = \"Crash Distribution by Road Type and Speed Limit\",\n    subtitle = \"Showing crash counts and injury rates (IR)\",\n    x = \"Speed Limit Category\",\n    y = \"Number of Crashes\",\n    fill = \"Road Type\"\n  )\n\ntime_road_analysis &lt;- road_analysis %&gt;%\n  mutate(\n    Hour = hour(Crash.Date.Time),\n    Time_Category = case_when(\n      Hour &gt;= 5 & Hour &lt; 10 ~ \"Morning Rush (5-9)\",\n      Hour &gt;= 10 & Hour &lt; 16 ~ \"Mid-Day (10-15)\",\n      Hour &gt;= 16 & Hour &lt; 20 ~ \"Evening Rush (16-19)\",\n      TRUE ~ \"Night (20-4)\"\n    )\n  ) %&gt;%\n  group_by(Road_Category, Time_Category) %&gt;%\n  summarise(\n    Crashes = n(),\n    Injury_Rate = mean(Severity == \"Injury\"),\n    .groups = 'drop'\n  )\n\nplot2 &lt;- ggplot(time_road_analysis, \n       aes(x = Road_Category, y = Crashes, fill = Time_Category)) +\n  geom_col(position = \"dodge\", width = 0.8) +\n  geom_text(aes(label = Crashes),\n            position = position_dodge(width = 0.8),\n            vjust = -0.5,\n            size = 3.5,\n            fontface = \"bold\") +\n  geom_text(aes(label = sprintf(\"%.1f%%\", Injury_Rate * 100)),\n            position = position_dodge(width = 0.8),\n            vjust = 1.5,\n            size = 3,\n            color = \"darkgray\") +\n  scale_fill_manual(values = c(\n    \"Morning Rush (5-9)\" = \"#3498DB\",\n    \"Mid-Day (10-15)\" = \"#2ECC71\",\n    \"Evening Rush (16-19)\" = \"#F1C40F\",\n    \"Night (20-4)\" = \"#95A5A6\"\n  )) +\n  scale_y_continuous(\n    expand = expansion(mult = c(0, 0.2)),\n    breaks = seq(0, max(time_road_analysis$Crashes), by = 25)\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(color = \"gray30\", size = 12),\n    axis.title = element_text(face = \"bold\", size = 12),\n    axis.text = element_text(size = 10),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.title = element_text(face = \"bold\", size = 12),\n    legend.text = element_text(size = 10),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    legend.position = \"right\",\n    legend.box.background = element_rect(color = \"gray80\", fill = \"white\", linewidth = 0.5),\n    plot.margin = margin(t = 20, r = 20, b = 40, l = 20)\n  ) +\n  labs(\n    title = \"Crash Distribution by Road Type and Time of Day\",\n    subtitle = \"Showing crash counts and injury rates (%)\",\n    x = \"Road Type\",\n    y = \"Number of Crashes\",\n    fill = \"Time Period\"\n  )\n\nprint(\"Summary by Road Type:\")\nprint(road_analysis %&gt;% \n      group_by(Road_Category) %&gt;%\n      summarise(\n        Total_Crashes = n(),\n        Avg_Injury_Rate = mean(Severity == \"Injury\"),\n        Avg_Speed_Limit = mean(Speed.Limit),\n        .groups = 'drop'\n      ) %&gt;%\n      arrange(desc(Total_Crashes)))\n\nprint(plot1)\nprint(plot2)\n\nImage 1 shows that medium-speed roads (26-35 mph) experience the highest number of crashes, with State Roads leading at 241 crashes and a 36.1% injury rate, followed by County Roads with 179 crashes and a 39.1% injury rate. High-speed roads (36-45 mph) show elevated injury rates, particularly on State Roads (43.5%) and County Roads (49.1%), despite lower crash volumes. Interestingly, very high-speed roads (&gt;45 mph) show relatively few crashes but maintain significant injury rates, with State Roads showing a 30.8% injury rate across 26 incidents.\nImage 2 reveals distinct temporal patterns across road types, with State Roads experiencing the highest crash volumes during mid-day (153 crashes, 35.3% injury rate) and evening rush (113 crashes, 38.9% injury rate). County Roads show a similar pattern but with lower volumes, peaking during mid-day (124 crashes, 29% injury rate). Municipal Roads and Interstates have considerably fewer crashes but show varying injury rates across time periods, with Municipal Roads experiencing higher injury rates during morning rush hours (21.4%). Night-time crashes show generally lower volumes across all road types but maintain concerning injury rates, particularly on State Roads (29.7% across 74 crashes)."
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html",
    "href": "posts/2023-12-20-examples/examples.html",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "href": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "dataset/Cleaning_Xiang.html",
    "href": "dataset/Cleaning_Xiang.html",
    "title": "crash car data clean",
    "section": "",
    "text": "options(repos = c(CRAN = \"https://cran.r-project.org\"))\ninstall.packages(\"skimr\")\n\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(lubridate)\nlibrary(skimr)  # for quick summary statistics\n\n\ninstall.packages(\"RCurl\")\nlibrary(RCurl)\nx &lt;- getURL(\"https://raw.githubusercontent.com/sussmanbu/ma-4615-fa24-final-project-group-1/main/Crash_Reporting_Drivers_Data.csv\")\ndrivers_data &lt;- read.csv(text = x)\n\n\nglimpse(drivers_data)\n\nskim(drivers_data)\n\nClean Column Names\n\ndrivers_data_clean &lt;- drivers_data %&gt;%\n  clean_names()\n\ncolnames(drivers_data_clean)\n\nHandle Date/Time Format\n\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    crash_date/time &lt;- mdy_hms(crash_date/time),\n    # create separate date and time columns if needed\n    crash_date = date(crash_date/time),\n    crash_time = format(crash_date/time, \"%H:%M:%S\")\n  )\n\nStandardize Categorical Variables\n\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # standardize agency names\n    agency_name = str_trim(agency_name),\n    \n    # clean and standardize route type\n    route_type = case_when(\n      is.na(route_type) ~ \"Unknown\",\n      TRUE ~ route_type\n    ),\n    \n    # standardize weather conditions\n    weather = str_to_title(weather),\n    \n    # clean surface condition\n    surface_condition = case_when(\n      is.na(surface_condition) ~ \"Unknown\",\n      TRUE ~ surface_condition\n    ),\n    \n    # standardize driver at fault\n    driver_at_fault = case_when(\n      str_to_lower(driver_at_fault) == \"yes\" ~ \"Yes\",\n      str_to_lower(driver_at_fault) == \"no\" ~ \"No\",\n      TRUE ~ \"Unknown\"\n    )\n  )\n\nHandle Missing Values\n\n# calculate missing values percentage\nmissing_values &lt;- drivers_data_clean %&gt;%\n  summarise(across(everything(), ~sum(is.na(.))/n()*100)) %&gt;%\n  pivot_longer(everything(), \n              names_to = \"column\", \n              values_to = \"missing_percentage\") %&gt;%\n  arrange(desc(missing_percentage))\n\n# display columns with missing values\nprint(missing_values %&gt;% filter(missing_percentage &gt; 0))\n\n# handle missing values based on column type\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # for route_type, road_name, and cross_street_name\n    # keep NA as is since they might be meaningful (e.g., off-road incidents)\n    \n    # for numeric columns, consider if 0 or NA is more appropriate\n    speed_limit = if_else(is.na(speed_limit), 0, speed_limit),\n    \n    # for categorical columns, mark unknown\n    surface_condition = if_else(is.na(surface_condition), \"Unknown\", surface_condition),\n    traffic_control = if_else(is.na(traffic_control), \"Unknown\", traffic_control)\n  )\n\nData Validation and Consistency Checks\n\n# check for logical consistencies\nvalidation_results &lt;- drivers_data_clean %&gt;%\n  summarise(\n    # check for future dates\n    future_dates = sum(crash_date &gt; Sys.Date()),\n    \n    # check for valid speed limits\n    invalid_speed = sum(speed_limit &gt; 70 | speed_limit &lt; 0, na.rm = TRUE),\n    \n    # check for valid vehicle years\n    invalid_vehicle_year = sum(vehicle_year &lt; 1900 | vehicle_year &gt; year(Sys.Date()) + 1, na.rm = TRUE),\n    \n    # check for valid coordinates\n    invalid_lat = sum(latitude &lt; 38 | latitude &gt; 40, na.rm = TRUE),\n    invalid_lon = sum(longitude &gt; -76 | longitude &lt; -78, na.rm = TRUE)\n  )\n\nprint(validation_results)\n\n# create flags for potential data quality issues\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    data_quality_flag = case_when(\n      crash_date &gt; Sys.Date() ~ \"Future date\",\n      speed_limit &gt; 70 | speed_limit &lt; 0 ~ \"Invalid speed limit\",\n      vehicle_year &lt; 1900 | vehicle_year &gt; year(Sys.Date()) + 1 ~ \"Invalid vehicle year\",\n      latitude &lt; 38 | latitude &gt; 40 | longitude &gt; -76 | longitude &lt; -78 ~ \"Invalid coordinates\",\n      TRUE ~ \"Valid\"\n    )\n  )\n\nCheck Value Distributions\n\n# check common categories\ncategory_summaries &lt;- list(\n  collision_types = table(drivers_data_clean$collision_type),\n  weather_conditions = table(drivers_data_clean$weather),\n  vehicle_types = table(drivers_data_clean$vehicle_body_type),\n  injury_severity = table(drivers_data_clean$injury_severity)\n)\n\nprint(category_summaries)\n\nCollision Types Need Standardization:\n\n# standardize collision types\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    collision_type = case_when(\n      collision_type %in% c(\"Angle\", \"ANGLE MEETS LEFT HEAD ON\", \"ANGLE MEETS LEFT TURN\",\n                           \"ANGLE MEETS RIGHT TURN\", \"STRAIGHT MOVEMENT ANGLE\") ~ \"ANGLE\",\n      collision_type %in% c(\"Front to Front\", \"HEAD ON\", \"HEAD ON LEFT TURN\") ~ \"HEAD_ON\",\n      collision_type %in% c(\"Front to Rear\", \"SAME DIR REAR END\", \n                           \"SAME DIR REND LEFT TURN\", \"SAME DIR REND RIGHT TURN\") ~ \"REAR_END\",\n      collision_type %in% c(\"SAME DIRECTION SIDESWIPE\", \"Sideswipe, Same Direction\") ~ \"SIDESWIPE_SAME_DIR\",\n      collision_type %in% c(\"OPPOSITE DIRECTION SIDESWIPE\", \"Sideswipe, Opposite Direction\") ~ \"SIDESWIPE_OPPOSITE_DIR\",\n      collision_type %in% c(\"SINGLE VEHICLE\", \"Single Vehicle\") ~ \"SINGLE_VEHICLE\",\n      collision_type %in% c(\"Other\", \"OTHER\") ~ \"OTHER\",\n      collision_type %in% c(\"Unknown\", \"UNKNOWN\", \"N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\nWeather Conditions Need Standardization:\n\n# standardize weather conditions\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    weather = case_when(\n      str_detect(str_to_upper(weather), \"CLEAR\") ~ \"CLEAR\",\n      str_detect(str_to_upper(weather), \"CLOUD\") ~ \"CLOUDY\",\n      str_detect(str_to_upper(weather), \"RAIN|RAINING\") ~ \"RAIN\",\n      str_detect(str_to_upper(weather), \"SNOW|BLOWING SNOW\") ~ \"SNOW\",\n      str_detect(str_to_upper(weather), \"FOG|SMOG|SMOKE\") ~ \"FOG\",\n      weather %in% c(\"N/A\", \"Unknown\", \"UNKNOWN\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\nVehicle Types Need Major Cleanup:\n\n# standardize vehicle types\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    vehicle_body_type = case_when(\n      str_detect(str_to_upper(vehicle_body_type), \"PASSENGER CAR|PASSENGER|CAR\") ~ \"PASSENGER_CAR\",\n      str_detect(str_to_upper(vehicle_body_type), \"SUV|SPORT UTILITY|UTILITY VEHICLE\") ~ \"SUV\",\n      str_detect(str_to_upper(vehicle_body_type), \"PICKUP|LIGHT TRUCK\") ~ \"PICKUP_TRUCK\",\n      str_detect(str_to_upper(vehicle_body_type), \"VAN|CARGO\") ~ \"VAN\",\n      str_detect(str_to_upper(vehicle_body_type), \"BUS\") ~ \"BUS\",\n      str_detect(str_to_upper(vehicle_body_type), \"MOTORCYCLE|MOPED\") ~ \"MOTORCYCLE\",\n      str_detect(str_to_upper(vehicle_body_type), \"EMERGENCY|POLICE|FIRE|AMBULANCE\") ~ \"EMERGENCY_VEHICLE\",\n      vehicle_body_type %in% c(\"N/A\", \"Unknown\", \"UNKNOWN\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\nInjury Severity Standardization:\n\n# standardize injury severity\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    injury_severity = case_when(\n      str_detect(str_to_upper(injury_severity), \"FATAL\") ~ \"FATAL\",\n      str_detect(str_to_upper(injury_severity), \"NO APPARENT|NO INJURY\") ~ \"NO_INJURY\",\n      str_detect(str_to_upper(injury_severity), \"POSSIBLE\") ~ \"POSSIBLE_INJURY\",\n      str_detect(str_to_upper(injury_severity), \"MINOR\") ~ \"MINOR_INJURY\",\n      str_detect(str_to_upper(injury_severity), \"SERIOUS\") ~ \"SERIOUS_INJURY\",\n      TRUE ~ \"UNKNOWN\"\n    )\n  )\n\nHandle Missing Values Strategy (based on the missing percentage analysis):\n\n# handle missing values based on context\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # high missing percentage (&gt;90%) - keep as NA but add flag\n    has_non_motorist = !is.na(related_non_motorist),\n    \n    # medium missing percentage (10-90%) - add meaningful categories\n    municipality = if_else(is.na(municipality), \"UNINCORPORATED\", municipality),\n    road_name = if_else(is.na(road_name), \"OFF_ROAD\", road_name),\n    cross_street_name = if_else(is.na(cross_street_name), \"NOT_APPLICABLE\", cross_street_name),\n    \n    # low missing percentage (&lt;10%) - impute with \"UNKNOWN\"\n    drivers_license_state = if_else(is.na(drivers_license_state), \"UNKNOWN\", drivers_license_state),\n    circumstance = if_else(is.na(circumstance), \"UNKNOWN\", circumstance),\n    vehicle_going_dir = if_else(is.na(vehicle_going_dir), \"UNKNOWN\", vehicle_going_dir)\n  )\n\nHandle Data Quality Issues (based on validation results):\n\n# add data quality flags\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    data_quality_flags = case_when(\n      speed_limit &gt; 70 | speed_limit &lt; 0 ~ \"INVALID_SPEED_LIMIT\",\n      vehicle_year &lt; 1900 | vehicle_year &gt; year(Sys.Date()) + 1 ~ \"INVALID_VEHICLE_YEAR\",\n      latitude &lt; 38 | latitude &gt; 40 | longitude &gt; -76 | longitude &lt; -78 ~ \"INVALID_COORDINATES\",\n      TRUE ~ \"VALID\"\n    ),\n    \n    # clean speed limits\n    speed_limit = case_when(\n      speed_limit &gt; 70 ~ NA_real_,\n      speed_limit &lt; 0 ~ NA_real_,\n      TRUE ~ speed_limit\n    ),\n    \n    # clean vehicle years\n    vehicle_year = if_else(\n      vehicle_year &lt; 1900 | vehicle_year &gt; year(Sys.Date()) + 1,\n      NA_real_,\n      vehicle_year\n    )\n  )\n\nAdditional cleaning for specific columns:\n\n# additional cleaning steps for specific columns and vehicle data\n\n# standardize vehicle makes (common misspellings and abbreviations)\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    vehicle_make = case_when(\n      # honda variations\n      str_detect(str_to_upper(vehicle_make), \"HOND|HDA\") ~ \"HONDA\",\n      \n      # toyota variations\n      str_detect(str_to_upper(vehicle_make), \"TOY|TOYT\") ~ \"TOYOTA\",\n      \n      # ford\n      str_detect(str_to_upper(vehicle_make), \"^FRD|FORD\") ~ \"FORD\",\n      \n      # chevrolet variations\n      str_detect(str_to_upper(vehicle_make), \"CHEV|CHEVY|CHV\") ~ \"CHEVROLET\",\n      \n      # nissan variations\n      str_detect(str_to_upper(vehicle_make), \"NISS|NISN\") ~ \"NISSAN\",\n      \n      # hyundai variations\n      str_detect(str_to_upper(vehicle_make), \"HYUN|HYU\") ~ \"HYUNDAI\",\n      \n      # volkswagen variations\n      str_detect(str_to_upper(vehicle_make), \"VW|VOLK|VOLKS\") ~ \"VOLKSWAGEN\",\n      \n      # BMW\n      str_detect(str_to_upper(vehicle_make), \"BMW|BMV\") ~ \"BMW\",\n      \n      # Mercedes-Benz variations\n      str_detect(str_to_upper(vehicle_make), \"MERZ|MENZ|MERCEDES|BENZ\") ~ \"MERCEDES-BENZ\",\n      \n      # lexus\n      str_detect(str_to_upper(vehicle_make), \"LEX|LEXS\") ~ \"LEXUS\",\n      \n      # mazda variations\n      str_detect(str_to_upper(vehicle_make), \"MAZ|MAZD\") ~ \"MAZDA\",\n      \n      # subaru variations\n      str_detect(str_to_upper(vehicle_make), \"SUB|SUBR\") ~ \"SUBARU\",\n      \n      # kia\n      str_detect(str_to_upper(vehicle_make), \"^KIA\") ~ \"KIA\",\n      \n      # audi\n      str_detect(str_to_upper(vehicle_make), \"^AUD\") ~ \"AUDI\",\n      \n      # acura\n      str_detect(str_to_upper(vehicle_make), \"ACUR|ACU\") ~ \"ACURA\",\n      \n      # infinity\n      str_detect(str_to_upper(vehicle_make), \"INF|INFIN\") ~ \"INFINITI\",\n      \n      TRUE ~ str_to_upper(vehicle_make)\n    )\n  )\n\n# clean vehicle models\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    vehicle_model = case_when(\n      # clean common abbreviations\n      str_detect(str_to_upper(vehicle_model), \"^CRV|CR-V\") ~ \"CR-V\",\n      str_detect(str_to_upper(vehicle_model), \"^RAV|RAV4\") ~ \"RAV4\",\n      str_detect(str_to_upper(vehicle_model), \"ACCORD|ACRD\") ~ \"ACCORD\",\n      str_detect(str_to_upper(vehicle_model), \"CAMRY|CAM\") ~ \"CAMRY\",\n      str_detect(str_to_upper(vehicle_model), \"CIVIC|CVC\") ~ \"CIVIC\",\n      str_detect(str_to_upper(vehicle_model), \"ALTIMA|ALT\") ~ \"ALTIMA\",\n      str_detect(str_to_upper(vehicle_model), \"COROLLA|COR\") ~ \"COROLLA\",\n      str_detect(str_to_upper(vehicle_model), \"EXPLORER|EXPLR\") ~ \"EXPLORER\",\n      str_detect(str_to_upper(vehicle_model), \"F-150|F150\") ~ \"F-150\",\n      str_detect(str_to_upper(vehicle_model), \"HIGHLANDER|HGLDR\") ~ \"HIGHLANDER\",\n      TRUE ~ str_to_upper(vehicle_model)\n    )\n  )\n\n# additional cleaning for other specific columns\n\n# clean light conditions\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    light = case_when(\n      str_detect(str_to_upper(light), \"DAYLIGHT\") ~ \"DAYLIGHT\",\n      str_detect(str_to_upper(light), \"DARK.*LIGHT.*ON|LIGHTED\") ~ \"DARK_WITH_LIGHTING\",\n      str_detect(str_to_upper(light), \"DARK.*NO.*LIGHT|UNLIGHTED\") ~ \"DARK_NO_LIGHTING\",\n      str_detect(str_to_upper(light), \"DAWN\") ~ \"DAWN\",\n      str_detect(str_to_upper(light), \"DUSK\") ~ \"DUSK\",\n      str_detect(str_to_upper(light), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# clean surface conditions\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    surface_condition = case_when(\n      str_detect(str_to_upper(surface_condition), \"DRY\") ~ \"DRY\",\n      str_detect(str_to_upper(surface_condition), \"WET\") ~ \"WET\",\n      str_detect(str_to_upper(surface_condition), \"ICE|ICY\") ~ \"ICE\",\n      str_detect(str_to_upper(surface_condition), \"SNOW|SLUSH\") ~ \"SNOW\",\n      str_detect(str_to_upper(surface_condition), \"SAND|DIRT|MUD\") ~ \"SAND_DIRT_MUD\",\n      str_detect(str_to_upper(surface_condition), \"OIL|GREASE\") ~ \"OIL_GREASE\",\n      is.na(surface_condition) | str_detect(str_to_upper(surface_condition), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# clean traffic control\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    traffic_control = case_when(\n      str_detect(str_to_upper(traffic_control), \"SIGNAL|TRAFFIC LIGHT\") ~ \"TRAFFIC_SIGNAL\",\n      str_detect(str_to_upper(traffic_control), \"STOP SIGN\") ~ \"STOP_SIGN\",\n      str_detect(str_to_upper(traffic_control), \"YIELD\") ~ \"YIELD_SIGN\",\n      str_detect(str_to_upper(traffic_control), \"FLASHING\") ~ \"FLASHING_SIGNAL\",\n      str_detect(str_to_upper(traffic_control), \"NO CONTROL|NONE\") ~ \"NO_CONTROL\",\n      str_detect(str_to_upper(traffic_control), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# clean driver substance abuse\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    driver_substance_abuse = case_when(\n      str_detect(str_to_upper(driver_substance_abuse), \"NONE|NO ABUSE\") ~ \"NONE\",\n      str_detect(str_to_upper(driver_substance_abuse), \"ALCOHOL\") ~ \"ALCOHOL\",\n      str_detect(str_to_upper(driver_substance_abuse), \"DRUG\") ~ \"DRUGS\",\n      str_detect(str_to_upper(driver_substance_abuse), \"BOTH|ALCOHOL.*DRUG|DRUG.*ALCOHOL\") ~ \"ALCOHOL_AND_DRUGS\",\n      str_detect(str_to_upper(driver_substance_abuse), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# clean driver distracted by\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    driver_distracted_by = case_when(\n      str_detect(str_to_upper(driver_distracted_by), \"NOT DISTRACTED\") ~ \"NOT_DISTRACTED\",\n      str_detect(str_to_upper(driver_distracted_by), \"CELL|PHONE|MOBILE\") ~ \"CELL_PHONE\",\n      str_detect(str_to_upper(driver_distracted_by), \"PASSENGER\") ~ \"PASSENGER\",\n      str_detect(str_to_upper(driver_distracted_by), \"RADIO|AUDIO\") ~ \"AUDIO_EQUIPMENT\",\n      str_detect(str_to_upper(driver_distracted_by), \"EAT|DRINK\") ~ \"EATING_DRINKING\",\n      str_detect(str_to_upper(driver_distracted_by), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# verify our cleaning by checking the unique values in each cleaned column\nverification_check &lt;- function(data, columns) {\n  map(columns, ~{\n    unique_vals &lt;- data %&gt;% \n      pull(.) %&gt;% \n      unique() %&gt;% \n      sort()\n    \n    cat(\"\\nUnique values in\", ., \":\\n\")\n    print(unique_vals)\n    cat(\"\\n\")\n  })\n}\n\n# verify the cleaning results for key columns\ncolumns_to_verify &lt;- c(\n  \"vehicle_make\", \n  \"vehicle_model\",\n  \"light\",\n  \"surface_condition\",\n  \"traffic_control\",\n  \"driver_substance_abuse\",\n  \"driver_distracted_by\"\n)\n\nverification_check(drivers_data_clean, columns_to_verify)\n\nvehicle_summary &lt;- drivers_data_clean %&gt;%\n  group_by(vehicle_make, vehicle_model) %&gt;%\n  summarise(\n    count = n(),\n    .groups = 'drop'\n  ) %&gt;%\n  arrange(desc(count))\n\nprint(vehicle_summary)\n\n\n# clean and standardize agency names\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    agency_name = case_when(\n      str_detect(str_to_upper(agency_name), \"MONTGOMERY|MONT|MCP\") ~ \"MONTGOMERY COUNTY POLICE\",\n      str_detect(str_to_upper(agency_name), \"GAITHERSBURG|GAITH\") ~ \"GAITHERSBURG POLICE\",\n      str_detect(str_to_upper(agency_name), \"ROCKVILLE|ROCK\") ~ \"ROCKVILLE POLICE\",\n      str_detect(str_to_upper(agency_name), \"TAKOMA|TAK\") ~ \"TAKOMA PARK POLICE\",\n      str_detect(str_to_upper(agency_name), \"PARK|MNCPP\") ~ \"MD NATIONAL CAPITAL PARK POLICE\",\n      TRUE ~ str_to_upper(agency_name)\n    )\n  )\n\n# clean and standardize vehicle damage extent\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    vehicle_damage_extent = case_when(\n      str_detect(str_to_upper(vehicle_damage_extent), \"NONE|NO DAMAGE\") ~ \"NO_DAMAGE\",\n      str_detect(str_to_upper(vehicle_damage_extent), \"MINOR|SUPERFICIAL\") ~ \"MINOR\",\n      str_detect(str_to_upper(vehicle_damage_extent), \"FUNCTIONAL\") ~ \"FUNCTIONAL\",\n      str_detect(str_to_upper(vehicle_damage_extent), \"DISABLE|DISABLING\") ~ \"DISABLING\",\n      str_detect(str_to_upper(vehicle_damage_extent), \"DESTROYED\") ~ \"DESTROYED\",\n      str_detect(str_to_upper(vehicle_damage_extent), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# clean and standardize vehicle first impact location\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    vehicle_first_impact_location = case_when(\n      str_detect(str_to_upper(vehicle_first_impact_location), \"FRONT|TWELVE|12\") ~ \"FRONT\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"RIGHT FRONT|ONE|1\") ~ \"RIGHT_FRONT\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"RIGHT SIDE|THREE|3\") ~ \"RIGHT_SIDE\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"RIGHT REAR|FOUR|4\") ~ \"RIGHT_REAR\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"REAR|SIX|6\") ~ \"REAR\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"LEFT REAR|SEVEN|7|EIGHT|8\") ~ \"LEFT_REAR\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"LEFT SIDE|NINE|9\") ~ \"LEFT_SIDE\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"LEFT FRONT|TEN|10|ELEVEN|11\") ~ \"LEFT_FRONT\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# clean and standardize vehicle movement\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    vehicle_movement = case_when(\n      str_detect(str_to_upper(vehicle_movement), \"STRAIGHT|CONSTANT\") ~ \"STRAIGHT\",\n      str_detect(str_to_upper(vehicle_movement), \"TURN.*RIGHT\") ~ \"TURNING_RIGHT\",\n      str_detect(str_to_upper(vehicle_movement), \"TURN.*LEFT\") ~ \"TURNING_LEFT\",\n      str_detect(str_to_upper(vehicle_movement), \"STOP|SLOW\") ~ \"SLOWING_STOPPING\",\n      str_detect(str_to_upper(vehicle_movement), \"BACK|REVERSE\") ~ \"BACKING\",\n      str_detect(str_to_upper(vehicle_movement), \"PARK\") ~ \"PARKED\",\n      str_detect(str_to_upper(vehicle_movement), \"START\") ~ \"STARTING\",\n      str_detect(str_to_upper(vehicle_movement), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n\n# create a comprehensive validation function\nvalidate_crash_data &lt;- function(data) {\n  # initialize empty list for validation results\n  validation_results &lt;- list()\n  \n  # temporal validations\n  validation_results$temporal &lt;- data %&gt;%\n    summarise(\n      future_dates = sum(crash_date/time &gt; Sys.time()),\n      weekend_crashes = sum(lubridate::wday(crash_date/time, week_start = 1) %in% c(6,7)),\n      night_crashes = sum(format(crash_date/time, \"%H:%M:%S\") &gt;= \"18:00:00\" | \n                         format(crash_date/time, \"%H:%M:%S\") &lt;= \"06:00:00\"),\n      crashes_by_hour = table(format(crash_date/time, \"%H\"))\n    )\n  \n  # geographic validations\n  validation_results$geographic &lt;- data %&gt;%\n    summarise(\n      invalid_lat = sum(latitude &lt; 38 | latitude &gt; 40, na.rm = TRUE),\n      invalid_lon = sum(longitude &gt; -76 | longitude &lt; -78, na.rm = TRUE),\n      missing_coords = sum(is.na(latitude) | is.na(longitude)),\n      unique_locations = n_distinct(location, na.rm = TRUE)\n    )\n  \n  # vehicle validations\n  validation_results$vehicle &lt;- data %&gt;%\n    summarise(\n      invalid_years = sum(vehicle_year &lt; 1900 | vehicle_year &gt; year(Sys.Date()) + 1, na.rm = TRUE),\n      missing_makes = sum(is.na(vehicle_make)),\n      missing_models = sum(is.na(vehicle_model)),\n      unique_makes = n_distinct(vehicle_make, na.rm = TRUE),\n      unique_models = n_distinct(vehicle_model, na.rm = TRUE)\n    )\n  \n  # logical consistency checks\n  validation_results$logical &lt;- data %&gt;%\n    summarise(\n      # check if parked vehicles are marked as having movement\n      parked_moving = sum(parked_vehicle == \"Yes\" & vehicle_movement != \"PARKED\", na.rm = TRUE),\n      \n      # check for injuries in no-damage crashes\n      injuries_no_damage = sum(injury_severity != \"NO_INJURY\" & \n                             vehicle_damage_extent == \"NO_DAMAGE\", na.rm = TRUE),\n      \n      # check for fatal crashes without severe damage\n      fatal_minor_damage = sum(injury_severity == \"FATAL\" & \n                             vehicle_damage_extent %in% c(\"NO_DAMAGE\", \"MINOR\"), na.rm = TRUE),\n      \n      # check for driverless vehicles marked with driver characteristics\n      driverless_with_driver = sum(driverless_vehicle == \"Yes\" & \n                                  !is.na(driver_substance_abuse), na.rm = TRUE)\n    )\n  \n  # cross-reference checks\n  validation_results$cross_reference &lt;- data %&gt;%\n    summarise(\n      # light condition vs time of day consistency\n      light_time_mismatch = sum(\n        (format(crash_date/time, \"%H:%M:%S\") &gt;= \"18:00:00\" | \n         format(crash_date/time, \"%H:%M:%S\") &lt;= \"06:00:00\") & \n        light == \"DAYLIGHT\", na.rm = TRUE\n      ),\n      \n      # weather vs surface condition consistency\n      weather_surface_mismatch = sum(\n        (weather %in% c(\"RAIN\", \"SNOW\") & surface_condition == \"DRY\") |\n        (weather == \"CLEAR\" & surface_condition %in% c(\"ICE\", \"SNOW\")), \n        na.rm = TRUE\n      )\n    )\n  \n  # value distribution checks\n  validation_results$distributions &lt;- list(\n    speed_distribution = table(data$speed_limit),\n    damage_by_speed = table(data$vehicle_damage_extent, cut(data$speed_limit, \n                                                          breaks = c(0, 25, 35, 45, 55, Inf))),\n    injury_by_speed = table(data$injury_severity, cut(data$speed_limit, \n                                                    breaks = c(0, 25, 35, 45, 55, Inf)))\n  )\n  \n  return(validation_results)\n}\n\n# run validation and create summary report\nvalidation_summary &lt;- validate_crash_data(drivers_data_clean)\n\n# create a function to print validation results in a readable format\nprint_validation_summary &lt;- function(validation_results) {\n  cat(\"\\n=== VALIDATION SUMMARY ===\\n\")\n  \n  cat(\"\\nTEMPORAL VALIDATION:\")\n  cat(\"\\n- Future dates:\", validation_results$temporal$future_dates)\n  cat(\"\\n- Weekend crashes:\", validation_results$temporal$weekend_crashes)\n  cat(\"\\n- Night crashes:\", validation_results$temporal$night_crashes)\n  \n  cat(\"\\n\\nGEOGRAPHIC VALIDATION:\")\n  print(validation_results$geographic)\n  \n  cat(\"\\n\\nVEHICLE VALIDATION:\")\n  print(validation_results$vehicle)\n  \n  cat(\"\\n\\nLOGICAL CONSISTENCY CHECKS:\")\n  print(validation_results$logical)\n  \n  cat(\"\\n\\nCROSS-REFERENCE CHECKS:\")\n  print(validation_results$cross_reference)\n  \n  cat(\"\\n\\nDISTRIBUTIONS:\")\n  cat(\"\\nSpeed Limit Distribution:\\n\")\n  print(validation_results$distributions$speed_distribution)\n  \n  cat(\"\\nDamage by Speed Range:\\n\")\n  print(validation_results$distributions$damage_by_speed)\n}\n\n# run and print validation summary\nprint_validation_summary(validation_summary)\n\n# create data quality flags based on validation results\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    data_quality_flags = case_when(\n      # temporal flags\n      crash_date/time &gt; Sys.time() ~ \"FUTURE_DATE\",\n      \n      # geographic flags\n      (latitude &lt; 38 | latitude &gt; 40 | longitude &gt; -76 | longitude &lt; -78) ~ \"INVALID_COORDINATES\",\n      \n      # vehicle flags\n      (vehicle_year &lt; 1900 | vehicle_year &gt; year(Sys.Date()) + 1) ~ \"INVALID_VEHICLE_YEAR\",\n      \n      # logical consistency flags\n      (parked_vehicle == \"Yes\" & vehicle_movement != \"PARKED\") ~ \"PARKED_MOVING_MISMATCH\",\n      (injury_severity != \"NO_INJURY\" & vehicle_damage_extent == \"NO_DAMAGE\") ~ \"INJURY_NO_DAMAGE_MISMATCH\",\n      (injury_severity == \"FATAL\" & vehicle_damage_extent %in% c(\"NO_DAMAGE\", \"MINOR\")) ~ \"FATAL_MINOR_DAMAGE_MISMATCH\",\n      \n      # cross-reference flags\n      (format(crash_date/time, \"%H:%M:%S\") &gt;= \"18:00:00\" | \n       format(crash_date/time, \"%H:%M:%S\") &lt;= \"06:00:00\") & \n      light == \"DAYLIGHT\" ~ \"LIGHT_TIME_MISMATCH\",\n      \n      (weather %in% c(\"RAIN\", \"SNOW\") & surface_condition == \"DRY\") |\n      (weather == \"CLEAR\" & surface_condition %in% c(\"ICE\", \"SNOW\")) ~ \"WEATHER_SURFACE_MISMATCH\",\n      \n      TRUE ~ \"VALID\"\n    )\n  )\n\nquality_flag_summary &lt;- drivers_data_clean %&gt;%\n  group_by(data_quality_flags) %&gt;%\n  summarise(\n    count = n(),\n    percentage = n() / nrow(drivers_data_clean) * 100\n  ) %&gt;%\n  arrange(desc(count))\n\nprint(\"\\nData Quality Flag Summary:\")\nprint(quality_flag_summary)\n\nSomething still seems off with the data quality flags.\n\n# fix Light-Time Mismatches (largest issue ~5.2% of data)\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # extract hour from crash_date/time\n    crash_hour = as.numeric(format(crash_date/time, \"%H\")),\n    \n    # correct light condition based on time\n    light = case_when(\n      # dawn hours (5-7 AM)\n      crash_hour &gt;= 5 & crash_hour &lt; 7 ~ \"DAWN\",\n      \n      # daylight hours (7 AM - 6 PM)\n      crash_hour &gt;= 7 & crash_hour &lt; 18 ~ \"DAYLIGHT\",\n      \n      # dusk hours (6-8 PM)\n      crash_hour &gt;= 18 & crash_hour &lt; 20 ~ \"DUSK\",\n      \n      # night hours\n      crash_hour &gt;= 20 | crash_hour &lt; 5 ~ \"DARK_WITH_LIGHTING\",\n      \n      TRUE ~ light\n    )\n  ) %&gt;%\n  select(-crash_hour) # remove temporary column\n\n# fix Weather-Surface Condition Mismatches (~0.37% of data)\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # Adjust surface condition based on weather\n    surface_condition = case_when(\n      weather %in% c(\"RAIN\", \"RAINING\") & surface_condition == \"DRY\" ~ \"WET\",\n      weather == \"SNOW\" & surface_condition == \"DRY\" ~ \"SNOW\",\n      weather == \"CLEAR\" & surface_condition %in% c(\"ICE\", \"SNOW\") & \n        format(crash_date/time, \"%m\") %in% c(\"12\", \"01\", \"02\") ~ surface_condition, # Keep if winter months\n      weather == \"CLEAR\" & surface_condition %in% c(\"ICE\", \"SNOW\") ~ \"UNKNOWN\", # Change to unknown if non-winter\n      TRUE ~ surface_condition\n    )\n  )\n\n# fix Injury-Damage Inconsistencies\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # adjust injury severity or damage extent for logical consistency\n    injury_severity = case_when(\n      injury_severity != \"NO_INJURY\" & vehicle_damage_extent == \"NO_DAMAGE\" ~ \"NO_INJURY\",\n      injury_severity == \"FATAL\" & vehicle_damage_extent %in% c(\"NO_DAMAGE\", \"MINOR\") ~ \"SERIOUS_INJURY\",\n      TRUE ~ injury_severity\n    ),\n    \n    # ensure fatal crashes have appropriate damage extent\n    vehicle_damage_extent = case_when(\n      injury_severity == \"FATAL\" & vehicle_damage_extent %in% c(\"NO_DAMAGE\", \"MINOR\") ~ \"DISABLING\",\n      TRUE ~ vehicle_damage_extent\n    )\n  )\n\n# fix Invalid Coordinates (Montgomery County, MD boundaries)\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # montgomery County approximate boundaries\n    latitude = case_when(\n      latitude &lt; 38.9 | latitude &gt; 39.4 ~ NA_real_,\n      TRUE ~ latitude\n    ),\n    longitude = case_when(\n      longitude &lt; -77.5 | longitude &gt; -76.9 ~ NA_real_,\n      TRUE ~ longitude\n    )\n  )\n\n# clean Vehicle Makes and Models with High Frequency Patterns\n# analyze the most common patterns\nvehicle_patterns &lt;- drivers_data_clean %&gt;%\n  group_by(vehicle_make, vehicle_model) %&gt;%\n  summarise(\n    count = n(),\n    .groups = 'drop'\n  ) %&gt;%\n  arrange(desc(count)) %&gt;%\n  head(20)  # Look at top 20 patterns\n\n# now clean based on common patterns\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # standardize common model names\n    vehicle_model = case_when(\n      # remove digits from end of model names\n      str_detect(vehicle_model, \"[A-Z]+\\\\d+$\") ~ str_extract(vehicle_model, \"[A-Z]+\"),\n      # remove common suffixes\n      str_detect(vehicle_model, \"SDN|CPE|CVT|HBK\") ~ str_replace(vehicle_model, \"SDN|CPE|CVT|HBK\", \"\"),\n      TRUE ~ vehicle_model\n    ),\n    \n    # trim whitespace and standardize case\n    vehicle_model = str_trim(vehicle_model),\n    vehicle_make = str_trim(vehicle_make)\n  )\n\n# add Data Quality Score\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    data_quality_score = case_when(\n      data_quality_flags == \"VALID\" ~ 100,\n      data_quality_flags == \"LIGHT_TIME_MISMATCH\" ~ 90,\n      data_quality_flags == \"WEATHER_SURFACE_MISMATCH\" ~ 85,\n      data_quality_flags == \"INJURY_NO_DAMAGE_MISMATCH\" ~ 80,\n      data_quality_flags == \"INVALID_COORDINATES\" ~ 75,\n      data_quality_flags %in% c(\"FATAL_MINOR_DAMAGE_MISMATCH\", \"PARKED_MOVING_MISMATCH\") ~ 70,\n      TRUE ~ 60\n    )\n  )\n\n# verify cleaning results\ncleaning_verification &lt;- list()\n\n# check light-time consistency after cleaning\ncleaning_verification$light_time &lt;- drivers_data_clean %&gt;%\n  summarise(\n    light_time_mismatch_after = sum(\n      (format(crash_date/time, \"%H:%M:%S\") &gt;= \"18:00:00\" | \n       format(crash_date/time, \"%H:%M:%S\") &lt;= \"06:00:00\") & \n      light == \"DAYLIGHT\", \n      na.rm = TRUE\n    )\n  )\n\n# check weather-surface consistency after cleaning\ncleaning_verification$weather_surface &lt;- drivers_data_clean %&gt;%\n  summarise(\n    weather_surface_mismatch_after = sum(\n      (weather %in% c(\"RAIN\", \"SNOW\") & surface_condition == \"DRY\") |\n      (weather == \"CLEAR\" & surface_condition %in% c(\"ICE\", \"SNOW\")), \n      na.rm = TRUE\n    )\n  )\n\n# check data quality scores distribution\ncleaning_verification$quality_scores &lt;- drivers_data_clean %&gt;%\n  group_by(data_quality_score) %&gt;%\n  summarise(\n    count = n(),\n    percentage = n() / nrow(drivers_data_clean) * 100,\n    .groups = 'drop'\n  ) %&gt;%\n  arrange(desc(data_quality_score))\n\ncat(\"\\n=== CLEANING VERIFICATION RESULTS ===\\n\")\nprint(cleaning_verification)\n\nFinal Cleaning Steps Based on Verification Results\n\n# handle remaining weather-surface mismatches (577 cases)\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # create a more detailed weather-surface relationship\n    surface_condition = case_when(\n      # if it's raining, surface should be wet\n      weather %in% c(\"RAIN\", \"RAINING\") ~ \"WET\",\n      \n      # if it's snowing, surface should be snow or ice\n      weather == \"SNOW\" ~ \"SNOW\",\n      \n      # if it's freezing rain, surface should be ice\n      weather == \"FREEZING RAIN\" ~ \"ICE\",\n      \n      # for clear weather, keep existing surface condition if reasonable\n      weather == \"CLEAR\" & surface_condition %in% c(\"DRY\", \"WET\") ~ surface_condition,\n      \n      # for cloudy weather, keep existing surface condition\n      weather == \"CLOUDY\" ~ surface_condition,\n      \n      # for other cases, mark as unknown if inconsistent\n      TRUE ~ case_when(\n        surface_condition %in% c(\"DRY\", \"WET\", \"SNOW\", \"ICE\") ~ surface_condition,\n        TRUE ~ \"UNKNOWN\"\n      )\n    )\n  )\n\n# add more detailed quality flags and improve quality score\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # create compound quality flags for multiple issues\n    detailed_quality_flags = map2_chr(\n      data_quality_flags,\n      surface_condition,\n      ~case_when(\n        .x != \"VALID\" & .y == \"UNKNOWN\" ~ paste(.x, \"WITH_UNKNOWN_SURFACE\"),\n        TRUE ~ .x\n      )\n    ),\n    \n    # create a more nuanced quality score (0-100)\n    refined_quality_score = case_when(\n      data_quality_flags == \"VALID\" & !is.na(surface_condition) & \n      surface_condition != \"UNKNOWN\" ~ 100,\n      \n      data_quality_flags == \"VALID\" & \n      (is.na(surface_condition) | surface_condition == \"UNKNOWN\") ~ 95,\n      \n      data_quality_flags == \"LIGHT_TIME_MISMATCH\" ~ 90,\n      \n      data_quality_flags == \"WEATHER_SURFACE_MISMATCH\" & \n      !is.na(surface_condition) ~ 85,\n      \n      data_quality_flags == \"WEATHER_SURFACE_MISMATCH\" & \n      is.na(surface_condition) ~ 80,\n      \n      data_quality_flags == \"INJURY_NO_DAMAGE_MISMATCH\" ~ 75,\n      \n      data_quality_flags == \"INVALID_COORDINATES\" ~ 70,\n      \n      data_quality_flags %in% c(\"FATAL_MINOR_DAMAGE_MISMATCH\", \n                               \"PARKED_MOVING_MISMATCH\") ~ 65,\n      \n      TRUE ~ 60\n    )\n  )\n\n# create final data quality summary\nfinal_quality_summary &lt;- drivers_data_clean %&gt;%\n  summarise(\n    total_records = n(),\n    high_quality_records = sum(refined_quality_score &gt;= 95),\n    medium_quality_records = sum(refined_quality_score &gt;= 80 & refined_quality_score &lt; 95),\n    low_quality_records = sum(refined_quality_score &lt; 80),\n    mean_quality_score = mean(refined_quality_score),\n    median_quality_score = median(refined_quality_score),\n    \n    # percentage calculations\n    high_quality_percentage = (high_quality_records / total_records) * 100,\n    medium_quality_percentage = (medium_quality_records / total_records) * 100,\n    low_quality_percentage = (low_quality_records / total_records) * 100\n  )\n\ncat(\"\\n=== FINAL DATA QUALITY SUMMARY ===\\n\")\nprint(final_quality_summary)\n\n# create recommendations for further improvements\ncat(\"\\n=== RECOMMENDATIONS FOR FURTHER IMPROVEMENTS ===\\n\")\ncat(\"1. Weather-Surface Condition Relationship:\\n\")\ncat(\"   - \", sum(drivers_data_clean$surface_condition == \"UNKNOWN\"), \n    \"records still have unknown surface conditions\\n\")\ncat(\"   - Consider adding temperature data for better ice/snow validation\\n\\n\")\n\ncat(\"2. Geographic Data Quality:\\n\")\ncat(\"   - \", sum(is.na(drivers_data_clean$latitude) | is.na(drivers_data_clean$longitude)),\n    \"records have missing or invalid coordinates\\n\")\ncat(\"   - Consider implementing address geocoding for missing coordinates\\n\\n\")\n\ncat(\"3. Vehicle Information:\\n\")\ncat(\"   - \", sum(is.na(drivers_data_clean$vehicle_make)), \"records with missing vehicle makes\\n\")\ncat(\"   - \", sum(is.na(drivers_data_clean$vehicle_model)), \"records with missing vehicle models\\n\")\ncat(\"   - Consider implementing VIN decoding for missing vehicle information\\n\\n\")\n\nprocessing_summary &lt;- list(\n  initial_issues = list(\n    light_time_mismatch = 9744,\n    weather_surface_mismatch = 702,\n    injury_damage_mismatch = 306,\n    invalid_coordinates = 26,\n    fatal_minor_damage = 1,\n    parked_moving = 1\n  ),\n  \n  final_status = list(\n    quality_summary = final_quality_summary,\n    remaining_weather_surface_mismatch = 577,\n    remaining_unknown_surface = sum(drivers_data_clean$surface_condition == \"UNKNOWN\", na.rm = TRUE)\n  )\n)\n\ncat(\"\\n=== DATA PROCESSING SUMMARY ===\\n\")\ncat(\"Initial Issues vs. Final Status:\\n\")\ncat(\"- Light-Time Mismatches: \", processing_summary$initial_issues$light_time_mismatch, \n    \" -&gt; 0\\n\")\ncat(\"- Weather-Surface Mismatches: \", processing_summary$initial_issues$weather_surface_mismatch,\n    \" -&gt; \", processing_summary$final_status$remaining_weather_surface_mismatch, \"\\n\")\n\nVisualizations.\n\ninstall.packages(\"viridis\")\n\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(patchwork)  # for combining plots\nlibrary(viridis)    # for colorblind-friendly palettes\nlibrary(lubridate)\n\n# crash Time Patterns\ntime_plot &lt;- drivers_data_clean %&gt;%\n  mutate(\n    hour = hour(crash_date/time),\n    weekday = wday(crash_date/time, label = TRUE),\n    month = month(crash_date/time, label = TRUE)\n  ) %&gt;%\n  ggplot(aes(x = hour)) +\n  geom_histogram(binwidth = 1, fill = \"steelblue\", color = \"white\") +\n  labs(title = \"Distribution of Crashes by Hour of Day\",\n       x = \"Hour of Day\",\n       y = \"Number of Crashes\") +\n  theme_minimal() +\n  scale_x_continuous(breaks = seq(0, 23, 2))\n\n# injury Severity by Vehicle Type\nseverity_vehicle_plot &lt;- drivers_data_clean %&gt;%\n  filter(!is.na(vehicle_body_type), !is.na(injury_severity)) %&gt;%\n  count(vehicle_body_type, injury_severity) %&gt;%\n  group_by(vehicle_body_type) %&gt;%\n  mutate(pct = n/sum(n) * 100) %&gt;%\n  ggplot(aes(x = reorder(vehicle_body_type, -pct), y = pct, fill = injury_severity)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(title = \"Injury Severity by Vehicle Type\",\n       x = \"Vehicle Type\",\n       y = \"Percentage\",\n       fill = \"Injury Severity\") +\n  theme_minimal() +\n  scale_fill_viridis_d()\n\n# weather Conditions and Crash Frequency\nweather_plot &lt;- drivers_data_clean %&gt;%\n  count(weather) %&gt;%\n  ggplot(aes(x = reorder(weather, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"darkgreen\") +\n  coord_flip() +\n  labs(title = \"Crashes by Weather Condition\",\n       x = \"Weather Condition\",\n       y = \"Number of Crashes\") +\n  theme_minimal()\n\n# geographic Distribution of Crashes\nmap_plot &lt;- drivers_data_clean %&gt;%\n  filter(!is.na(latitude), !is.na(longitude)) %&gt;%\n  ggplot(aes(x = longitude, y = latitude)) +\n  geom_point(alpha = 0.1, color = \"red\") +\n  labs(title = \"Geographic Distribution of Crashes\",\n       x = \"Longitude\",\n       y = \"Latitude\") +\n  theme_minimal()\n\n# collision Types\ncollision_plot &lt;- drivers_data_clean %&gt;%\n  count(collision_type) %&gt;%\n  mutate(pct = n/sum(n) * 100) %&gt;%\n  ggplot(aes(x = reorder(collision_type, -pct), y = pct)) +\n  geom_bar(stat = \"identity\", fill = \"orange\") +\n  coord_flip() +\n  labs(title = \"Distribution of Collision Types\",\n       x = \"Collision Type\",\n       y = \"Percentage\") +\n  theme_minimal()\n\n# speed Limit and Crash Severity\nspeed_severity_plot &lt;- drivers_data_clean %&gt;%\n  filter(!is.na(speed_limit), !is.na(injury_severity)) %&gt;%\n  ggplot(aes(x = factor(speed_limit), fill = injury_severity)) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"Injury Severity by Speed Limit\",\n       x = \"Speed Limit\",\n       y = \"Proportion\",\n       fill = \"Injury Severity\") +\n  theme_minimal() +\n  scale_fill_viridis_d()\n\n# vehicle Age Distribution\nvehicle_age_plot &lt;- drivers_data_clean %&gt;%\n  mutate(vehicle_age = year(crash_date/time) - vehicle_year) %&gt;%\n  filter(vehicle_age &gt;= 0, vehicle_age &lt;= 30) %&gt;%\n  ggplot(aes(x = vehicle_age)) +\n  geom_histogram(binwidth = 1, fill = \"purple\", color = \"white\") +\n  labs(title = \"Distribution of Vehicle Age\",\n       x = \"Vehicle Age (Years)\",\n       y = \"Count\") +\n  theme_minimal()\n\n# surface Condition and Weather Relationship\nsurface_weather_plot &lt;- drivers_data_clean %&gt;%\n  filter(!is.na(surface_condition), !is.na(weather)) %&gt;%\n  count(surface_condition, weather) %&gt;%\n  group_by(weather) %&gt;%\n  mutate(pct = n/sum(n) * 100) %&gt;%\n  ggplot(aes(x = weather, y = surface_condition, fill = pct)) +\n  geom_tile() +\n  scale_fill_viridis_c() +\n  labs(title = \"Surface Condition by Weather\",\n       x = \"Weather\",\n       y = \"Surface Condition\",\n       fill = \"Percentage\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n# combine plots using patchwork\nlayout &lt;- \"\nAABB\nCCDD\nEEFF\nGGHH\n\"\n\ncombined_plots &lt;- time_plot + severity_vehicle_plot + \n                 weather_plot + map_plot + \n                 collision_plot + speed_severity_plot + \n                 vehicle_age_plot + surface_weather_plot +\n                 plot_layout(design = layout)\n\nprint(combined_plots)\n\nggsave(\"crash_analysis_dashboard.pdf\", combined_plots, width = 20, height = 24)\n\nsummary_stats &lt;- list(\n  \n  time_stats = drivers_data_clean %&gt;%\n    mutate(\n      hour = hour(crash_date/time),\n      weekday = wday(crash_date/time, label = TRUE)\n    ) %&gt;%\n    summarise(\n      peak_hour = names(which.max(table(hour))),\n      weekend_crashes = mean(weekday %in% c(\"Sat\", \"Sun\")) * 100\n    ),\n  \n  severity_stats = drivers_data_clean %&gt;%\n    group_by(injury_severity) %&gt;%\n    summarise(\n      count = n(),\n      percentage = n() / nrow(drivers_data_clean) * 100\n    ),\n  \n  weather_stats = drivers_data_clean %&gt;%\n    group_by(weather) %&gt;%\n    summarise(\n      count = n(),\n      percentage = n() / nrow(drivers_data_clean) * 100\n    ),\n  \n  vehicle_stats = drivers_data_clean %&gt;%\n    summarise(\n      avg_vehicle_age = mean(year(crash_date/time) - vehicle_year, na.rm = TRUE),\n      most_common_make = names(which.max(table(vehicle_make)))\n    )\n)\n\nprint(summary_stats)\n\nSave the final cleaned dataset.\n\ncurrent_timestamp &lt;- format(Sys.time(), \"%Y%m%d_%H%M\")\n\n# create directories if they don't exist\ndir.create(\"cleaned_data\", showWarnings = FALSE)\ndir.create(\"data_documentation\", showWarnings = FALSE)\n\nstr_result &lt;- capture.output(str(drivers_data_clean))\nprint(\"Data structure:\")\nprint(str_result)\n\ncolumn_info &lt;- data.frame(\n  column_name = names(drivers_data_clean),\n  data_type = sapply(drivers_data_clean, function(x) class(x)[1]),  # Take first class if multiple\n  stringsAsFactors = FALSE\n)\n\ncolumn_info$description &lt;- sapply(column_info$column_name, function(col_name) {\n  case_when(\n    col_name == \"report_number\" ~ \"Unique report identifier\",\n    col_name == \"local_case_number\" ~ \"Local case number for the incident\",\n    col_name == \"agency_name\" ~ \"Name of reporting agency\",\n    col_name == \"acrs_report_type\" ~ \"Type of crash report\",\n    col_name == \"crash_date/time\" ~ \"Date and time of the crash\",\n    col_name == \"route_type\" ~ \"Type of route where crash occurred\",\n    col_name == \"road_name\" ~ \"Name of the road where crash occurred\",\n    col_name == \"cross_street_name\" ~ \"Name of the nearest cross-street\",\n    col_name == \"off_road_description\" ~ \"Description for off-road incidents\",\n    col_name == \"municipality\" ~ \"Municipality where crash occurred\",\n    col_name == \"related_non_motorist\" ~ \"Type of non-motorist involved\",\n    col_name == \"collision_type\" ~ \"Type of collision\",\n    col_name == \"weather\" ~ \"Weather conditions during crash\",\n    col_name == \"surface_condition\" ~ \"Road surface condition\",\n    col_name == \"light\" ~ \"Light conditions\",\n    col_name == \"traffic_control\" ~ \"Traffic control present\",\n    col_name == \"driver_substance_abuse\" ~ \"Driver substance abuse status\",\n    col_name == \"non_motorist_substance_abuse\" ~ \"Non-motorist substance abuse status\",\n    col_name == \"person_id\" ~ \"Unique identifier for person involved\",\n    col_name == \"driver_at_fault\" ~ \"Indicator if driver was at fault\",\n    col_name == \"injury_severity\" ~ \"Severity of injuries\",\n    col_name == \"circumstance\" ~ \"Contributing circumstances\",\n    col_name == \"driver_distracted_by\" ~ \"Driver distraction factors\",\n    col_name == \"drivers_license_state\" ~ \"State of driver's license\",\n    col_name == \"vehicle_id\" ~ \"Unique identifier for vehicle\",\n    col_name == \"vehicle_damage_extent\" ~ \"Extent of vehicle damage\",\n    col_name == \"vehicle_first_impact_location\" ~ \"Location of first impact on vehicle\",\n    col_name == \"vehicle_body_type\" ~ \"Type of vehicle body\",\n    col_name == \"vehicle_movement\" ~ \"Vehicle movement during crash\",\n    col_name == \"vehicle_going_dir\" ~ \"Direction vehicle was traveling\",\n    col_name == \"speed_limit\" ~ \"Posted speed limit\",\n    col_name == \"driverless_vehicle\" ~ \"Indicator if vehicle was driverless\",\n    col_name == \"parked_vehicle\" ~ \"Indicator if vehicle was parked\",\n    col_name == \"vehicle_year\" ~ \"Year of vehicle manufacture\",\n    col_name == \"vehicle_make\" ~ \"Vehicle manufacturer\",\n    col_name == \"vehicle_model\" ~ \"Vehicle model\",\n    col_name == \"latitude\" ~ \"Latitude of crash location\",\n    col_name == \"longitude\" ~ \"Longitude of crash location\",\n    col_name == \"location\" ~ \"Combined location coordinates\",\n    col_name == \"data_quality_flags\" ~ \"Data quality flags from cleaning process\",\n    col_name == \"detailed_quality_flags\" ~ \"Detailed quality flags from cleaning process\",\n    col_name == \"refined_quality_score\" ~ \"Numerical score indicating data quality\",\n    TRUE ~ paste(\"Description for\", col_name)  # Default description for any new columns\n  )\n})\n\ncolumn_info$example_values &lt;- sapply(drivers_data_clean, function(x) {\n  if (is.numeric(x)) {\n    paste(\"Range:\", min(x, na.rm = TRUE), \"to\", max(x, na.rm = TRUE))\n  } else {\n    unique_vals &lt;- unique(na.omit(x))\n    if (length(unique_vals) &gt; 5) {\n      paste(paste(unique_vals[1:5], collapse = \", \"), \"... and\", length(unique_vals) - 5, \"more values\")\n    } else {\n      paste(unique_vals, collapse = \", \")\n    }\n  }\n})\n\nwrite_csv(drivers_data_clean, \n          file = paste0(\"cleaned_data/crash_data_clean_\", current_timestamp, \".csv\"))\n\nsaveRDS(drivers_data_clean, \n        file = paste0(\"cleaned_data/crash_data_clean_\", current_timestamp, \".rds\"))\n\nwrite_csv(column_info, \n          file = paste0(\"data_documentation/data_dictionary_\", current_timestamp, \".csv\"))\n\nreadme_text &lt;- sprintf(\"\n# Crash Data Cleaning Documentation\n\n## Version Information\n- Date Created: %s\n- Original Records: 186,931\n- Final Records: %d\n\n## Data Quality Metrics\n- Total Records: %d\n- Records with Quality Flags: %d\n- Clean Records: %d\n\n## Files in this Package\n1. crash_data_clean_%s.csv - Main data file (CSV format)\n2. crash_data_clean_%s.rds - R data file (RDS format)\n3. data_dictionary_%s.csv - Data dictionary with column descriptions\n\n## Column Summary\nTotal number of columns: %d\nSee data dictionary file for detailed information about each column.\n\n## Data Quality Notes\n- Data has been cleaned and standardized\n- Quality flags have been added to mark potential issues\n- Missing values have been handled according to context\n- Inconsistent categories have been standardized\n\n## Usage Notes\n- Please refer to the data dictionary for column descriptions\n- Check quality flags before analysis\n- Some columns may contain standardized values\n\n## Contact\nFor questions about this dataset, please contact [Your Contact Information]\n\n## Last Updated\n%s\n\",\n    format(Sys.time(), \"%Y-%m-%d %H:%M:%S\"),\n    nrow(drivers_data_clean),\n    nrow(drivers_data_clean),\n    sum(drivers_data_clean$data_quality_flags != \"VALID\"),\n    sum(drivers_data_clean$data_quality_flags == \"VALID\"),\n    current_timestamp,\n    current_timestamp,\n    current_timestamp,\n    ncol(drivers_data_clean),\n    format(Sys.time(), \"%Y-%m-%d %H:%M:%S\")\n)\n\nwriteLines(readme_text, \n           paste0(\"data_documentation/README_\", current_timestamp, \".md\"))\n\nzip_files &lt;- c(\n    paste0(\"cleaned_data/crash_data_clean_\", current_timestamp, \".csv\"),\n    paste0(\"cleaned_data/crash_data_clean_\", current_timestamp, \".rds\"),\n    paste0(\"data_documentation/data_dictionary_\", current_timestamp, \".csv\"),\n    paste0(\"data_documentation/README_\", current_timestamp, \".md\")\n)\n\nzip(paste0(\"cleaned_data/crash_data_package_\", current_timestamp, \".zip\"),\n    files = zip_files)\n\ncat(\"\\nData saving complete!\\n\")\ncat(\"Files saved:\\n\")\ncat(\"1. CSV data file\\n\")\ncat(\"2. RDS data file\\n\")\ncat(\"3. Data dictionary\\n\")\ncat(\"4. README documentation\\n\")\ncat(\"5. Complete package (zip)\\n\")\ncat(\"\\nLocation: ./cleaned_data/ and ./data_documentation/\\n\")"
  },
  {
    "objectID": "dataset/Cleaning_Xiang.html#setup",
    "href": "dataset/Cleaning_Xiang.html#setup",
    "title": "crash car data clean",
    "section": "",
    "text": "options(repos = c(CRAN = \"https://cran.r-project.org\"))\ninstall.packages(\"skimr\")\n\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(lubridate)\nlibrary(skimr)  # for quick summary statistics\n\n\ninstall.packages(\"RCurl\")\nlibrary(RCurl)\nx &lt;- getURL(\"https://raw.githubusercontent.com/sussmanbu/ma-4615-fa24-final-project-group-1/main/Crash_Reporting_Drivers_Data.csv\")\ndrivers_data &lt;- read.csv(text = x)\n\n\nglimpse(drivers_data)\n\nskim(drivers_data)\n\nClean Column Names\n\ndrivers_data_clean &lt;- drivers_data %&gt;%\n  clean_names()\n\ncolnames(drivers_data_clean)\n\nHandle Date/Time Format\n\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    crash_date/time &lt;- mdy_hms(crash_date/time),\n    # create separate date and time columns if needed\n    crash_date = date(crash_date/time),\n    crash_time = format(crash_date/time, \"%H:%M:%S\")\n  )\n\nStandardize Categorical Variables\n\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # standardize agency names\n    agency_name = str_trim(agency_name),\n    \n    # clean and standardize route type\n    route_type = case_when(\n      is.na(route_type) ~ \"Unknown\",\n      TRUE ~ route_type\n    ),\n    \n    # standardize weather conditions\n    weather = str_to_title(weather),\n    \n    # clean surface condition\n    surface_condition = case_when(\n      is.na(surface_condition) ~ \"Unknown\",\n      TRUE ~ surface_condition\n    ),\n    \n    # standardize driver at fault\n    driver_at_fault = case_when(\n      str_to_lower(driver_at_fault) == \"yes\" ~ \"Yes\",\n      str_to_lower(driver_at_fault) == \"no\" ~ \"No\",\n      TRUE ~ \"Unknown\"\n    )\n  )\n\nHandle Missing Values\n\n# calculate missing values percentage\nmissing_values &lt;- drivers_data_clean %&gt;%\n  summarise(across(everything(), ~sum(is.na(.))/n()*100)) %&gt;%\n  pivot_longer(everything(), \n              names_to = \"column\", \n              values_to = \"missing_percentage\") %&gt;%\n  arrange(desc(missing_percentage))\n\n# display columns with missing values\nprint(missing_values %&gt;% filter(missing_percentage &gt; 0))\n\n# handle missing values based on column type\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # for route_type, road_name, and cross_street_name\n    # keep NA as is since they might be meaningful (e.g., off-road incidents)\n    \n    # for numeric columns, consider if 0 or NA is more appropriate\n    speed_limit = if_else(is.na(speed_limit), 0, speed_limit),\n    \n    # for categorical columns, mark unknown\n    surface_condition = if_else(is.na(surface_condition), \"Unknown\", surface_condition),\n    traffic_control = if_else(is.na(traffic_control), \"Unknown\", traffic_control)\n  )\n\nData Validation and Consistency Checks\n\n# check for logical consistencies\nvalidation_results &lt;- drivers_data_clean %&gt;%\n  summarise(\n    # check for future dates\n    future_dates = sum(crash_date &gt; Sys.Date()),\n    \n    # check for valid speed limits\n    invalid_speed = sum(speed_limit &gt; 70 | speed_limit &lt; 0, na.rm = TRUE),\n    \n    # check for valid vehicle years\n    invalid_vehicle_year = sum(vehicle_year &lt; 1900 | vehicle_year &gt; year(Sys.Date()) + 1, na.rm = TRUE),\n    \n    # check for valid coordinates\n    invalid_lat = sum(latitude &lt; 38 | latitude &gt; 40, na.rm = TRUE),\n    invalid_lon = sum(longitude &gt; -76 | longitude &lt; -78, na.rm = TRUE)\n  )\n\nprint(validation_results)\n\n# create flags for potential data quality issues\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    data_quality_flag = case_when(\n      crash_date &gt; Sys.Date() ~ \"Future date\",\n      speed_limit &gt; 70 | speed_limit &lt; 0 ~ \"Invalid speed limit\",\n      vehicle_year &lt; 1900 | vehicle_year &gt; year(Sys.Date()) + 1 ~ \"Invalid vehicle year\",\n      latitude &lt; 38 | latitude &gt; 40 | longitude &gt; -76 | longitude &lt; -78 ~ \"Invalid coordinates\",\n      TRUE ~ \"Valid\"\n    )\n  )\n\nCheck Value Distributions\n\n# check common categories\ncategory_summaries &lt;- list(\n  collision_types = table(drivers_data_clean$collision_type),\n  weather_conditions = table(drivers_data_clean$weather),\n  vehicle_types = table(drivers_data_clean$vehicle_body_type),\n  injury_severity = table(drivers_data_clean$injury_severity)\n)\n\nprint(category_summaries)\n\nCollision Types Need Standardization:\n\n# standardize collision types\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    collision_type = case_when(\n      collision_type %in% c(\"Angle\", \"ANGLE MEETS LEFT HEAD ON\", \"ANGLE MEETS LEFT TURN\",\n                           \"ANGLE MEETS RIGHT TURN\", \"STRAIGHT MOVEMENT ANGLE\") ~ \"ANGLE\",\n      collision_type %in% c(\"Front to Front\", \"HEAD ON\", \"HEAD ON LEFT TURN\") ~ \"HEAD_ON\",\n      collision_type %in% c(\"Front to Rear\", \"SAME DIR REAR END\", \n                           \"SAME DIR REND LEFT TURN\", \"SAME DIR REND RIGHT TURN\") ~ \"REAR_END\",\n      collision_type %in% c(\"SAME DIRECTION SIDESWIPE\", \"Sideswipe, Same Direction\") ~ \"SIDESWIPE_SAME_DIR\",\n      collision_type %in% c(\"OPPOSITE DIRECTION SIDESWIPE\", \"Sideswipe, Opposite Direction\") ~ \"SIDESWIPE_OPPOSITE_DIR\",\n      collision_type %in% c(\"SINGLE VEHICLE\", \"Single Vehicle\") ~ \"SINGLE_VEHICLE\",\n      collision_type %in% c(\"Other\", \"OTHER\") ~ \"OTHER\",\n      collision_type %in% c(\"Unknown\", \"UNKNOWN\", \"N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\nWeather Conditions Need Standardization:\n\n# standardize weather conditions\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    weather = case_when(\n      str_detect(str_to_upper(weather), \"CLEAR\") ~ \"CLEAR\",\n      str_detect(str_to_upper(weather), \"CLOUD\") ~ \"CLOUDY\",\n      str_detect(str_to_upper(weather), \"RAIN|RAINING\") ~ \"RAIN\",\n      str_detect(str_to_upper(weather), \"SNOW|BLOWING SNOW\") ~ \"SNOW\",\n      str_detect(str_to_upper(weather), \"FOG|SMOG|SMOKE\") ~ \"FOG\",\n      weather %in% c(\"N/A\", \"Unknown\", \"UNKNOWN\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\nVehicle Types Need Major Cleanup:\n\n# standardize vehicle types\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    vehicle_body_type = case_when(\n      str_detect(str_to_upper(vehicle_body_type), \"PASSENGER CAR|PASSENGER|CAR\") ~ \"PASSENGER_CAR\",\n      str_detect(str_to_upper(vehicle_body_type), \"SUV|SPORT UTILITY|UTILITY VEHICLE\") ~ \"SUV\",\n      str_detect(str_to_upper(vehicle_body_type), \"PICKUP|LIGHT TRUCK\") ~ \"PICKUP_TRUCK\",\n      str_detect(str_to_upper(vehicle_body_type), \"VAN|CARGO\") ~ \"VAN\",\n      str_detect(str_to_upper(vehicle_body_type), \"BUS\") ~ \"BUS\",\n      str_detect(str_to_upper(vehicle_body_type), \"MOTORCYCLE|MOPED\") ~ \"MOTORCYCLE\",\n      str_detect(str_to_upper(vehicle_body_type), \"EMERGENCY|POLICE|FIRE|AMBULANCE\") ~ \"EMERGENCY_VEHICLE\",\n      vehicle_body_type %in% c(\"N/A\", \"Unknown\", \"UNKNOWN\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\nInjury Severity Standardization:\n\n# standardize injury severity\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    injury_severity = case_when(\n      str_detect(str_to_upper(injury_severity), \"FATAL\") ~ \"FATAL\",\n      str_detect(str_to_upper(injury_severity), \"NO APPARENT|NO INJURY\") ~ \"NO_INJURY\",\n      str_detect(str_to_upper(injury_severity), \"POSSIBLE\") ~ \"POSSIBLE_INJURY\",\n      str_detect(str_to_upper(injury_severity), \"MINOR\") ~ \"MINOR_INJURY\",\n      str_detect(str_to_upper(injury_severity), \"SERIOUS\") ~ \"SERIOUS_INJURY\",\n      TRUE ~ \"UNKNOWN\"\n    )\n  )\n\nHandle Missing Values Strategy (based on the missing percentage analysis):\n\n# handle missing values based on context\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # high missing percentage (&gt;90%) - keep as NA but add flag\n    has_non_motorist = !is.na(related_non_motorist),\n    \n    # medium missing percentage (10-90%) - add meaningful categories\n    municipality = if_else(is.na(municipality), \"UNINCORPORATED\", municipality),\n    road_name = if_else(is.na(road_name), \"OFF_ROAD\", road_name),\n    cross_street_name = if_else(is.na(cross_street_name), \"NOT_APPLICABLE\", cross_street_name),\n    \n    # low missing percentage (&lt;10%) - impute with \"UNKNOWN\"\n    drivers_license_state = if_else(is.na(drivers_license_state), \"UNKNOWN\", drivers_license_state),\n    circumstance = if_else(is.na(circumstance), \"UNKNOWN\", circumstance),\n    vehicle_going_dir = if_else(is.na(vehicle_going_dir), \"UNKNOWN\", vehicle_going_dir)\n  )\n\nHandle Data Quality Issues (based on validation results):\n\n# add data quality flags\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    data_quality_flags = case_when(\n      speed_limit &gt; 70 | speed_limit &lt; 0 ~ \"INVALID_SPEED_LIMIT\",\n      vehicle_year &lt; 1900 | vehicle_year &gt; year(Sys.Date()) + 1 ~ \"INVALID_VEHICLE_YEAR\",\n      latitude &lt; 38 | latitude &gt; 40 | longitude &gt; -76 | longitude &lt; -78 ~ \"INVALID_COORDINATES\",\n      TRUE ~ \"VALID\"\n    ),\n    \n    # clean speed limits\n    speed_limit = case_when(\n      speed_limit &gt; 70 ~ NA_real_,\n      speed_limit &lt; 0 ~ NA_real_,\n      TRUE ~ speed_limit\n    ),\n    \n    # clean vehicle years\n    vehicle_year = if_else(\n      vehicle_year &lt; 1900 | vehicle_year &gt; year(Sys.Date()) + 1,\n      NA_real_,\n      vehicle_year\n    )\n  )\n\nAdditional cleaning for specific columns:\n\n# additional cleaning steps for specific columns and vehicle data\n\n# standardize vehicle makes (common misspellings and abbreviations)\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    vehicle_make = case_when(\n      # honda variations\n      str_detect(str_to_upper(vehicle_make), \"HOND|HDA\") ~ \"HONDA\",\n      \n      # toyota variations\n      str_detect(str_to_upper(vehicle_make), \"TOY|TOYT\") ~ \"TOYOTA\",\n      \n      # ford\n      str_detect(str_to_upper(vehicle_make), \"^FRD|FORD\") ~ \"FORD\",\n      \n      # chevrolet variations\n      str_detect(str_to_upper(vehicle_make), \"CHEV|CHEVY|CHV\") ~ \"CHEVROLET\",\n      \n      # nissan variations\n      str_detect(str_to_upper(vehicle_make), \"NISS|NISN\") ~ \"NISSAN\",\n      \n      # hyundai variations\n      str_detect(str_to_upper(vehicle_make), \"HYUN|HYU\") ~ \"HYUNDAI\",\n      \n      # volkswagen variations\n      str_detect(str_to_upper(vehicle_make), \"VW|VOLK|VOLKS\") ~ \"VOLKSWAGEN\",\n      \n      # BMW\n      str_detect(str_to_upper(vehicle_make), \"BMW|BMV\") ~ \"BMW\",\n      \n      # Mercedes-Benz variations\n      str_detect(str_to_upper(vehicle_make), \"MERZ|MENZ|MERCEDES|BENZ\") ~ \"MERCEDES-BENZ\",\n      \n      # lexus\n      str_detect(str_to_upper(vehicle_make), \"LEX|LEXS\") ~ \"LEXUS\",\n      \n      # mazda variations\n      str_detect(str_to_upper(vehicle_make), \"MAZ|MAZD\") ~ \"MAZDA\",\n      \n      # subaru variations\n      str_detect(str_to_upper(vehicle_make), \"SUB|SUBR\") ~ \"SUBARU\",\n      \n      # kia\n      str_detect(str_to_upper(vehicle_make), \"^KIA\") ~ \"KIA\",\n      \n      # audi\n      str_detect(str_to_upper(vehicle_make), \"^AUD\") ~ \"AUDI\",\n      \n      # acura\n      str_detect(str_to_upper(vehicle_make), \"ACUR|ACU\") ~ \"ACURA\",\n      \n      # infinity\n      str_detect(str_to_upper(vehicle_make), \"INF|INFIN\") ~ \"INFINITI\",\n      \n      TRUE ~ str_to_upper(vehicle_make)\n    )\n  )\n\n# clean vehicle models\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    vehicle_model = case_when(\n      # clean common abbreviations\n      str_detect(str_to_upper(vehicle_model), \"^CRV|CR-V\") ~ \"CR-V\",\n      str_detect(str_to_upper(vehicle_model), \"^RAV|RAV4\") ~ \"RAV4\",\n      str_detect(str_to_upper(vehicle_model), \"ACCORD|ACRD\") ~ \"ACCORD\",\n      str_detect(str_to_upper(vehicle_model), \"CAMRY|CAM\") ~ \"CAMRY\",\n      str_detect(str_to_upper(vehicle_model), \"CIVIC|CVC\") ~ \"CIVIC\",\n      str_detect(str_to_upper(vehicle_model), \"ALTIMA|ALT\") ~ \"ALTIMA\",\n      str_detect(str_to_upper(vehicle_model), \"COROLLA|COR\") ~ \"COROLLA\",\n      str_detect(str_to_upper(vehicle_model), \"EXPLORER|EXPLR\") ~ \"EXPLORER\",\n      str_detect(str_to_upper(vehicle_model), \"F-150|F150\") ~ \"F-150\",\n      str_detect(str_to_upper(vehicle_model), \"HIGHLANDER|HGLDR\") ~ \"HIGHLANDER\",\n      TRUE ~ str_to_upper(vehicle_model)\n    )\n  )\n\n# additional cleaning for other specific columns\n\n# clean light conditions\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    light = case_when(\n      str_detect(str_to_upper(light), \"DAYLIGHT\") ~ \"DAYLIGHT\",\n      str_detect(str_to_upper(light), \"DARK.*LIGHT.*ON|LIGHTED\") ~ \"DARK_WITH_LIGHTING\",\n      str_detect(str_to_upper(light), \"DARK.*NO.*LIGHT|UNLIGHTED\") ~ \"DARK_NO_LIGHTING\",\n      str_detect(str_to_upper(light), \"DAWN\") ~ \"DAWN\",\n      str_detect(str_to_upper(light), \"DUSK\") ~ \"DUSK\",\n      str_detect(str_to_upper(light), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# clean surface conditions\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    surface_condition = case_when(\n      str_detect(str_to_upper(surface_condition), \"DRY\") ~ \"DRY\",\n      str_detect(str_to_upper(surface_condition), \"WET\") ~ \"WET\",\n      str_detect(str_to_upper(surface_condition), \"ICE|ICY\") ~ \"ICE\",\n      str_detect(str_to_upper(surface_condition), \"SNOW|SLUSH\") ~ \"SNOW\",\n      str_detect(str_to_upper(surface_condition), \"SAND|DIRT|MUD\") ~ \"SAND_DIRT_MUD\",\n      str_detect(str_to_upper(surface_condition), \"OIL|GREASE\") ~ \"OIL_GREASE\",\n      is.na(surface_condition) | str_detect(str_to_upper(surface_condition), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# clean traffic control\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    traffic_control = case_when(\n      str_detect(str_to_upper(traffic_control), \"SIGNAL|TRAFFIC LIGHT\") ~ \"TRAFFIC_SIGNAL\",\n      str_detect(str_to_upper(traffic_control), \"STOP SIGN\") ~ \"STOP_SIGN\",\n      str_detect(str_to_upper(traffic_control), \"YIELD\") ~ \"YIELD_SIGN\",\n      str_detect(str_to_upper(traffic_control), \"FLASHING\") ~ \"FLASHING_SIGNAL\",\n      str_detect(str_to_upper(traffic_control), \"NO CONTROL|NONE\") ~ \"NO_CONTROL\",\n      str_detect(str_to_upper(traffic_control), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# clean driver substance abuse\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    driver_substance_abuse = case_when(\n      str_detect(str_to_upper(driver_substance_abuse), \"NONE|NO ABUSE\") ~ \"NONE\",\n      str_detect(str_to_upper(driver_substance_abuse), \"ALCOHOL\") ~ \"ALCOHOL\",\n      str_detect(str_to_upper(driver_substance_abuse), \"DRUG\") ~ \"DRUGS\",\n      str_detect(str_to_upper(driver_substance_abuse), \"BOTH|ALCOHOL.*DRUG|DRUG.*ALCOHOL\") ~ \"ALCOHOL_AND_DRUGS\",\n      str_detect(str_to_upper(driver_substance_abuse), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# clean driver distracted by\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    driver_distracted_by = case_when(\n      str_detect(str_to_upper(driver_distracted_by), \"NOT DISTRACTED\") ~ \"NOT_DISTRACTED\",\n      str_detect(str_to_upper(driver_distracted_by), \"CELL|PHONE|MOBILE\") ~ \"CELL_PHONE\",\n      str_detect(str_to_upper(driver_distracted_by), \"PASSENGER\") ~ \"PASSENGER\",\n      str_detect(str_to_upper(driver_distracted_by), \"RADIO|AUDIO\") ~ \"AUDIO_EQUIPMENT\",\n      str_detect(str_to_upper(driver_distracted_by), \"EAT|DRINK\") ~ \"EATING_DRINKING\",\n      str_detect(str_to_upper(driver_distracted_by), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# verify our cleaning by checking the unique values in each cleaned column\nverification_check &lt;- function(data, columns) {\n  map(columns, ~{\n    unique_vals &lt;- data %&gt;% \n      pull(.) %&gt;% \n      unique() %&gt;% \n      sort()\n    \n    cat(\"\\nUnique values in\", ., \":\\n\")\n    print(unique_vals)\n    cat(\"\\n\")\n  })\n}\n\n# verify the cleaning results for key columns\ncolumns_to_verify &lt;- c(\n  \"vehicle_make\", \n  \"vehicle_model\",\n  \"light\",\n  \"surface_condition\",\n  \"traffic_control\",\n  \"driver_substance_abuse\",\n  \"driver_distracted_by\"\n)\n\nverification_check(drivers_data_clean, columns_to_verify)\n\nvehicle_summary &lt;- drivers_data_clean %&gt;%\n  group_by(vehicle_make, vehicle_model) %&gt;%\n  summarise(\n    count = n(),\n    .groups = 'drop'\n  ) %&gt;%\n  arrange(desc(count))\n\nprint(vehicle_summary)\n\n\n# clean and standardize agency names\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    agency_name = case_when(\n      str_detect(str_to_upper(agency_name), \"MONTGOMERY|MONT|MCP\") ~ \"MONTGOMERY COUNTY POLICE\",\n      str_detect(str_to_upper(agency_name), \"GAITHERSBURG|GAITH\") ~ \"GAITHERSBURG POLICE\",\n      str_detect(str_to_upper(agency_name), \"ROCKVILLE|ROCK\") ~ \"ROCKVILLE POLICE\",\n      str_detect(str_to_upper(agency_name), \"TAKOMA|TAK\") ~ \"TAKOMA PARK POLICE\",\n      str_detect(str_to_upper(agency_name), \"PARK|MNCPP\") ~ \"MD NATIONAL CAPITAL PARK POLICE\",\n      TRUE ~ str_to_upper(agency_name)\n    )\n  )\n\n# clean and standardize vehicle damage extent\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    vehicle_damage_extent = case_when(\n      str_detect(str_to_upper(vehicle_damage_extent), \"NONE|NO DAMAGE\") ~ \"NO_DAMAGE\",\n      str_detect(str_to_upper(vehicle_damage_extent), \"MINOR|SUPERFICIAL\") ~ \"MINOR\",\n      str_detect(str_to_upper(vehicle_damage_extent), \"FUNCTIONAL\") ~ \"FUNCTIONAL\",\n      str_detect(str_to_upper(vehicle_damage_extent), \"DISABLE|DISABLING\") ~ \"DISABLING\",\n      str_detect(str_to_upper(vehicle_damage_extent), \"DESTROYED\") ~ \"DESTROYED\",\n      str_detect(str_to_upper(vehicle_damage_extent), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# clean and standardize vehicle first impact location\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    vehicle_first_impact_location = case_when(\n      str_detect(str_to_upper(vehicle_first_impact_location), \"FRONT|TWELVE|12\") ~ \"FRONT\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"RIGHT FRONT|ONE|1\") ~ \"RIGHT_FRONT\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"RIGHT SIDE|THREE|3\") ~ \"RIGHT_SIDE\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"RIGHT REAR|FOUR|4\") ~ \"RIGHT_REAR\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"REAR|SIX|6\") ~ \"REAR\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"LEFT REAR|SEVEN|7|EIGHT|8\") ~ \"LEFT_REAR\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"LEFT SIDE|NINE|9\") ~ \"LEFT_SIDE\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"LEFT FRONT|TEN|10|ELEVEN|11\") ~ \"LEFT_FRONT\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# clean and standardize vehicle movement\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    vehicle_movement = case_when(\n      str_detect(str_to_upper(vehicle_movement), \"STRAIGHT|CONSTANT\") ~ \"STRAIGHT\",\n      str_detect(str_to_upper(vehicle_movement), \"TURN.*RIGHT\") ~ \"TURNING_RIGHT\",\n      str_detect(str_to_upper(vehicle_movement), \"TURN.*LEFT\") ~ \"TURNING_LEFT\",\n      str_detect(str_to_upper(vehicle_movement), \"STOP|SLOW\") ~ \"SLOWING_STOPPING\",\n      str_detect(str_to_upper(vehicle_movement), \"BACK|REVERSE\") ~ \"BACKING\",\n      str_detect(str_to_upper(vehicle_movement), \"PARK\") ~ \"PARKED\",\n      str_detect(str_to_upper(vehicle_movement), \"START\") ~ \"STARTING\",\n      str_detect(str_to_upper(vehicle_movement), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n\n# create a comprehensive validation function\nvalidate_crash_data &lt;- function(data) {\n  # initialize empty list for validation results\n  validation_results &lt;- list()\n  \n  # temporal validations\n  validation_results$temporal &lt;- data %&gt;%\n    summarise(\n      future_dates = sum(crash_date/time &gt; Sys.time()),\n      weekend_crashes = sum(lubridate::wday(crash_date/time, week_start = 1) %in% c(6,7)),\n      night_crashes = sum(format(crash_date/time, \"%H:%M:%S\") &gt;= \"18:00:00\" | \n                         format(crash_date/time, \"%H:%M:%S\") &lt;= \"06:00:00\"),\n      crashes_by_hour = table(format(crash_date/time, \"%H\"))\n    )\n  \n  # geographic validations\n  validation_results$geographic &lt;- data %&gt;%\n    summarise(\n      invalid_lat = sum(latitude &lt; 38 | latitude &gt; 40, na.rm = TRUE),\n      invalid_lon = sum(longitude &gt; -76 | longitude &lt; -78, na.rm = TRUE),\n      missing_coords = sum(is.na(latitude) | is.na(longitude)),\n      unique_locations = n_distinct(location, na.rm = TRUE)\n    )\n  \n  # vehicle validations\n  validation_results$vehicle &lt;- data %&gt;%\n    summarise(\n      invalid_years = sum(vehicle_year &lt; 1900 | vehicle_year &gt; year(Sys.Date()) + 1, na.rm = TRUE),\n      missing_makes = sum(is.na(vehicle_make)),\n      missing_models = sum(is.na(vehicle_model)),\n      unique_makes = n_distinct(vehicle_make, na.rm = TRUE),\n      unique_models = n_distinct(vehicle_model, na.rm = TRUE)\n    )\n  \n  # logical consistency checks\n  validation_results$logical &lt;- data %&gt;%\n    summarise(\n      # check if parked vehicles are marked as having movement\n      parked_moving = sum(parked_vehicle == \"Yes\" & vehicle_movement != \"PARKED\", na.rm = TRUE),\n      \n      # check for injuries in no-damage crashes\n      injuries_no_damage = sum(injury_severity != \"NO_INJURY\" & \n                             vehicle_damage_extent == \"NO_DAMAGE\", na.rm = TRUE),\n      \n      # check for fatal crashes without severe damage\n      fatal_minor_damage = sum(injury_severity == \"FATAL\" & \n                             vehicle_damage_extent %in% c(\"NO_DAMAGE\", \"MINOR\"), na.rm = TRUE),\n      \n      # check for driverless vehicles marked with driver characteristics\n      driverless_with_driver = sum(driverless_vehicle == \"Yes\" & \n                                  !is.na(driver_substance_abuse), na.rm = TRUE)\n    )\n  \n  # cross-reference checks\n  validation_results$cross_reference &lt;- data %&gt;%\n    summarise(\n      # light condition vs time of day consistency\n      light_time_mismatch = sum(\n        (format(crash_date/time, \"%H:%M:%S\") &gt;= \"18:00:00\" | \n         format(crash_date/time, \"%H:%M:%S\") &lt;= \"06:00:00\") & \n        light == \"DAYLIGHT\", na.rm = TRUE\n      ),\n      \n      # weather vs surface condition consistency\n      weather_surface_mismatch = sum(\n        (weather %in% c(\"RAIN\", \"SNOW\") & surface_condition == \"DRY\") |\n        (weather == \"CLEAR\" & surface_condition %in% c(\"ICE\", \"SNOW\")), \n        na.rm = TRUE\n      )\n    )\n  \n  # value distribution checks\n  validation_results$distributions &lt;- list(\n    speed_distribution = table(data$speed_limit),\n    damage_by_speed = table(data$vehicle_damage_extent, cut(data$speed_limit, \n                                                          breaks = c(0, 25, 35, 45, 55, Inf))),\n    injury_by_speed = table(data$injury_severity, cut(data$speed_limit, \n                                                    breaks = c(0, 25, 35, 45, 55, Inf)))\n  )\n  \n  return(validation_results)\n}\n\n# run validation and create summary report\nvalidation_summary &lt;- validate_crash_data(drivers_data_clean)\n\n# create a function to print validation results in a readable format\nprint_validation_summary &lt;- function(validation_results) {\n  cat(\"\\n=== VALIDATION SUMMARY ===\\n\")\n  \n  cat(\"\\nTEMPORAL VALIDATION:\")\n  cat(\"\\n- Future dates:\", validation_results$temporal$future_dates)\n  cat(\"\\n- Weekend crashes:\", validation_results$temporal$weekend_crashes)\n  cat(\"\\n- Night crashes:\", validation_results$temporal$night_crashes)\n  \n  cat(\"\\n\\nGEOGRAPHIC VALIDATION:\")\n  print(validation_results$geographic)\n  \n  cat(\"\\n\\nVEHICLE VALIDATION:\")\n  print(validation_results$vehicle)\n  \n  cat(\"\\n\\nLOGICAL CONSISTENCY CHECKS:\")\n  print(validation_results$logical)\n  \n  cat(\"\\n\\nCROSS-REFERENCE CHECKS:\")\n  print(validation_results$cross_reference)\n  \n  cat(\"\\n\\nDISTRIBUTIONS:\")\n  cat(\"\\nSpeed Limit Distribution:\\n\")\n  print(validation_results$distributions$speed_distribution)\n  \n  cat(\"\\nDamage by Speed Range:\\n\")\n  print(validation_results$distributions$damage_by_speed)\n}\n\n# run and print validation summary\nprint_validation_summary(validation_summary)\n\n# create data quality flags based on validation results\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    data_quality_flags = case_when(\n      # temporal flags\n      crash_date/time &gt; Sys.time() ~ \"FUTURE_DATE\",\n      \n      # geographic flags\n      (latitude &lt; 38 | latitude &gt; 40 | longitude &gt; -76 | longitude &lt; -78) ~ \"INVALID_COORDINATES\",\n      \n      # vehicle flags\n      (vehicle_year &lt; 1900 | vehicle_year &gt; year(Sys.Date()) + 1) ~ \"INVALID_VEHICLE_YEAR\",\n      \n      # logical consistency flags\n      (parked_vehicle == \"Yes\" & vehicle_movement != \"PARKED\") ~ \"PARKED_MOVING_MISMATCH\",\n      (injury_severity != \"NO_INJURY\" & vehicle_damage_extent == \"NO_DAMAGE\") ~ \"INJURY_NO_DAMAGE_MISMATCH\",\n      (injury_severity == \"FATAL\" & vehicle_damage_extent %in% c(\"NO_DAMAGE\", \"MINOR\")) ~ \"FATAL_MINOR_DAMAGE_MISMATCH\",\n      \n      # cross-reference flags\n      (format(crash_date/time, \"%H:%M:%S\") &gt;= \"18:00:00\" | \n       format(crash_date/time, \"%H:%M:%S\") &lt;= \"06:00:00\") & \n      light == \"DAYLIGHT\" ~ \"LIGHT_TIME_MISMATCH\",\n      \n      (weather %in% c(\"RAIN\", \"SNOW\") & surface_condition == \"DRY\") |\n      (weather == \"CLEAR\" & surface_condition %in% c(\"ICE\", \"SNOW\")) ~ \"WEATHER_SURFACE_MISMATCH\",\n      \n      TRUE ~ \"VALID\"\n    )\n  )\n\nquality_flag_summary &lt;- drivers_data_clean %&gt;%\n  group_by(data_quality_flags) %&gt;%\n  summarise(\n    count = n(),\n    percentage = n() / nrow(drivers_data_clean) * 100\n  ) %&gt;%\n  arrange(desc(count))\n\nprint(\"\\nData Quality Flag Summary:\")\nprint(quality_flag_summary)\n\nSomething still seems off with the data quality flags.\n\n# fix Light-Time Mismatches (largest issue ~5.2% of data)\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # extract hour from crash_date/time\n    crash_hour = as.numeric(format(crash_date/time, \"%H\")),\n    \n    # correct light condition based on time\n    light = case_when(\n      # dawn hours (5-7 AM)\n      crash_hour &gt;= 5 & crash_hour &lt; 7 ~ \"DAWN\",\n      \n      # daylight hours (7 AM - 6 PM)\n      crash_hour &gt;= 7 & crash_hour &lt; 18 ~ \"DAYLIGHT\",\n      \n      # dusk hours (6-8 PM)\n      crash_hour &gt;= 18 & crash_hour &lt; 20 ~ \"DUSK\",\n      \n      # night hours\n      crash_hour &gt;= 20 | crash_hour &lt; 5 ~ \"DARK_WITH_LIGHTING\",\n      \n      TRUE ~ light\n    )\n  ) %&gt;%\n  select(-crash_hour) # remove temporary column\n\n# fix Weather-Surface Condition Mismatches (~0.37% of data)\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # Adjust surface condition based on weather\n    surface_condition = case_when(\n      weather %in% c(\"RAIN\", \"RAINING\") & surface_condition == \"DRY\" ~ \"WET\",\n      weather == \"SNOW\" & surface_condition == \"DRY\" ~ \"SNOW\",\n      weather == \"CLEAR\" & surface_condition %in% c(\"ICE\", \"SNOW\") & \n        format(crash_date/time, \"%m\") %in% c(\"12\", \"01\", \"02\") ~ surface_condition, # Keep if winter months\n      weather == \"CLEAR\" & surface_condition %in% c(\"ICE\", \"SNOW\") ~ \"UNKNOWN\", # Change to unknown if non-winter\n      TRUE ~ surface_condition\n    )\n  )\n\n# fix Injury-Damage Inconsistencies\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # adjust injury severity or damage extent for logical consistency\n    injury_severity = case_when(\n      injury_severity != \"NO_INJURY\" & vehicle_damage_extent == \"NO_DAMAGE\" ~ \"NO_INJURY\",\n      injury_severity == \"FATAL\" & vehicle_damage_extent %in% c(\"NO_DAMAGE\", \"MINOR\") ~ \"SERIOUS_INJURY\",\n      TRUE ~ injury_severity\n    ),\n    \n    # ensure fatal crashes have appropriate damage extent\n    vehicle_damage_extent = case_when(\n      injury_severity == \"FATAL\" & vehicle_damage_extent %in% c(\"NO_DAMAGE\", \"MINOR\") ~ \"DISABLING\",\n      TRUE ~ vehicle_damage_extent\n    )\n  )\n\n# fix Invalid Coordinates (Montgomery County, MD boundaries)\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # montgomery County approximate boundaries\n    latitude = case_when(\n      latitude &lt; 38.9 | latitude &gt; 39.4 ~ NA_real_,\n      TRUE ~ latitude\n    ),\n    longitude = case_when(\n      longitude &lt; -77.5 | longitude &gt; -76.9 ~ NA_real_,\n      TRUE ~ longitude\n    )\n  )\n\n# clean Vehicle Makes and Models with High Frequency Patterns\n# analyze the most common patterns\nvehicle_patterns &lt;- drivers_data_clean %&gt;%\n  group_by(vehicle_make, vehicle_model) %&gt;%\n  summarise(\n    count = n(),\n    .groups = 'drop'\n  ) %&gt;%\n  arrange(desc(count)) %&gt;%\n  head(20)  # Look at top 20 patterns\n\n# now clean based on common patterns\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # standardize common model names\n    vehicle_model = case_when(\n      # remove digits from end of model names\n      str_detect(vehicle_model, \"[A-Z]+\\\\d+$\") ~ str_extract(vehicle_model, \"[A-Z]+\"),\n      # remove common suffixes\n      str_detect(vehicle_model, \"SDN|CPE|CVT|HBK\") ~ str_replace(vehicle_model, \"SDN|CPE|CVT|HBK\", \"\"),\n      TRUE ~ vehicle_model\n    ),\n    \n    # trim whitespace and standardize case\n    vehicle_model = str_trim(vehicle_model),\n    vehicle_make = str_trim(vehicle_make)\n  )\n\n# add Data Quality Score\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    data_quality_score = case_when(\n      data_quality_flags == \"VALID\" ~ 100,\n      data_quality_flags == \"LIGHT_TIME_MISMATCH\" ~ 90,\n      data_quality_flags == \"WEATHER_SURFACE_MISMATCH\" ~ 85,\n      data_quality_flags == \"INJURY_NO_DAMAGE_MISMATCH\" ~ 80,\n      data_quality_flags == \"INVALID_COORDINATES\" ~ 75,\n      data_quality_flags %in% c(\"FATAL_MINOR_DAMAGE_MISMATCH\", \"PARKED_MOVING_MISMATCH\") ~ 70,\n      TRUE ~ 60\n    )\n  )\n\n# verify cleaning results\ncleaning_verification &lt;- list()\n\n# check light-time consistency after cleaning\ncleaning_verification$light_time &lt;- drivers_data_clean %&gt;%\n  summarise(\n    light_time_mismatch_after = sum(\n      (format(crash_date/time, \"%H:%M:%S\") &gt;= \"18:00:00\" | \n       format(crash_date/time, \"%H:%M:%S\") &lt;= \"06:00:00\") & \n      light == \"DAYLIGHT\", \n      na.rm = TRUE\n    )\n  )\n\n# check weather-surface consistency after cleaning\ncleaning_verification$weather_surface &lt;- drivers_data_clean %&gt;%\n  summarise(\n    weather_surface_mismatch_after = sum(\n      (weather %in% c(\"RAIN\", \"SNOW\") & surface_condition == \"DRY\") |\n      (weather == \"CLEAR\" & surface_condition %in% c(\"ICE\", \"SNOW\")), \n      na.rm = TRUE\n    )\n  )\n\n# check data quality scores distribution\ncleaning_verification$quality_scores &lt;- drivers_data_clean %&gt;%\n  group_by(data_quality_score) %&gt;%\n  summarise(\n    count = n(),\n    percentage = n() / nrow(drivers_data_clean) * 100,\n    .groups = 'drop'\n  ) %&gt;%\n  arrange(desc(data_quality_score))\n\ncat(\"\\n=== CLEANING VERIFICATION RESULTS ===\\n\")\nprint(cleaning_verification)\n\nFinal Cleaning Steps Based on Verification Results\n\n# handle remaining weather-surface mismatches (577 cases)\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # create a more detailed weather-surface relationship\n    surface_condition = case_when(\n      # if it's raining, surface should be wet\n      weather %in% c(\"RAIN\", \"RAINING\") ~ \"WET\",\n      \n      # if it's snowing, surface should be snow or ice\n      weather == \"SNOW\" ~ \"SNOW\",\n      \n      # if it's freezing rain, surface should be ice\n      weather == \"FREEZING RAIN\" ~ \"ICE\",\n      \n      # for clear weather, keep existing surface condition if reasonable\n      weather == \"CLEAR\" & surface_condition %in% c(\"DRY\", \"WET\") ~ surface_condition,\n      \n      # for cloudy weather, keep existing surface condition\n      weather == \"CLOUDY\" ~ surface_condition,\n      \n      # for other cases, mark as unknown if inconsistent\n      TRUE ~ case_when(\n        surface_condition %in% c(\"DRY\", \"WET\", \"SNOW\", \"ICE\") ~ surface_condition,\n        TRUE ~ \"UNKNOWN\"\n      )\n    )\n  )\n\n# add more detailed quality flags and improve quality score\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # create compound quality flags for multiple issues\n    detailed_quality_flags = map2_chr(\n      data_quality_flags,\n      surface_condition,\n      ~case_when(\n        .x != \"VALID\" & .y == \"UNKNOWN\" ~ paste(.x, \"WITH_UNKNOWN_SURFACE\"),\n        TRUE ~ .x\n      )\n    ),\n    \n    # create a more nuanced quality score (0-100)\n    refined_quality_score = case_when(\n      data_quality_flags == \"VALID\" & !is.na(surface_condition) & \n      surface_condition != \"UNKNOWN\" ~ 100,\n      \n      data_quality_flags == \"VALID\" & \n      (is.na(surface_condition) | surface_condition == \"UNKNOWN\") ~ 95,\n      \n      data_quality_flags == \"LIGHT_TIME_MISMATCH\" ~ 90,\n      \n      data_quality_flags == \"WEATHER_SURFACE_MISMATCH\" & \n      !is.na(surface_condition) ~ 85,\n      \n      data_quality_flags == \"WEATHER_SURFACE_MISMATCH\" & \n      is.na(surface_condition) ~ 80,\n      \n      data_quality_flags == \"INJURY_NO_DAMAGE_MISMATCH\" ~ 75,\n      \n      data_quality_flags == \"INVALID_COORDINATES\" ~ 70,\n      \n      data_quality_flags %in% c(\"FATAL_MINOR_DAMAGE_MISMATCH\", \n                               \"PARKED_MOVING_MISMATCH\") ~ 65,\n      \n      TRUE ~ 60\n    )\n  )\n\n# create final data quality summary\nfinal_quality_summary &lt;- drivers_data_clean %&gt;%\n  summarise(\n    total_records = n(),\n    high_quality_records = sum(refined_quality_score &gt;= 95),\n    medium_quality_records = sum(refined_quality_score &gt;= 80 & refined_quality_score &lt; 95),\n    low_quality_records = sum(refined_quality_score &lt; 80),\n    mean_quality_score = mean(refined_quality_score),\n    median_quality_score = median(refined_quality_score),\n    \n    # percentage calculations\n    high_quality_percentage = (high_quality_records / total_records) * 100,\n    medium_quality_percentage = (medium_quality_records / total_records) * 100,\n    low_quality_percentage = (low_quality_records / total_records) * 100\n  )\n\ncat(\"\\n=== FINAL DATA QUALITY SUMMARY ===\\n\")\nprint(final_quality_summary)\n\n# create recommendations for further improvements\ncat(\"\\n=== RECOMMENDATIONS FOR FURTHER IMPROVEMENTS ===\\n\")\ncat(\"1. Weather-Surface Condition Relationship:\\n\")\ncat(\"   - \", sum(drivers_data_clean$surface_condition == \"UNKNOWN\"), \n    \"records still have unknown surface conditions\\n\")\ncat(\"   - Consider adding temperature data for better ice/snow validation\\n\\n\")\n\ncat(\"2. Geographic Data Quality:\\n\")\ncat(\"   - \", sum(is.na(drivers_data_clean$latitude) | is.na(drivers_data_clean$longitude)),\n    \"records have missing or invalid coordinates\\n\")\ncat(\"   - Consider implementing address geocoding for missing coordinates\\n\\n\")\n\ncat(\"3. Vehicle Information:\\n\")\ncat(\"   - \", sum(is.na(drivers_data_clean$vehicle_make)), \"records with missing vehicle makes\\n\")\ncat(\"   - \", sum(is.na(drivers_data_clean$vehicle_model)), \"records with missing vehicle models\\n\")\ncat(\"   - Consider implementing VIN decoding for missing vehicle information\\n\\n\")\n\nprocessing_summary &lt;- list(\n  initial_issues = list(\n    light_time_mismatch = 9744,\n    weather_surface_mismatch = 702,\n    injury_damage_mismatch = 306,\n    invalid_coordinates = 26,\n    fatal_minor_damage = 1,\n    parked_moving = 1\n  ),\n  \n  final_status = list(\n    quality_summary = final_quality_summary,\n    remaining_weather_surface_mismatch = 577,\n    remaining_unknown_surface = sum(drivers_data_clean$surface_condition == \"UNKNOWN\", na.rm = TRUE)\n  )\n)\n\ncat(\"\\n=== DATA PROCESSING SUMMARY ===\\n\")\ncat(\"Initial Issues vs. Final Status:\\n\")\ncat(\"- Light-Time Mismatches: \", processing_summary$initial_issues$light_time_mismatch, \n    \" -&gt; 0\\n\")\ncat(\"- Weather-Surface Mismatches: \", processing_summary$initial_issues$weather_surface_mismatch,\n    \" -&gt; \", processing_summary$final_status$remaining_weather_surface_mismatch, \"\\n\")\n\nVisualizations.\n\ninstall.packages(\"viridis\")\n\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(patchwork)  # for combining plots\nlibrary(viridis)    # for colorblind-friendly palettes\nlibrary(lubridate)\n\n# crash Time Patterns\ntime_plot &lt;- drivers_data_clean %&gt;%\n  mutate(\n    hour = hour(crash_date/time),\n    weekday = wday(crash_date/time, label = TRUE),\n    month = month(crash_date/time, label = TRUE)\n  ) %&gt;%\n  ggplot(aes(x = hour)) +\n  geom_histogram(binwidth = 1, fill = \"steelblue\", color = \"white\") +\n  labs(title = \"Distribution of Crashes by Hour of Day\",\n       x = \"Hour of Day\",\n       y = \"Number of Crashes\") +\n  theme_minimal() +\n  scale_x_continuous(breaks = seq(0, 23, 2))\n\n# injury Severity by Vehicle Type\nseverity_vehicle_plot &lt;- drivers_data_clean %&gt;%\n  filter(!is.na(vehicle_body_type), !is.na(injury_severity)) %&gt;%\n  count(vehicle_body_type, injury_severity) %&gt;%\n  group_by(vehicle_body_type) %&gt;%\n  mutate(pct = n/sum(n) * 100) %&gt;%\n  ggplot(aes(x = reorder(vehicle_body_type, -pct), y = pct, fill = injury_severity)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(title = \"Injury Severity by Vehicle Type\",\n       x = \"Vehicle Type\",\n       y = \"Percentage\",\n       fill = \"Injury Severity\") +\n  theme_minimal() +\n  scale_fill_viridis_d()\n\n# weather Conditions and Crash Frequency\nweather_plot &lt;- drivers_data_clean %&gt;%\n  count(weather) %&gt;%\n  ggplot(aes(x = reorder(weather, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"darkgreen\") +\n  coord_flip() +\n  labs(title = \"Crashes by Weather Condition\",\n       x = \"Weather Condition\",\n       y = \"Number of Crashes\") +\n  theme_minimal()\n\n# geographic Distribution of Crashes\nmap_plot &lt;- drivers_data_clean %&gt;%\n  filter(!is.na(latitude), !is.na(longitude)) %&gt;%\n  ggplot(aes(x = longitude, y = latitude)) +\n  geom_point(alpha = 0.1, color = \"red\") +\n  labs(title = \"Geographic Distribution of Crashes\",\n       x = \"Longitude\",\n       y = \"Latitude\") +\n  theme_minimal()\n\n# collision Types\ncollision_plot &lt;- drivers_data_clean %&gt;%\n  count(collision_type) %&gt;%\n  mutate(pct = n/sum(n) * 100) %&gt;%\n  ggplot(aes(x = reorder(collision_type, -pct), y = pct)) +\n  geom_bar(stat = \"identity\", fill = \"orange\") +\n  coord_flip() +\n  labs(title = \"Distribution of Collision Types\",\n       x = \"Collision Type\",\n       y = \"Percentage\") +\n  theme_minimal()\n\n# speed Limit and Crash Severity\nspeed_severity_plot &lt;- drivers_data_clean %&gt;%\n  filter(!is.na(speed_limit), !is.na(injury_severity)) %&gt;%\n  ggplot(aes(x = factor(speed_limit), fill = injury_severity)) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"Injury Severity by Speed Limit\",\n       x = \"Speed Limit\",\n       y = \"Proportion\",\n       fill = \"Injury Severity\") +\n  theme_minimal() +\n  scale_fill_viridis_d()\n\n# vehicle Age Distribution\nvehicle_age_plot &lt;- drivers_data_clean %&gt;%\n  mutate(vehicle_age = year(crash_date/time) - vehicle_year) %&gt;%\n  filter(vehicle_age &gt;= 0, vehicle_age &lt;= 30) %&gt;%\n  ggplot(aes(x = vehicle_age)) +\n  geom_histogram(binwidth = 1, fill = \"purple\", color = \"white\") +\n  labs(title = \"Distribution of Vehicle Age\",\n       x = \"Vehicle Age (Years)\",\n       y = \"Count\") +\n  theme_minimal()\n\n# surface Condition and Weather Relationship\nsurface_weather_plot &lt;- drivers_data_clean %&gt;%\n  filter(!is.na(surface_condition), !is.na(weather)) %&gt;%\n  count(surface_condition, weather) %&gt;%\n  group_by(weather) %&gt;%\n  mutate(pct = n/sum(n) * 100) %&gt;%\n  ggplot(aes(x = weather, y = surface_condition, fill = pct)) +\n  geom_tile() +\n  scale_fill_viridis_c() +\n  labs(title = \"Surface Condition by Weather\",\n       x = \"Weather\",\n       y = \"Surface Condition\",\n       fill = \"Percentage\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n# combine plots using patchwork\nlayout &lt;- \"\nAABB\nCCDD\nEEFF\nGGHH\n\"\n\ncombined_plots &lt;- time_plot + severity_vehicle_plot + \n                 weather_plot + map_plot + \n                 collision_plot + speed_severity_plot + \n                 vehicle_age_plot + surface_weather_plot +\n                 plot_layout(design = layout)\n\nprint(combined_plots)\n\nggsave(\"crash_analysis_dashboard.pdf\", combined_plots, width = 20, height = 24)\n\nsummary_stats &lt;- list(\n  \n  time_stats = drivers_data_clean %&gt;%\n    mutate(\n      hour = hour(crash_date/time),\n      weekday = wday(crash_date/time, label = TRUE)\n    ) %&gt;%\n    summarise(\n      peak_hour = names(which.max(table(hour))),\n      weekend_crashes = mean(weekday %in% c(\"Sat\", \"Sun\")) * 100\n    ),\n  \n  severity_stats = drivers_data_clean %&gt;%\n    group_by(injury_severity) %&gt;%\n    summarise(\n      count = n(),\n      percentage = n() / nrow(drivers_data_clean) * 100\n    ),\n  \n  weather_stats = drivers_data_clean %&gt;%\n    group_by(weather) %&gt;%\n    summarise(\n      count = n(),\n      percentage = n() / nrow(drivers_data_clean) * 100\n    ),\n  \n  vehicle_stats = drivers_data_clean %&gt;%\n    summarise(\n      avg_vehicle_age = mean(year(crash_date/time) - vehicle_year, na.rm = TRUE),\n      most_common_make = names(which.max(table(vehicle_make)))\n    )\n)\n\nprint(summary_stats)\n\nSave the final cleaned dataset.\n\ncurrent_timestamp &lt;- format(Sys.time(), \"%Y%m%d_%H%M\")\n\n# create directories if they don't exist\ndir.create(\"cleaned_data\", showWarnings = FALSE)\ndir.create(\"data_documentation\", showWarnings = FALSE)\n\nstr_result &lt;- capture.output(str(drivers_data_clean))\nprint(\"Data structure:\")\nprint(str_result)\n\ncolumn_info &lt;- data.frame(\n  column_name = names(drivers_data_clean),\n  data_type = sapply(drivers_data_clean, function(x) class(x)[1]),  # Take first class if multiple\n  stringsAsFactors = FALSE\n)\n\ncolumn_info$description &lt;- sapply(column_info$column_name, function(col_name) {\n  case_when(\n    col_name == \"report_number\" ~ \"Unique report identifier\",\n    col_name == \"local_case_number\" ~ \"Local case number for the incident\",\n    col_name == \"agency_name\" ~ \"Name of reporting agency\",\n    col_name == \"acrs_report_type\" ~ \"Type of crash report\",\n    col_name == \"crash_date/time\" ~ \"Date and time of the crash\",\n    col_name == \"route_type\" ~ \"Type of route where crash occurred\",\n    col_name == \"road_name\" ~ \"Name of the road where crash occurred\",\n    col_name == \"cross_street_name\" ~ \"Name of the nearest cross-street\",\n    col_name == \"off_road_description\" ~ \"Description for off-road incidents\",\n    col_name == \"municipality\" ~ \"Municipality where crash occurred\",\n    col_name == \"related_non_motorist\" ~ \"Type of non-motorist involved\",\n    col_name == \"collision_type\" ~ \"Type of collision\",\n    col_name == \"weather\" ~ \"Weather conditions during crash\",\n    col_name == \"surface_condition\" ~ \"Road surface condition\",\n    col_name == \"light\" ~ \"Light conditions\",\n    col_name == \"traffic_control\" ~ \"Traffic control present\",\n    col_name == \"driver_substance_abuse\" ~ \"Driver substance abuse status\",\n    col_name == \"non_motorist_substance_abuse\" ~ \"Non-motorist substance abuse status\",\n    col_name == \"person_id\" ~ \"Unique identifier for person involved\",\n    col_name == \"driver_at_fault\" ~ \"Indicator if driver was at fault\",\n    col_name == \"injury_severity\" ~ \"Severity of injuries\",\n    col_name == \"circumstance\" ~ \"Contributing circumstances\",\n    col_name == \"driver_distracted_by\" ~ \"Driver distraction factors\",\n    col_name == \"drivers_license_state\" ~ \"State of driver's license\",\n    col_name == \"vehicle_id\" ~ \"Unique identifier for vehicle\",\n    col_name == \"vehicle_damage_extent\" ~ \"Extent of vehicle damage\",\n    col_name == \"vehicle_first_impact_location\" ~ \"Location of first impact on vehicle\",\n    col_name == \"vehicle_body_type\" ~ \"Type of vehicle body\",\n    col_name == \"vehicle_movement\" ~ \"Vehicle movement during crash\",\n    col_name == \"vehicle_going_dir\" ~ \"Direction vehicle was traveling\",\n    col_name == \"speed_limit\" ~ \"Posted speed limit\",\n    col_name == \"driverless_vehicle\" ~ \"Indicator if vehicle was driverless\",\n    col_name == \"parked_vehicle\" ~ \"Indicator if vehicle was parked\",\n    col_name == \"vehicle_year\" ~ \"Year of vehicle manufacture\",\n    col_name == \"vehicle_make\" ~ \"Vehicle manufacturer\",\n    col_name == \"vehicle_model\" ~ \"Vehicle model\",\n    col_name == \"latitude\" ~ \"Latitude of crash location\",\n    col_name == \"longitude\" ~ \"Longitude of crash location\",\n    col_name == \"location\" ~ \"Combined location coordinates\",\n    col_name == \"data_quality_flags\" ~ \"Data quality flags from cleaning process\",\n    col_name == \"detailed_quality_flags\" ~ \"Detailed quality flags from cleaning process\",\n    col_name == \"refined_quality_score\" ~ \"Numerical score indicating data quality\",\n    TRUE ~ paste(\"Description for\", col_name)  # Default description for any new columns\n  )\n})\n\ncolumn_info$example_values &lt;- sapply(drivers_data_clean, function(x) {\n  if (is.numeric(x)) {\n    paste(\"Range:\", min(x, na.rm = TRUE), \"to\", max(x, na.rm = TRUE))\n  } else {\n    unique_vals &lt;- unique(na.omit(x))\n    if (length(unique_vals) &gt; 5) {\n      paste(paste(unique_vals[1:5], collapse = \", \"), \"... and\", length(unique_vals) - 5, \"more values\")\n    } else {\n      paste(unique_vals, collapse = \", \")\n    }\n  }\n})\n\nwrite_csv(drivers_data_clean, \n          file = paste0(\"cleaned_data/crash_data_clean_\", current_timestamp, \".csv\"))\n\nsaveRDS(drivers_data_clean, \n        file = paste0(\"cleaned_data/crash_data_clean_\", current_timestamp, \".rds\"))\n\nwrite_csv(column_info, \n          file = paste0(\"data_documentation/data_dictionary_\", current_timestamp, \".csv\"))\n\nreadme_text &lt;- sprintf(\"\n# Crash Data Cleaning Documentation\n\n## Version Information\n- Date Created: %s\n- Original Records: 186,931\n- Final Records: %d\n\n## Data Quality Metrics\n- Total Records: %d\n- Records with Quality Flags: %d\n- Clean Records: %d\n\n## Files in this Package\n1. crash_data_clean_%s.csv - Main data file (CSV format)\n2. crash_data_clean_%s.rds - R data file (RDS format)\n3. data_dictionary_%s.csv - Data dictionary with column descriptions\n\n## Column Summary\nTotal number of columns: %d\nSee data dictionary file for detailed information about each column.\n\n## Data Quality Notes\n- Data has been cleaned and standardized\n- Quality flags have been added to mark potential issues\n- Missing values have been handled according to context\n- Inconsistent categories have been standardized\n\n## Usage Notes\n- Please refer to the data dictionary for column descriptions\n- Check quality flags before analysis\n- Some columns may contain standardized values\n\n## Contact\nFor questions about this dataset, please contact [Your Contact Information]\n\n## Last Updated\n%s\n\",\n    format(Sys.time(), \"%Y-%m-%d %H:%M:%S\"),\n    nrow(drivers_data_clean),\n    nrow(drivers_data_clean),\n    sum(drivers_data_clean$data_quality_flags != \"VALID\"),\n    sum(drivers_data_clean$data_quality_flags == \"VALID\"),\n    current_timestamp,\n    current_timestamp,\n    current_timestamp,\n    ncol(drivers_data_clean),\n    format(Sys.time(), \"%Y-%m-%d %H:%M:%S\")\n)\n\nwriteLines(readme_text, \n           paste0(\"data_documentation/README_\", current_timestamp, \".md\"))\n\nzip_files &lt;- c(\n    paste0(\"cleaned_data/crash_data_clean_\", current_timestamp, \".csv\"),\n    paste0(\"cleaned_data/crash_data_clean_\", current_timestamp, \".rds\"),\n    paste0(\"data_documentation/data_dictionary_\", current_timestamp, \".csv\"),\n    paste0(\"data_documentation/README_\", current_timestamp, \".md\")\n)\n\nzip(paste0(\"cleaned_data/crash_data_package_\", current_timestamp, \".zip\"),\n    files = zip_files)\n\ncat(\"\\nData saving complete!\\n\")\ncat(\"Files saved:\\n\")\ncat(\"1. CSV data file\\n\")\ncat(\"2. RDS data file\\n\")\ncat(\"3. Data dictionary\\n\")\ncat(\"4. README documentation\\n\")\ncat(\"5. Complete package (zip)\\n\")\ncat(\"\\nLocation: ./cleaned_data/ and ./data_documentation/\\n\")"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This comes from the file about.qmd.\nThis is a website for the final project for MA[46]15 Data Science with R by Team TEAMNAME. The members of this team are below."
  },
  {
    "objectID": "about.html#yifei-zhang",
    "href": "about.html#yifei-zhang",
    "title": "About",
    "section": "Yifei Zhang",
    "text": "Yifei Zhang\nMajor: Economics & Math\nFrom: China"
  },
  {
    "objectID": "about.html#primah-muwanga",
    "href": "about.html#primah-muwanga",
    "title": "About",
    "section": "Primah Muwanga",
    "text": "Primah Muwanga\nMajor: Data Science\nMinor: Engineering Science\nFrom: Uganda"
  },
  {
    "objectID": "about.html#xiang-fu",
    "href": "about.html#xiang-fu",
    "title": "About",
    "section": "Xiang Fu",
    "text": "Xiang Fu\nMajor: Data Science\nMinor: Statistics\nFrom: China"
  },
  {
    "objectID": "about.html#nora-oneill",
    "href": "about.html#nora-oneill",
    "title": "About",
    "section": "Nora O’Neill",
    "text": "Nora O’Neill\nMajor: Math(Concentration in Statistics)\nMinor: Business\nFrom: Newton, Massachusetts"
  },
  {
    "objectID": "about.html#tejas-kaur",
    "href": "about.html#tejas-kaur",
    "title": "About",
    "section": "Tejas Kaur",
    "text": "Tejas Kaur\nMajor: Economics and Math\nMinor: Data Science\nFrom: Jaipur, India\n\n\nAbout this Template.\nThis is based off of the standard Quarto website template from RStudio (2023.09.0 Build 463)."
  },
  {
    "objectID": "big_picture.html",
    "href": "big_picture.html",
    "title": "Big Picture",
    "section": "",
    "text": "This comes from the file big_picture.Rmd.\nThink of this page as your 538/Upshot style article. This means that you should try to tell a story through the data and your analysis. Read articles from those sites and similar sites to get a feeling for what they are like. Try to write in the style of a news or popular article. Importantly, this page should be geared towards the general public. You shouldn’t assume the reader understands how to interpret a linear regression or a complicated plot. Focus on interpretation and visualizations."
  },
  {
    "objectID": "big_picture.html#rubric-on-this-page",
    "href": "big_picture.html#rubric-on-this-page",
    "title": "Big Picture",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nTitle\n\nYour big picture page should have a creative/click-bait-y title/headline that provides a hint about your thesis.\n\nClarity of Explanation\n\nYou should have a clear thesis/goal for this page. What are you trying to show? Make sure that you explain your analysis in detail but don’t go into top much mathematics or statistics. The audience for this page is the general public (to the extent possible). Your thesis should be a statement, not a question.\nEach figure should be very polished and also not too complicated. There should be a clear interpretation of the figure so the figure has a clear purpose. Even something like a histogram can be difficult to interpret for non-experts.\n\nCreativity\n\nDo your best to make things interesting. Think of a story. Think of how each part of your analysis supports the previous part or provides a different perspective.\n\nInteractive component\n\nQuality and ease of use of the interactive components. Is it clear what can be explored using your interactive components? Does it enhance and reinforce your conclusions?\n\nThis page should be self-contained.\n\nNote: This page should have no code visible, i.e. use #| echo: FALSE."
  },
  {
    "objectID": "big_picture.html#rubric-other-components",
    "href": "big_picture.html#rubric-other-components",
    "title": "Big Picture",
    "section": "Rubric: Other components",
    "text": "Rubric: Other components\n\nVideo Recording\nMake a video recording (probably using Zoom) demonstrating your interactive components. You should provide a quick explanation of your data and demonstrate some of the conclusions from your EDA. This video should be no longer than 4 minutes. Include a link to your video (and password if needed) in your README.md file on your Github repository. You are not required to provide a link on the website. This can be presented by any subset of the team members.\n\n\nRest of the Site\nFinally, here are important things to keep in mind for the rest of the site.\nThe main title of your page is informative. Each post has an author/description/informative title. All lab required posts are present. Each page (including the home page) has a nice featured image associated with it. Your about page is up to date and clean. You have removed the generic posts from the initial site template."
  }
]