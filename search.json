[
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "The primary dataset used in this analysis comes from Montgomery County, Maryland’s open data portal, specifically the “Crash Reporting - Drivers Data” dataset. This dataset is publicly available and can be accessed at Montgomery County’s Data Portal. The data is collected through the Automated Crash Reporting System (ACRS) of the Maryland State Police and includes reports from multiple law enforcement agencies: Montgomery County Police, Gaithersburg Police, Rockville Police, and the Maryland-National Capital Park Police.\nThe dataset was created and is maintained as part of Montgomery County’s commitment to public safety and transparency, particularly in support of their Vision Zero initiative (as indicated by the dataset’s keywords). Vision Zero is a strategy to eliminate all traffic fatalities and severe injuries while increasing safe, healthy, equitable mobility for all. The data collection serves multiple purposes: - To track and analyze traffic collision patterns across the county - To inform policy decisions regarding traffic safety measures - To provide transparency about traffic incidents to the public - To support the coordination between different law enforcement agencies in the county\nThe dataset is updated weekly and contains detailed information about traffic collisions occurring on county and local roadways. Each record represents a driver involved in a collision, with 39 variables capturing various aspects of the incident, including:\nKey Variables: - Temporal and Spatial Information: Crash Date/Time, Location (Latitude/Longitude), Road Name - Environmental Conditions: Weather, Surface Condition, Light, Traffic Control - Driver Details: Driver At Fault, Injury Severity, Driver Substance Abuse, Driver Distracted By - Vehicle Information: Vehicle Make, Model, Year, Body Type, Damage Extent"
  },
  {
    "objectID": "data.html#primary-data-source",
    "href": "data.html#primary-data-source",
    "title": "Data",
    "section": "",
    "text": "The primary dataset used in this analysis comes from Montgomery County, Maryland’s open data portal, specifically the “Crash Reporting - Drivers Data” dataset. This dataset is publicly available and can be accessed at Montgomery County’s Data Portal. The data is collected through the Automated Crash Reporting System (ACRS) of the Maryland State Police and includes reports from multiple law enforcement agencies: Montgomery County Police, Gaithersburg Police, Rockville Police, and the Maryland-National Capital Park Police.\nThe dataset was created and is maintained as part of Montgomery County’s commitment to public safety and transparency, particularly in support of their Vision Zero initiative (as indicated by the dataset’s keywords). Vision Zero is a strategy to eliminate all traffic fatalities and severe injuries while increasing safe, healthy, equitable mobility for all. The data collection serves multiple purposes: - To track and analyze traffic collision patterns across the county - To inform policy decisions regarding traffic safety measures - To provide transparency about traffic incidents to the public - To support the coordination between different law enforcement agencies in the county\nThe dataset is updated weekly and contains detailed information about traffic collisions occurring on county and local roadways. Each record represents a driver involved in a collision, with 39 variables capturing various aspects of the incident, including:\nKey Variables: - Temporal and Spatial Information: Crash Date/Time, Location (Latitude/Longitude), Road Name - Environmental Conditions: Weather, Surface Condition, Light, Traffic Control - Driver Details: Driver At Fault, Injury Severity, Driver Substance Abuse, Driver Distracted By - Vehicle Information: Vehicle Make, Model, Year, Body Type, Damage Extent"
  },
  {
    "objectID": "data.html#data-description-and-variables",
    "href": "data.html#data-description-and-variables",
    "title": "Data",
    "section": "Data Description and Variables",
    "text": "Data Description and Variables"
  },
  {
    "objectID": "data.html#data-cleaning-and-processing",
    "href": "data.html#data-cleaning-and-processing",
    "title": "Data",
    "section": "Data Cleaning and Processing",
    "text": "Data Cleaning and Processing\n\nData Loading\n\n\nVariable Cleaning\n\n\nAdditional Packages\n\n\nData Integration\n\nThis comes from the file data.qmd.\nYour first steps in this project will be to find data to work on.\nInitially, you will study one dataset but later you will need to combine that data with another dataset. For this reason, I recommend finding data that has some date and/or location components. These types of data are conducive to interesting visualizations and analysis and you can also combine this data with other data that also has a date or location variable. Data from the census, weather data, economic data, are all relatively easy to combine with other data with time/location components."
  },
  {
    "objectID": "data.html#what-makes-a-good-data-set",
    "href": "data.html#what-makes-a-good-data-set",
    "title": "Data",
    "section": "What makes a good data set?",
    "text": "What makes a good data set?\n\nData you are interested in and care about.\nData where there are a lot of potential questions that you can explore.\nA data set that isn’t completely cleaned already.\nMultiple sources for data that you can combine.\nSome type of time and/or location component."
  },
  {
    "objectID": "data.html#where-to-keep-data",
    "href": "data.html#where-to-keep-data",
    "title": "Data",
    "section": "Where to keep data?",
    "text": "Where to keep data?\nBelow 50mb: In dataset folder\nAbove 50mb: In dataset_ignore folder. This folder will be ignored by git so you’ll have to manually sync these files across your team.\n\nSharing your data\nFor small datasets (&lt;50mb), you can use the dataset folder that is tracked by github. Add the files just like you would any other file.\nIf you create a folder named data this will cause problems.\nFor larger datasets, you’ll need to create a new folder in the project root directory named dataset-ignore. This will be ignored by git (based off the .gitignore file in the project root directory) which will help you avoid issues with Github’s size limits. Your team will have to manually make sure the data files in dataset-ignore are synced across team members.\nYour load_and_clean_data.R file is how you will load and clean your data. Here is a an example of a very simple one.\n\nsource(\n  \"scripts/load_and_clean_data.R\",\n  echo = TRUE # Use echo=FALSE or omit it to avoid code output  \n)\n\n\n&gt; library(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n&gt; loan_data &lt;- read_csv(here::here(\"dataset\", \"loan_refusal.csv\"))\n\n\nRows: 20 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): bank\ndbl (4): min, white, himin, hiwhite\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n&gt; loan_data_clean &lt;- pivot_longer(loan_data, 2:5, names_to = \"group\", \n+     values_to = \"refusal_rate\")\n\n&gt; write_rds(loan_data_clean, file = here::here(\"dataset\", \n+     \"loan_refusal_clean.rds\"))\n\n\nYou should never use absolute paths (eg. /Users/danielsussman/path/to/project/ or C:\\MA415\\\\Final_Project\\).\nYou might consider using the here function from the here package to avoid path problems.\n\n\nLoad and clean data script\nThe idea behind this file is that someone coming to your website could largely replicate your analyses after running this script on the original data sets to clean them. This file might create a derivative data set that you then use for your subsequent analysis. Note that you don’t need to run this script from every post/page. Instead, you can load in the results of this script, which could be plain text files or .RData files. In your data page you’ll describe how these results were created. If you have a very large data set, you might save smaller data sets that you can use for exploration purposes. To link to this file, you can use [cleaning script](/scripts/load_and_clean_data.R) wich appears as cleaning script."
  },
  {
    "objectID": "data.html#rubric-on-this-page",
    "href": "data.html#rubric-on-this-page",
    "title": "Data",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nDescribe where/how to find data.\n\nYou must include a link to the original data source(s). Make sure to provide attribution to those who collected the data.\nWhy was the data collected/curated? Who put it together? (This is important, if you don’t know why it was collected then that might not be a good dataset to look at.\n\nDescribe the different data files used and what each variable means.\n\nIf you have many variables then only describe the most relevant ones and summarize the rest.\n\nDescribe any cleaning you had to do for your data.\n\nYou must include a link to your load_and_clean_data.R file.\nRrename variables and recode factors to make data more clear.\nAlso, describe any additional R packages you used outside of those covered in class.\nDescribe and show code for how you combined multiple data files and any cleaning that was necessary for that.\nSome repetition of what you do in your load_and_clean_data.R file is fine and encouraged if it helps explain what you did.\n\nOrganization, clarity, cleanliness of the page\n\nMake sure to remove excessive warnings, use clean easy-to-read code (without side scrolling), organize with sections, use bullets and other organization tools, etc.\nThis page should be self-contained."
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "This comes from the file analysis.qmd.\nWe describe here our detailed data analysis. This page will provide an overview of what questions you addressed, illustrations of relevant aspects of the data with tables and figures, and a statistical model that attempts to answer part of the question. You’ll also reflect on next steps and further analysis.\nThe audience for this page is someone like your class mates, so you can expect that they have some level of statistical and quantitative sophistication and understand ideas like linear and logistic regression, coefficients, confidence intervals, overfitting, etc.\nWhile the exact number of figures and tables will vary and depend on your analysis, you should target around 5 to 6. An overly long analysis could lead to losing points. If you want you can link back to your blog posts or create separate pages with more details.\nThe style of this paper should aim to be that of an academic paper. I don’t expect this to be of publication quality but you should keep that aim in mind. Avoid using “we” too frequently, for example “We also found that …”. Describe your methodology and your findings but don’t describe your whole process."
  },
  {
    "objectID": "analysis.html#note-on-attribution",
    "href": "analysis.html#note-on-attribution",
    "title": "Analysis",
    "section": "Note on Attribution",
    "text": "Note on Attribution\nIn general, you should try to provide links to relevant resources, especially those that helped you. You don’t have to link to every StackOverflow post you used but if there are explainers on aspects of the data or specific models that you found helpful, try to link to those. Also, try to link to other sources that might support (or refute) your analysis. These can just be regular hyperlinks. You don’t need a formal citation.\nIf you are directly quoting from a source, please make that clear. You can show quotes using &gt; like this\n&gt; To be or not to be.\n\nTo be or not to be."
  },
  {
    "objectID": "analysis.html#rubric-on-this-page",
    "href": "analysis.html#rubric-on-this-page",
    "title": "Analysis",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nIntroduce what motivates your Data Analysis (DA)\n\nWhich variables and relationships are you most interested in?\nWhat questions are you interested in answering?\nProvide context for the rest of the page. This will include figures/tables that illustrate aspects of the data of your question.\n\nModeling and Inference\n\nThe page will include some kind of formal statistical model. This could be a linear regression, logistic regression, or another modeling framework.\nExplain the ideas and techniques you used to choose the predictors for your model. (Think about including interaction terms and other transformations of your variables.)\nDescribe the results of your modelling and make sure to give a sense of the uncertainty in your estimates and conclusions.\n\nExplain the flaws and limitations of your analysis\n\nAre there some assumptions that you needed to make that might not hold? Is there other data that would help to answer your questions?\n\nClarity Figures\n\nAre your figures/tables/results easy to read, informative, without problems like overplotting, hard-to-read labels, etc?\nEach figure should provide a key insight. Too many figures or other data summaries can detract from this. (While not a hard limit, around 5 total figures is probably a good target.)\nDefault lm output and plots are typically not acceptable.\n\nClarity of Explanations\n\nHow well do you explain each figure/result?\nDo you provide interpretations that suggest further analysis or explanations for observed phenomenon?\n\nOrganization and cleanliness.\n\nMake sure to remove excessive warnings, hide most or all code, organize with sections or multiple pages, use bullets, etc.\nThis page should be self-contained, i.e. provide a description of the relevant data."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MA [46]15 Final Project",
    "section": "",
    "text": "Final Project due May 7, 2024 at 11:59pm.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEDA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 7\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2024\n\n\nGroup 1\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 6\n\n\n\n\n\n\n\n\n\n\n\nNov 18, 2024\n\n\nGroup 1\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 5: Additional Datasets\n\n\n\n\n\n\n\n\n\n\n\nNov 11, 2024\n\n\nGroup 1\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 3: EDA\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2024\n\n\nGroup 1\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 2/3: Data Clean\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2024\n\n\nGroup 1\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 2/3: Data Equity\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2024\n\n\nGroup 1\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 2: Data Background\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2024\n\n\nGroup 1\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 1: Choosing a Dataset\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2024\n\n\nGroup 1\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n\n\n\n\n\n\nGetting started\n\n\n\n\n\n\n\n\nDirections to set up your website and create your first post. \n\n\n\n\n\nFeb 23, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n\n\n\n\n\n\nFirst Team Meeting\n\n\n\n\n\n\n\n\nThis post details the steps you’ll take for your first team meeting. \n\n\n\n\n\nFeb 21, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "dataset/2017_PoliceXCrash.html",
    "href": "dataset/2017_PoliceXCrash.html",
    "title": "2017_Mileage data with Crash Reports",
    "section": "",
    "text": "library(readxl)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(sf)\nlibrary(leaflet)\n\n\n# Assuming `crash_df` and PoPo are your data frames.\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(lubridate)\n\ncrash_data &lt;- readRDS(\"dataset/cleaned_dataset.rds\")\npopo_data &lt;- readRDS(\"dataset/Police_Reports.rds\")\n\ncol_crash &lt;- colnames(crash_data)\ncol_Popo &lt;- colnames(popo_data)\n\nprint(col_crash)\nprint(col_Popo)\n\n\n# Aggregate data\ncrash_count &lt;- as.data.frame(table(crash_data$Municipality))\ncolnames(crash_count) &lt;- c(\"City\", \"Crash_Count\")\n\nincident_count &lt;- as.data.frame(table(popo_data$City))\ncolnames(incident_count) &lt;- c(\"City\", \"Incident_Count\")\n\n# Merge data by city\nmerged_data &lt;- merge(crash_count, incident_count, by = \"City\", all = TRUE)\nmerged_data[is.na(merged_data)] &lt;- 0  # Replace NA with 0\n\n# Create the bar chart\nlibrary(ggplot2)\nggplot(merged_data, aes(x = City)) +\n  geom_bar(aes(y = Crash_Count, fill = \"Crashes\"), stat = \"identity\", position = \"dodge\") +\n  geom_bar(aes(y = Incident_Count, fill = \"Incidents\"), stat = \"identity\", position = \"dodge\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(title = \"Incident and Crash Counts by City\", x = \"City\", y = \"Count\", fill = \"Type\")\n\n\nlibrary(lubridate)\n\n# Define thresholds\ntime_threshold &lt;- hours(1)\ndistance_threshold &lt;- 500  # in meters\n\n\n# Extract longitude and latitude from the Location column\nlibrary(stringr)\n\npopo_data$Longitude &lt;- as.numeric(str_extract(popo_data$Location, \"-[0-9]+\\\\.[0-9]+\"))\npopo_data$Latitude &lt;- as.numeric(str_extract(popo_data$Location, \"(?&lt;= )[0-9]+\\\\.[0-9]+\"))\n\n# Check the extracted columns\nhead(popo_data)\n\n\n# Create heatmap data\nlibrary(ggplot2)\nlibrary(ggmap)\nlibrary(stringr)\n\n# Combine datasets for visualization\ncrash_data$type &lt;- \"Crash\"\npopo_data$type &lt;- \"Incident\"\n\n# Select only necessary columns\ncrash_locations &lt;- crash_data[, c(\"Latitude\", \"Longitude\", \"type\")]\nlocation_data &lt;- rbind(crash_locations, popo_data[, c(\"Latitude\", \"Longitude\", \"type\")])\n\nggplot(location_data, aes(x = Longitude, y = Latitude)) +\n  stat_density2d(aes(fill = after_stat(level), alpha = ..level..), geom = \"polygon\") +\n  scale_fill_gradient(low = \"blue\", high = \"red\") +\n  facet_wrap(~type) +\n  labs(title = \"Heatmap of Crash and Incident Locations\", x = \"Longitude\", y = \"Latitude\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html",
    "href": "posts/2023-12-20-examples/examples.html",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "href": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2024-11-11-blog-post/blog-05.html",
    "href": "posts/2024-11-11-blog-post/blog-05.html",
    "title": "Blog Post 5: Additional Datasets",
    "section": "",
    "text": "Link: Pavement Condition Index 2019\nThe dataset I am combining is Montgomery County’s Pavement Condition Index (PCI), which assesses the condition of 5,200 lane miles of roadways. PCI uses a scale from 0-100 to represent road conditions, where lower values indicate poorer conditions (e.g., PCI of 30), and higher values indicate better conditions (e.g., PCI of 80).\nI am matching the geographic locations of crashes with the corresponding PCI values for those road segments, allowing analysis of whether areas with poorer road conditions have higher crash rates. I have identified key variables needed to join the two datasets, focusing on road-related variables, such as road name, in the crash dataset.\nThe challenge I encountered is that the road information between the two datasets doesn’t perfectly align. For example, one dataset has broader location descriptions than the other, making the merge challenging. My next steps are to refine the matching variables to improve accuracy and then conduct a more detailed analysis to examine the relationship between crash rates and PCI values."
  },
  {
    "objectID": "posts/2024-11-11-blog-post/blog-05.html#pavement-condition-index",
    "href": "posts/2024-11-11-blog-post/blog-05.html#pavement-condition-index",
    "title": "Blog Post 5: Additional Datasets",
    "section": "",
    "text": "Link: Pavement Condition Index 2019\nThe dataset I am combining is Montgomery County’s Pavement Condition Index (PCI), which assesses the condition of 5,200 lane miles of roadways. PCI uses a scale from 0-100 to represent road conditions, where lower values indicate poorer conditions (e.g., PCI of 30), and higher values indicate better conditions (e.g., PCI of 80).\nI am matching the geographic locations of crashes with the corresponding PCI values for those road segments, allowing analysis of whether areas with poorer road conditions have higher crash rates. I have identified key variables needed to join the two datasets, focusing on road-related variables, such as road name, in the crash dataset.\nThe challenge I encountered is that the road information between the two datasets doesn’t perfectly align. For example, one dataset has broader location descriptions than the other, making the merge challenging. My next steps are to refine the matching variables to improve accuracy and then conduct a more detailed analysis to examine the relationship between crash rates and PCI values."
  },
  {
    "objectID": "posts/2024-11-11-blog-post/blog-05.html#annual-highway-mileage-reports",
    "href": "posts/2024-11-11-blog-post/blog-05.html#annual-highway-mileage-reports",
    "title": "Blog Post 5: Additional Datasets",
    "section": "Annual Highway Mileage Reports",
    "text": "Annual Highway Mileage Reports\nLink: Annual Highway Mileage Reports\nThe primary dataset, Crash Reporting - Drivers Data, provides detailed information about traffic collisions on county and local roadways, including driver behaviors, environmental conditions, and collision outcomes. To enhance our analysis, we’re incorporating the Annual Highway Mileage Reports from the Maryland Department of Transportation State Highway Administration, which offers complementary information about road infrastructure, usage patterns, and road classifications.\nThe Highway Mileage Reports contain valuable metrics such as annual vehicle miles traveled, functional road classifications, and lane mileage data, which can provide context to our crash analysis. By combining these datasets, we can investigate whether certain road types experience higher crash rates, if traffic volume correlates with accident frequency, and how road infrastructure improvements might impact safety outcomes.\nThe process of combining these datasets requires careful consideration of common linking elements. We’re using geographic location as our primary joining key, specifically focusing on Montgomery County data. The crash data provides specific location information through latitude, longitude, and road names, while the highway mileage data is organized by county and road functional classifications. This allows us to analyze crash patterns in relation to road types and traffic volumes.\nHowever, we’ve encountered several challenges in the data integration process. First, the temporal alignment of the datasets requires attention, as we’re working with crash data from 2015-2023 and need to match it with corresponding annual highway reports. Second, the granularity of data differs between sources - crash data is incident-specific, while highway data is aggregated at the county level. We’re addressing this by creating appropriate aggregation levels that maintain analytical value while ensuring meaningful comparisons.\nOur next steps involve developing more sophisticated analysis methods to account for exposure rates (crashes per vehicle mile traveled) and examining how different road classifications correlate with crash severity. We’re also planning to incorporate additional variables such as lane width and surface type from the highway data to build a more comprehensive understanding of infrastructure-related safety factors."
  },
  {
    "objectID": "posts/2024-11-11-blog-post/blog-05.html#ems-responses---nemsis-maryland-state-data-set",
    "href": "posts/2024-11-11-blog-post/blog-05.html#ems-responses---nemsis-maryland-state-data-set",
    "title": "Blog Post 5: Additional Datasets",
    "section": "EMS Responses - NEMSIS Maryland State Data Set",
    "text": "EMS Responses - NEMSIS Maryland State Data Set\nLink: EMS Responses - Maryland State Data Set\nTo enhance our crash analysis, we are integrating the NEMSIS Maryland State Data Set for EMS responses, which includes detailed information on response times, dispatch times, and patient transfers. By aligning this data with the Montgomery County crash dataset, we aim to assess how EMS response times vary by crash severity, location, and road conditions. This integration will allow us to identify patterns in EMS efficiency, highlighting areas where response times may be longer due to factors like poor road quality or high traffic. One challenge in combining these datasets is accurately matching crashes with EMS records, as location and time details differ slightly across sources. Our next steps involve refining these joins to better understand how EMS response correlates with injury severity and crash frequency in specific areas."
  },
  {
    "objectID": "posts/2024-11-11-blog-post/blog-05.html#marylands-traffic-volume-maps",
    "href": "posts/2024-11-11-blog-post/blog-05.html#marylands-traffic-volume-maps",
    "title": "Blog Post 5: Additional Datasets",
    "section": "Maryland’s Traffic Volume Maps",
    "text": "Maryland’s Traffic Volume Maps\nLink: Maryland Traffic Volume Maps\nFor this analysis, we incorporated Maryland’s Traffic Volume Maps data, which provide Annual Average Daily Traffic (AADT) metrics across multiple roadways in Montgomery County and other regions. By integrating this dataset with our crash and road condition data, we aim to examine how traffic volume correlates with crash rates, particularly in areas with varying pavement conditions. This data enables a more nuanced analysis of risk factors, as higher traffic volumes may influence crash frequency and severity. A key challenge in this integration has been aligning traffic volume data with the specific crash locations due to slight mismatches in geographic identifiers between datasets. Our next steps will involve refining the matching process to ensure accurate associations, followed by statistical analyses to identify any significant relationships between traffic volume, road conditions, and crash incidences."
  },
  {
    "objectID": "posts/2024-11-11-blog-post/blog-05.html#crash-reporting-for-montgomery-county",
    "href": "posts/2024-11-11-blog-post/blog-05.html#crash-reporting-for-montgomery-county",
    "title": "Blog Post 5: Additional Datasets",
    "section": "Crash Reporting for Montgomery County",
    "text": "Crash Reporting for Montgomery County\nLink: Crime Dataset\nTo add in tandem with our Crash Reporting for Montgomery County, MD. I found a data set based on crime data in the same county. This data set offers variables such as location, crime committed, dates, and street names. This could offer some additional relationships we can graph with linear regression to see how crime in the county is related to crashes.\nThis data set provides information from other counties as well, which we would need to filter out, additionally, it is not offered in csv format but we can reformat it and fit it into our project. Additionally, with thirty variables/columns, it may take more time to choose the right ones and investigate possible relationships without the main data set.\nThis information comes from an official US website, DATA.GOV, and was shared for the purpose of public access. This dataset is updated daily by Montgomery Polic Department."
  },
  {
    "objectID": "posts/2024-10-28-blog-post/data_equity.html",
    "href": "posts/2024-10-28-blog-post/data_equity.html",
    "title": "Blog Post 2/3: Data Equity",
    "section": "",
    "text": "Think about how the principles for advancing equitable data practice are relevant to your data.For two or three of principles, write about how they are relevant and what adhering to the practice would entail for your data.Think about what might be some limitations of your analysis and the potential for abuse or misuse of the data.\nWe can utilize the principles for advancing equitable data practices by applying the principles of beneficence, respect for persons, and justice to the exploration of our data set.\nThese principles are relevant for a few reasons. First, looking at beneficence, we may consider how the findings in our data set are being used by local or federal governments, as well as how we are using the data. We can see from this principle what the best actions for the county to take in response to traffic violations would be. Additionally adhering to this practice for our group’s project may look like researching into how the government is using this dataset.\nFurthermore, we can look at the principle of justice. This is relevant to our data as we can see the members of this sample that are facing violations or get into accidents. We can see if they are more at risk than others, and provide some insight through our findings to see if there is any injustice within this county of Maryland response, whether through law or bill.\nSome limitations of our analysis could be missing data, data not provided or included as a variable, and lack of information on the internet to see the local or federal governments lack of/ or response to this data. Some misuse of the data could be government surveillance in high at-risk areas or strict law enforcement. It could also look like policy changes without community input and in turn renforce community inequities."
  },
  {
    "objectID": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "href": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "title": "First Team Meeting",
    "section": "",
    "text": "These are the steps that you will take today to get started on your project. Today, you will just be brainstorming, and then next week, you’ll get started on the main aspects of the project.\n\nStart by introducing yourselves to each other. I also recommend creating a private channel on Microsoft Teams with all your team members. This will be a place that you can communicate and share ideas, code, problems, etc.\nDiscuss what aspects of the project each of you are more or less excited about. These include\n\nCollecting, cleaning, and munging data ,\nStatistical Modeling,\nVisualization,\nWriting about analyses, and\nManaging and reviewing team work.\n\nBased on this, discuss where you feel your strengths and weaknesses might be.\nNext, start brainstorming questions you hope to answer as part of this project. This question should in some way be addressing issues around racial disparities. The questions you come up with should be at the level of the question we started with when exploring the HMDA data. (“Are there differences in the ease of securing a loan based on the race of the applicant?”) You’ll revise your questions a lot over the course of the project. Come up with a few questions that your group might be interested in exploring.\nBased on these questions, start looking around for data that might help you analyze this. If you are looking at U.S. based data, data.gov is a good source and if you are looking internationally, I recommend checking out the World Bank. Also, try Googling for data. Include “data set” or “dataset” in your query. You might even include “CSV” or some other format. Using “data” by itself in your query often doesn’t work too well. Spend some time searching for data and try to come up with at least three possible data sets. (For your first blog post, you’ll write short proposals about each of them that I’ll give feedback on.)\nCome up with a team name. Next week, I’ll provide the Github Classroom assignment that will be where you work on your final project and you’ll have to have your team name finalized by then. Your project will be hosted online at the website with a URL like sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME.\n\nNext time, you’ll get your final project website set up and write your first blog post."
  },
  {
    "objectID": "posts/2023-10-15-getting-started/getting-started.html",
    "href": "posts/2023-10-15-getting-started/getting-started.html",
    "title": "Getting started",
    "section": "",
    "text": "Below, the items marked with [[OP]] should only be done by one person on the team.\n\nTo get started\n\n[[OP]] One person from the team should click the Github Classroom link on Teams.\n[[OP]] That person types in the group name for their group.\nThe rest of the team now clicks the Github Classroom link and selects their team from the dropdown list.\nFinally, each of you can clone the repository to your laptop like a normal assignment.\n\n\n\nSetting up the site\n\n[[OP]] Open the terminal and run quarto publish gh-pages.\n[[OP]] Select Yes to the prompt:  ? Publish site to https://sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME/ using gh-pages? (Y/n)\n[[OP]] Wait for the process to finish.\nOnce it is done, you can go to the URL it asked you about to see your site.\n\nNote: This is the process you will use every time you want to update your published site. Make sure to always follow the steps below for rendering, previewing, and committing your changes before doing these publish steps. Anyone can publish in the future.\n\n\nCustomize your site\n\n[[OP]] Open the _quarto.yml file and update the title to include your team name.\n[[OP]] Go to the about.qmd and remove the TF’s and professor’s names.\nadd your own along with a short introduction and a link to your Github user page.\n[[OP]] Render the site.\n[[OP]] Check and make sure you didn’t get any errors.\n[[OP]] Commit your changes and push.\n[[OP]] Repeat the steps under Setting up your site.\n\nOnce one person is done with this, each teammate in the group can, in turn, repeat steps 3-7. Before doing so, make sure to pull the changes from teammates before starting to make new changes. (We’ll talk soon about ways to organize your work and resolve conflicts.)\n\n\nStart your first post\n\nTo start your first post first, run remotes::install_github(\"sussmanbu/quartopost\") in your Console.\n[[OP]] Run quartopost::quartopost() (or click Addins-&gt;Create Quarto Post, or use C-Shift-P, type “Create Quarto” and press enter to run the command).\n\nNow you can start working on your post. You’ll want to render your post to see what it will look like on the site.\n\nEvery time you want to make a new post, you can repeat step 2 above.\nWhen you want to publish your progress, follow steps 4-7 from Customize your site.\n\nFinally, make sure to read through everything on this site which has the directions and rubric for the final project."
  },
  {
    "objectID": "posts/2024-10-28-blog-post/data_clean.html",
    "href": "posts/2024-10-28-blog-post/data_clean.html",
    "title": "Blog Post 2/3: Data Clean",
    "section": "",
    "text": "In our recent data cleaning work, we started by removing columns that had a high percentage of missing values, specifically those with over 80% missing data, to ensure the reliability of our analysis. We then transformed the format of columns “Local Case Number” to a character format to preserve specific formatting details, and the “Crash Date and Time” to a datetime format for better temporal analysis.\nAdditionally, we addressed missing entries in the “Route Type” column by filling them with the most frequently occurring value, ensuring consistency in our categorical data. We also removed all duplicate entries to maintain the uniqueness of the dataset."
  },
  {
    "objectID": "posts/2024-10-11-blog-post/blog-01.html",
    "href": "posts/2024-10-11-blog-post/blog-01.html",
    "title": "Blog Post 1: Choosing a Dataset",
    "section": "",
    "text": "Summaries: Choosing a Data Set\nNational Student Loan Data System: https://catalog.data.gov/dataset/national-student-loan-data-system-722b0 The National Student Loan data contains information about loans and grants awarded to students under Title IV of the Higher Education Act of 1965, which refers to federal financial aid programs for post secondary education. The data includes various loan categories like Direct Loans, Federal Family Education Loans (FFEL), and Perkins Loans. It spans the entire life cycle of these loans, from disbursement to closure. Most files contain between 10 to 25 columns, with key information such as loan types, disbursement amounts, repayment plans, deferment and forbearance details, and loan statuses. These columns typically include financial metrics like “Dollars Outstanding”, “Recipients”, and “Number of Loans”. The files also vary in terms of row counts; some are relatively small, like summary files with a few hundred rows, while detailed loan or quarterly activity reports contain thousands of rows (ranging from 3798 to 4057 rows). Some files also have additional formatting and metadata rows.\nThis data was originally collected to centralize and monitor federal student aid, helping policymakers, educational institutions, and borrowers manage financial aid efficiently. Main challenges might include cleaning and consolidating disparate files and handling large amounts of historical data, especially if some files are incomplete or inconsistent in format. Some key questions to consider might include loan default rates over time, trends in deferment or forbearance, and the impact of repayment plans on loan outcomes. Additionally, we can explore how loan activity varies across schools and regions, using the quarterly reports to identify trends in loan disbursements and borrower demographics. Another key question would be how the Public Service Loan Forgiveness and Teacher Loan Forgiveness programs are affecting long-term repayment patterns and loan forgiveness rate across various borrower groups.\nCrash Reporting-Drivers Data: https://catalog.data.gov/dataset/crash-reporting-drivers-data This dataset includes information on vehicles that have been involved in collisions on public and local roads within Montgomery County, Maryland. It includes 39 columns and 186,170 rows or entries, and we are able to clean the data as well as load it into the environment. It was collected by an Automated Reporting System (ARS) through the Police department and showed each collision recorded as well as the drivers involved. This data may include verified as well as unverified data and is non-federal.\nThe main questions we hope to address with this data set include data on road safety, driver behaviors, as well as accident patterns within Montgomery County. We could use response and predictor variables such as location, vehicle model, collision type, and time of day (light) to name a few. Some challenges we could face with this data are the number of entries it holds, as well as many NA entries that were unable to be filled by the ARS. This means cleaning this data could take much longer than anticipated. Additionally, it doesn’t provide variables such as Age or Race which could be helpful indicators to see patterns in possible criminal injustice. Overall this data set is very interesting but it may not be the best of the three for analyzing as well as finding prejudices.\nCrime Data from 2020 to Present: https://catalog.data.gov/dataset/crime-data-from-2020-to-present This dataset contains records of crime incidents in Los Angeles, California, from 2020 until 2024. The dataset was digitized from paper reports originally maintained by the LA Police Department and later transferred to an electronic format. It includes 22 columns and over 200,000 rows of data. Due to digitizing, there may be some discrepancies, such as missing or incomplete data, particularly in location fields. For privacy reasons, locations are only recorded up to the hundredth block rather than specific addresses.\nThe dataset contains over 200,000 rows and 22 columns, offering detailed information about various incidents, including crime type, date, time, location, and the involved parties. The Los Angeles Police Department originally collected this data as part of routine crime reporting. Initially maintained as paper records, it was later digitized and entered into an electronic database. This data collection aims to track crime incidents across the city for law enforcement purposes, urban planning, and policy analysis. It is also used to inform the public and local authorities about crime trends and patterns in Los Angeles.\nThe data can be loaded and cleaned, but its large size and missing entries from the transition from paper to digital format may make cleaning time-consuming. Special attention will be needed for missing values, especially in location and categorical fields, and date/time reformatting may be required. Key questions include identifying crime trends from 2020 to 2024, examining neighborhoods with higher crime rates, and finding correlations between crime types and factors like time of day or proximity to locations. Challenges include handling over 200,000 rows, missing location data, and limited precision due to privacy restrictions on location details. Ensuring data consistency after the transition is another potential issue."
  },
  {
    "objectID": "posts/2024-11-06-blog-post/blog-03.html",
    "href": "posts/2024-11-06-blog-post/blog-03.html",
    "title": "Blog Post 3: EDA",
    "section": "",
    "text": "The road with the highest number of crashes is unnamed, with just under 125 crashes recorded. The road with the second-highest number of crashes is Georgia Avenue, which has significantly fewer crashes, totaling just under 50\n\n# The top 10 roads with the highest number of crashes \nlibrary(dplyr)\nlibrary(ggplot2)\n\ncleaned_dataset &lt;- read.csv(\"/dataset/cleaned_dataset.rds\")\n\nroad_crash_data &lt;- cleaned_dataset %&gt;%\n  filter(!is.na(`Road.Name`)) %&gt;%\n  count(`Road.Name`, sort = TRUE) %&gt;%\n  top_n(10, n)\n\nggplot(road_crash_data, aes(x = reorder(`Road.Name`, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +\n  labs(title = \"Top 10 Roads with the Most Crashes\",x = \"Road Name\", y = \"Number of Crashes\") +\n  theme_minimal()\n\n\n# Distribution of Simplified Surface conditions by collision type\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\nsimplified_barplot_data &lt;- cleaned_dataset %&gt;%\n  mutate(Simplified_Surface = case_when(\n    Surface.Condition %in% c(\"Dry\", \"DRY\") ~ \"Dry\",\n    Surface.Condition %in% c(\"Wet\", \"WET\", \"Water (standing, moving)\", \"WATER(STANDING/MOVING)\") ~ \"Wet\",\n    Surface.Condition %in% c(\"Ice\", \"ICE\", \"Ice/Frost\", \"Snow\", \"SNOW\", \"Slush\", \"SLUSH\") ~ \"Snow/Ice\",\n    Surface.Condition %in% c(\"Mud, Dirt, Gravel\", \"MUD, DIRT, GRAVEL\") ~ \"Mud/Dirt\",\n    Surface.Condition %in% c(\"Sand\", \"SAND\", \"OIL\", \"Other\", \"OTHER\") ~ \"Other\",\n    TRUE ~ \"Unknown\"\n  )) %&gt;%\n  count(Collision.Type, Simplified_Surface) %&gt;%\n  group_by(Collision.Type) %&gt;%\n  mutate(percentage = n / sum(n))\n\nggplot(simplified_barplot_data, aes(x = Collision.Type, y = percentage, fill = Simplified_Surface)) +\n  geom_bar(stat = \"identity\", position = \"fill\") +\n  scale_y_continuous(labels = scales::percent) +\n  labs(title = \"Distribution of Simplified Surface Conditions by Collision Type\",\n       x = \"Collision Type\", y = \"Percentage\",\n       fill = \"Surface Condition\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nThere is no definitive way to predict the time of year crashes will occur, but the trend shows a notable spike in crashes from January to March in 2015 and 2017. Additionally, there is often an increase in crashes in the month of November.\n\n# Monthly trend of Crash COunts by Year\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(lubridate)\n\n\ncrash_data &lt;- cleaned_dataset %&gt;%\n  mutate(\n    Crash_Date = ymd_hms(Crash.Date.Time), \n    Year = year(Crash_Date),\n    Month = month(Crash_Date, label = TRUE)\n  ) %&gt;%\n  filter(!is.na(Crash_Date))  \n\nmonthly_data &lt;- crash_data %&gt;%\n  group_by(Year, Month) %&gt;%\n  summarise(Crash_Count = n(), .groups = 'drop')\n\nggplot(monthly_data, aes(x = Month, y = Crash_Count, group = Year)) +\n  geom_line() +\n  labs(\n    title = \"Monthly Trend of Crash Counts by Year\",\n    x = \"Month\", y = \"Number of Crashes\"\n  ) +\n  facet_wrap(~ Year, scales = \"free_y\") +\n  theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nOut of the top 10 roads with the most crashes the surface conditions are mostly dry.\n\n# Find the top 10 roads with the most crashes\ntop_road_crash_data &lt;- cleaned_dataset %&gt;%\n  filter(!is.na(`Road.Name`)) %&gt;%\n  count(`Road.Name`, sort = TRUE) %&gt;%\n  top_n(10, n)\n\n# Filter the cleaned dataset to only include these top 10 roads\ntop_road_conditions &lt;- cleaned_dataset %&gt;%\n  filter(`Road.Name` %in% top_road_crash_data$`Road.Name`) %&gt;%\n  mutate(Simplified_Surface = case_when(\n    Surface.Condition %in% c(\"Dry\", \"DRY\") ~ \"Dry\",\n    Surface.Condition %in% c(\"Wet\", \"WET\", \"Water (standing, moving)\", \"WATER(STANDING/MOVING)\") ~ \"Wet\",\n    Surface.Condition %in% c(\"Ice\", \"ICE\", \"Ice/Frost\", \"Snow\", \"SNOW\", \"Slush\", \"SLUSH\") ~ \"Snow/Ice\",\n    Surface.Condition %in% c(\"Mud, Dirt, Gravel\", \"MUD, DIRT, GRAVEL\") ~ \"Mud/Dirt\",\n    Surface.Condition %in% c(\"Sand\", \"SAND\", \"OIL\", \"Other\", \"OTHER\") ~ \"Other\",\n    TRUE ~ \"Unknown\"\n  )) %&gt;%\n  count(`Road.Name`, Simplified_Surface) %&gt;%\n  group_by(`Road.Name`) %&gt;%\n  mutate(percentage = n / sum(n))\n\n# Plot the distribution of surface conditions for the top 10 roads\nggplot(top_road_conditions, aes(x = reorder(`Road.Name`, -n), y = percentage, fill = Simplified_Surface)) +\n  geom_bar(stat = \"identity\", position = \"fill\") +\n  scale_y_continuous(labels = scales::percent) +\n  coord_flip() +\n  labs(\n    title = \"Surface Conditions for the Top 10 Roads with the Most Crashes\",\n    x = \"Road Name\", y = \"Percentage of Crashes\",\n    fill = \"Surface Condition\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "posts/2024-10-21-blog-post/blog-02.html",
    "href": "posts/2024-10-21-blog-post/blog-02.html",
    "title": "Blog Post 2: Data Background",
    "section": "",
    "text": "The data comes from the Automated Crash Reporting System (ACRS) of the Maryland State Police. It includes traffic collision reports from Montgomery County Police, Gaithersburg Police, Rockville Police, and the Maryland National Capital Park Police. The data was collected by the ACRS and local law enforcement agencies. The data was collected to document and analyze traffic collisions involving motor vehicle operators on county and local roads in Montgomery County. The purpose of this data collection might be to help enforcement agencies understand the circumstances of traffic collisions. It can be found on the original website, which is the Montgomery County government data site."
  },
  {
    "objectID": "posts/2024-10-21-blog-post/blog-02.html#where-does-the-data-come-from-who-collected-it-why-was-it-collected-are-you-able-to-find-the-data-from-the-original-source",
    "href": "posts/2024-10-21-blog-post/blog-02.html#where-does-the-data-come-from-who-collected-it-why-was-it-collected-are-you-able-to-find-the-data-from-the-original-source",
    "title": "Blog Post 2: Data Background",
    "section": "",
    "text": "The data comes from the Automated Crash Reporting System (ACRS) of the Maryland State Police. It includes traffic collision reports from Montgomery County Police, Gaithersburg Police, Rockville Police, and the Maryland National Capital Park Police. The data was collected by the ACRS and local law enforcement agencies. The data was collected to document and analyze traffic collisions involving motor vehicle operators on county and local roads in Montgomery County. The purpose of this data collection might be to help enforcement agencies understand the circumstances of traffic collisions. It can be found on the original website, which is the Montgomery County government data site."
  },
  {
    "objectID": "posts/2024-10-21-blog-post/blog-02.html#are-there-any-issues-you-can-see-with-how-the-data-was-collected-what-is-the-sample-population-are-there-reasons-to-think-the-sample-is-biased-in-some-way",
    "href": "posts/2024-10-21-blog-post/blog-02.html#are-there-any-issues-you-can-see-with-how-the-data-was-collected-what-is-the-sample-population-are-there-reasons-to-think-the-sample-is-biased-in-some-way",
    "title": "Blog Post 2: Data Background",
    "section": "Are there any issues you can see with how the data was collected? What is the sample population? Are there reasons to think the sample is biased in some way?",
    "text": "Are there any issues you can see with how the data was collected? What is the sample population? Are there reasons to think the sample is biased in some way?\nAccording to the information from the original government website providing the data, since the collision reports are based on preliminary information supplied to the Police Department by the reporting parties, there are some potential issues with the data collection process, including:\n\nInformation not yet verified by further investigation\nInformation that may include verified and unverified collision data\nPreliminary collision classifications may be changed at a later date based on further investigation\nInformation may include mechanical or human error\n\nThe sample population consists of motor vehicle operators involved in traffic collisions on county and local roadways within Montgomery County, Maryland. The dataset might be biased because it may not include minor collisions or unreported incidents."
  },
  {
    "objectID": "posts/2024-10-21-blog-post/blog-02.html#how-is-this-data-used-what-questions-have-others-ask-about-this-data",
    "href": "posts/2024-10-21-blog-post/blog-02.html#how-is-this-data-used-what-questions-have-others-ask-about-this-data",
    "title": "Blog Post 2: Data Background",
    "section": "How is this data used? What questions have others ask about this data?",
    "text": "How is this data used? What questions have others ask about this data?\nThe data is widely used by government agencies, law enforcement, transportation planners, and other stakeholders who might be looking into improving road safety and traffic management. By analyzing the collision patterns, they can identify areas that might be prone to accidents and take corrective measures. The data set also includes information on driver at-fault status, substance abuse, and distractions providing insights into common behavioral causes of collisions. This can inform stricter law enforcement measures. Officials may also use this data to assess the effectiveness of traffic regulations. This data can also be used by insurance companies for underwriting and to assess claims involving collisions. Some questions that others might ask about the data are:\n\nWhich roads or intersections are leading to the highest collision rates and why? This will help identify accident hotspots that need infrastructural improvements\nWhat is the correlation between weather conditions and the rate of accidents? This can inform them on whether or not they need more safety warnings during hazardous weather conditions that might help people navigate these conditions better and potentially lead to fewer collisions\nWhat time of the day do most accidents occur? Understanding this better can help change street lighting or traffic patterns to make driving at night easier\nAre there certain car models that have more accidents than others? This could lead to a ban on certain kinds of vehicles or even help manufacturers fix flaws"
  },
  {
    "objectID": "posts/2024-10-21-blog-post/blog-02.html#has-there-been-other-research-on-the-same-data-is-this-data-being-used-for-some-policy-decisions",
    "href": "posts/2024-10-21-blog-post/blog-02.html#has-there-been-other-research-on-the-same-data-is-this-data-being-used-for-some-policy-decisions",
    "title": "Blog Post 2: Data Background",
    "section": "Has there been other research on the same data? Is this data being used for some policy decisions?",
    "text": "Has there been other research on the same data? Is this data being used for some policy decisions?\nThis data provided by the state of New Jersey is being used by the New Jersey Department of Transportation (NJDOT) through a database called Safety Voyager. This tool allows them to utilize data like this set as well as similar datasets from other counties in the state, to visualize and analyze crash data. A dataset like ours is used in combination with the surrounding towns to identify areas of high risk. This data is also used to better safety improvement projects such as The Highway Safety Improvement Project (HSIP) which gathers this information from NJDOT to reduce crashes in crash-dense areas. Additionally, the New Jersey Transportation Planning Authority (NJTPA) also refers to information synthesized from our data to create overall safety examinations and better planning efforts. Along with NJDOT, HSIP, and NJTPA, the New Jersey Safety Outcomes and Data Warehouse (NJ-SHO) integrates crash data such as ours as well as health consequences to investigate public safety. Overall, it is mainly the state departments of New Jersey that are leading research projects with these crash reports, and creating programs and initiatives that will encourage future safety measures in the state."
  },
  {
    "objectID": "posts/2024-11-18-blog-post/blog-06.html",
    "href": "posts/2024-11-18-blog-post/blog-06.html",
    "title": "Blog Post 6",
    "section": "",
    "text": "The Pavement Condition Index (PCI) is a numerical expression ranging from 0 to 100, representing the condition of pavement. Based on my own hypothesis, assuming “the higher the PCI, the lower the accident rate,” I conducted a linear regression analysis using PCI (avg_pci) as the independent variable. The results show that the intercept (Intercept) is 11.09965, indicating the predicted number of accidents when avg_pci = 0. The estimated coefficient for avg_pci is -0.04909, which suggests that for every one-unit increase in PCI, the number of accidents decreases by 0.049 on average. This coefficient is statistically significant (p = 0.00337).\nHowever, the adjusted R-squared value is only 0.004102, indicating that PCI has very limited explanatory power for the number of accidents. The scatter plot shows that most data points are concentrated in the lower PCI range with considerable dispersion, and the linear regression line has a very slight negative slope. These observations suggest that PCI may have a minimal linear effect on accident rates.\n\n\n\nFurther analysis revealed that areas with a higher number of accidents tend to fall within the middle PCI range (30-80). This may imply that busier roads often have higher PCI values, and these roads also experience more accidents. This observation challenges the initial hypothesis and suggests a more complex relationship between PCI and the number of accidents.\n\n\n\nRelationship Between PCI and Accident Count\n\n\n\n\n\nTo explore the impact of PCI on accident rates more comprehensively, I am implementing the following strategies:\n\nIncorporating Additional Variables: After cleaning and integrating the data, I am including additional potential influencing factors such as traffic volume, weather, and speed limits. By using multivariable regression analysis, I aim to quantify the combined effects of these variables on accident rates.\nNonlinear Relationship Investigation: I will include quadratic or higher-order terms for PCI in the regression model to capture possible nonlinear patterns and improve the model’s fit, further validating the hypothesis.\nExploring Discrete Data Models: Given that accident rates are discrete count data, I may employ Poisson regression or negative binomial regression models for a more accurate representation of accident count distributions.\n\n\n\n\nCurrently, I am focusing on the following tasks:\n\nData Cleaning and Integration:\nI have standardized field names for consistency, resolving many-to-many relationships between road names across datasets, ensuring alignment of fields between different datasets.\nExploratory Statistical Analysis:\nI am grouping the data by key fields to calculate averages and accident counts, providing a foundation for subsequent modeling.\nRegression Modeling:\nI am gradually incorporating additional variables, such as weather and speed limits, to analyze their significant impacts on accident rates beyond PCI.\nVisualization of Relationships:\nI am creating scatter plots with trend lines to visually represent the relationships between variables, aiding in the interpretation and validation of analysis results."
  },
  {
    "objectID": "posts/2024-11-18-blog-post/blog-06.html#pavement-condition-index-pci",
    "href": "posts/2024-11-18-blog-post/blog-06.html#pavement-condition-index-pci",
    "title": "Blog Post 6",
    "section": "",
    "text": "The Pavement Condition Index (PCI) is a numerical expression ranging from 0 to 100, representing the condition of pavement. Based on my own hypothesis, assuming “the higher the PCI, the lower the accident rate,” I conducted a linear regression analysis using PCI (avg_pci) as the independent variable. The results show that the intercept (Intercept) is 11.09965, indicating the predicted number of accidents when avg_pci = 0. The estimated coefficient for avg_pci is -0.04909, which suggests that for every one-unit increase in PCI, the number of accidents decreases by 0.049 on average. This coefficient is statistically significant (p = 0.00337).\nHowever, the adjusted R-squared value is only 0.004102, indicating that PCI has very limited explanatory power for the number of accidents. The scatter plot shows that most data points are concentrated in the lower PCI range with considerable dispersion, and the linear regression line has a very slight negative slope. These observations suggest that PCI may have a minimal linear effect on accident rates.\n\n\n\nFurther analysis revealed that areas with a higher number of accidents tend to fall within the middle PCI range (30-80). This may imply that busier roads often have higher PCI values, and these roads also experience more accidents. This observation challenges the initial hypothesis and suggests a more complex relationship between PCI and the number of accidents.\n\n\n\nRelationship Between PCI and Accident Count\n\n\n\n\n\nTo explore the impact of PCI on accident rates more comprehensively, I am implementing the following strategies:\n\nIncorporating Additional Variables: After cleaning and integrating the data, I am including additional potential influencing factors such as traffic volume, weather, and speed limits. By using multivariable regression analysis, I aim to quantify the combined effects of these variables on accident rates.\nNonlinear Relationship Investigation: I will include quadratic or higher-order terms for PCI in the regression model to capture possible nonlinear patterns and improve the model’s fit, further validating the hypothesis.\nExploring Discrete Data Models: Given that accident rates are discrete count data, I may employ Poisson regression or negative binomial regression models for a more accurate representation of accident count distributions.\n\n\n\n\nCurrently, I am focusing on the following tasks:\n\nData Cleaning and Integration:\nI have standardized field names for consistency, resolving many-to-many relationships between road names across datasets, ensuring alignment of fields between different datasets.\nExploratory Statistical Analysis:\nI am grouping the data by key fields to calculate averages and accident counts, providing a foundation for subsequent modeling.\nRegression Modeling:\nI am gradually incorporating additional variables, such as weather and speed limits, to analyze their significant impacts on accident rates beyond PCI.\nVisualization of Relationships:\nI am creating scatter plots with trend lines to visually represent the relationships between variables, aiding in the interpretation and validation of analysis results."
  },
  {
    "objectID": "posts/2024-11-18-blog-post/blog-06.html#crime-rate-analysis",
    "href": "posts/2024-11-18-blog-post/blog-06.html#crime-rate-analysis",
    "title": "Blog Post 6",
    "section": "Crime Rate Analysis",
    "text": "Crime Rate Analysis\n\nHypothesis Test\n\nH_0: There is no relationship between between daily crime count and daily crash reporting.\nH_a: Days with higher crime counts mean higher crash reporting.\n\n\n\nSetting up Data for Analysis\nIn order to do this I filtered the original crime data from DATA.GOV to show crime reporting for only Montgomery Village within Montgomery County. I then chose to focus on the crime’s start time in order to analyze these crime counts as they began. Next, I adjusted the start date column into a proper date-time format in order to pull the date and mutate a column onto the filtered crime data set. Once doing this I was then able to create a new data frame that counted crimes that occurred on a specific day. I then joined these two frames into a data set that showed all days that had been reported whether crime or crash, and counted each.\n\n\nAnalyzing a Possible Relationship\nTo see if there is anything to this hypothesis or if we don’t have a strong enough claim I used a couple of different plots. First I began with a scatterplot that included the line of best fit with method “lm.” This is shown below and represents the general relationship between the two variables.\n\n\n\nRelationship Between Daily Crime and Crash Counts\n\n\nWe can see that the line is nearly horizontal, but there is a slight increase, meaning a possible positive correlation between our variables. Given that we can find the correlation to be about 0.0326, we can see there’s nearly no significant linear relationship between our variables.\nNext, before we know what test to run for our hypothesis I wanted to check for homoscedasticity and normality. This required me to look at the NQ-Q and Residual Plots. The NQ-Q plot helped to show the quantiles graphed against the standardized quantiles, which helps to show a bit of skewness at the tails of our distribution. Additionally, looking at our Residual plot shows there to be a few outliers, but generally, we can summarize as the fitted values increase our variance decreases. Overall both of these plots are interesting but not necessarily valuable because we can’t see any huge significance in either. Additionally, I wouldn’t assume normality or homoscedasticity which eliminates many testing options. The best case would be to use a Monte Carlo simulation for further exploration.\n\n\n\nQ-Q Residuals and Residuals vs Fitted\n\n\nLastly, we can look at a density graph that shows the different distributions of crashes on “High” versus “Low” crime count days. These thresholds are defined by whether the crime count for one day is above or below the median of total crime counts.\n\n\n\nDensity Plot of Crash Counts by Crime Level\n\n\nThere seems to be a significant overlap between the distributions which means the range for crash reports is about the same given higher or lower crime counts. However, there is a slight shift to the right for the “High” distribution which agrees with our previous scatterplot.\n\n\nPlan for further analysis\nGiven that these variables don’t appear to be largely related; I would look to another data set that could provide further enhancement to our crash reporting or I could focus on this case where we maybe don’t see a strong relationship between crime in this county and vehicle crashes. This could be interesting to further analyze and one may assume they would have a significant relationship.\nIf I were to continue with this analysis, I would first figure out what test I think would be the best to run and see if there’s any significance between our variables or if we fail to reject our null."
  },
  {
    "objectID": "dataset/Cleaning_Xiang.html",
    "href": "dataset/Cleaning_Xiang.html",
    "title": "crash car data clean",
    "section": "",
    "text": "options(repos = c(CRAN = \"https://cran.r-project.org\"))\ninstall.packages(\"skimr\")\n\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(lubridate)\nlibrary(skimr)  # for quick summary statistics\n\n\ninstall.packages(\"RCurl\")\nlibrary(RCurl)\nx &lt;- getURL(\"https://raw.githubusercontent.com/sussmanbu/ma-4615-fa24-final-project-group-1/main/Crash_Reporting_Drivers_Data.csv\")\ndrivers_data &lt;- read.csv(text = x)\n\n\nglimpse(drivers_data)\n\nskim(drivers_data)\n\nClean Column Names\n\ndrivers_data_clean &lt;- drivers_data %&gt;%\n  clean_names()\n\ncolnames(drivers_data_clean)\n\nHandle Date/Time Format\n\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    crash_date/time &lt;- mdy_hms(crash_date/time),\n    # create separate date and time columns if needed\n    crash_date = date(crash_date/time),\n    crash_time = format(crash_date/time, \"%H:%M:%S\")\n  )\n\nStandardize Categorical Variables\n\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # standardize agency names\n    agency_name = str_trim(agency_name),\n    \n    # clean and standardize route type\n    route_type = case_when(\n      is.na(route_type) ~ \"Unknown\",\n      TRUE ~ route_type\n    ),\n    \n    # standardize weather conditions\n    weather = str_to_title(weather),\n    \n    # clean surface condition\n    surface_condition = case_when(\n      is.na(surface_condition) ~ \"Unknown\",\n      TRUE ~ surface_condition\n    ),\n    \n    # standardize driver at fault\n    driver_at_fault = case_when(\n      str_to_lower(driver_at_fault) == \"yes\" ~ \"Yes\",\n      str_to_lower(driver_at_fault) == \"no\" ~ \"No\",\n      TRUE ~ \"Unknown\"\n    )\n  )\n\nHandle Missing Values\n\n# calculate missing values percentage\nmissing_values &lt;- drivers_data_clean %&gt;%\n  summarise(across(everything(), ~sum(is.na(.))/n()*100)) %&gt;%\n  pivot_longer(everything(), \n              names_to = \"column\", \n              values_to = \"missing_percentage\") %&gt;%\n  arrange(desc(missing_percentage))\n\n# display columns with missing values\nprint(missing_values %&gt;% filter(missing_percentage &gt; 0))\n\n# handle missing values based on column type\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # for route_type, road_name, and cross_street_name\n    # keep NA as is since they might be meaningful (e.g., off-road incidents)\n    \n    # for numeric columns, consider if 0 or NA is more appropriate\n    speed_limit = if_else(is.na(speed_limit), 0, speed_limit),\n    \n    # for categorical columns, mark unknown\n    surface_condition = if_else(is.na(surface_condition), \"Unknown\", surface_condition),\n    traffic_control = if_else(is.na(traffic_control), \"Unknown\", traffic_control)\n  )\n\nData Validation and Consistency Checks\n\n# check for logical consistencies\nvalidation_results &lt;- drivers_data_clean %&gt;%\n  summarise(\n    # check for future dates\n    future_dates = sum(crash_date &gt; Sys.Date()),\n    \n    # check for valid speed limits\n    invalid_speed = sum(speed_limit &gt; 70 | speed_limit &lt; 0, na.rm = TRUE),\n    \n    # check for valid vehicle years\n    invalid_vehicle_year = sum(vehicle_year &lt; 1900 | vehicle_year &gt; year(Sys.Date()) + 1, na.rm = TRUE),\n    \n    # check for valid coordinates\n    invalid_lat = sum(latitude &lt; 38 | latitude &gt; 40, na.rm = TRUE),\n    invalid_lon = sum(longitude &gt; -76 | longitude &lt; -78, na.rm = TRUE)\n  )\n\nprint(validation_results)\n\n# create flags for potential data quality issues\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    data_quality_flag = case_when(\n      crash_date &gt; Sys.Date() ~ \"Future date\",\n      speed_limit &gt; 70 | speed_limit &lt; 0 ~ \"Invalid speed limit\",\n      vehicle_year &lt; 1900 | vehicle_year &gt; year(Sys.Date()) + 1 ~ \"Invalid vehicle year\",\n      latitude &lt; 38 | latitude &gt; 40 | longitude &gt; -76 | longitude &lt; -78 ~ \"Invalid coordinates\",\n      TRUE ~ \"Valid\"\n    )\n  )\n\nCheck Value Distributions\n\n# check common categories\ncategory_summaries &lt;- list(\n  collision_types = table(drivers_data_clean$collision_type),\n  weather_conditions = table(drivers_data_clean$weather),\n  vehicle_types = table(drivers_data_clean$vehicle_body_type),\n  injury_severity = table(drivers_data_clean$injury_severity)\n)\n\nprint(category_summaries)\n\nCollision Types Need Standardization:\n\n# standardize collision types\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    collision_type = case_when(\n      collision_type %in% c(\"Angle\", \"ANGLE MEETS LEFT HEAD ON\", \"ANGLE MEETS LEFT TURN\",\n                           \"ANGLE MEETS RIGHT TURN\", \"STRAIGHT MOVEMENT ANGLE\") ~ \"ANGLE\",\n      collision_type %in% c(\"Front to Front\", \"HEAD ON\", \"HEAD ON LEFT TURN\") ~ \"HEAD_ON\",\n      collision_type %in% c(\"Front to Rear\", \"SAME DIR REAR END\", \n                           \"SAME DIR REND LEFT TURN\", \"SAME DIR REND RIGHT TURN\") ~ \"REAR_END\",\n      collision_type %in% c(\"SAME DIRECTION SIDESWIPE\", \"Sideswipe, Same Direction\") ~ \"SIDESWIPE_SAME_DIR\",\n      collision_type %in% c(\"OPPOSITE DIRECTION SIDESWIPE\", \"Sideswipe, Opposite Direction\") ~ \"SIDESWIPE_OPPOSITE_DIR\",\n      collision_type %in% c(\"SINGLE VEHICLE\", \"Single Vehicle\") ~ \"SINGLE_VEHICLE\",\n      collision_type %in% c(\"Other\", \"OTHER\") ~ \"OTHER\",\n      collision_type %in% c(\"Unknown\", \"UNKNOWN\", \"N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\nWeather Conditions Need Standardization:\n\n# standardize weather conditions\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    weather = case_when(\n      str_detect(str_to_upper(weather), \"CLEAR\") ~ \"CLEAR\",\n      str_detect(str_to_upper(weather), \"CLOUD\") ~ \"CLOUDY\",\n      str_detect(str_to_upper(weather), \"RAIN|RAINING\") ~ \"RAIN\",\n      str_detect(str_to_upper(weather), \"SNOW|BLOWING SNOW\") ~ \"SNOW\",\n      str_detect(str_to_upper(weather), \"FOG|SMOG|SMOKE\") ~ \"FOG\",\n      weather %in% c(\"N/A\", \"Unknown\", \"UNKNOWN\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\nVehicle Types Need Major Cleanup:\n\n# standardize vehicle types\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    vehicle_body_type = case_when(\n      str_detect(str_to_upper(vehicle_body_type), \"PASSENGER CAR|PASSENGER|CAR\") ~ \"PASSENGER_CAR\",\n      str_detect(str_to_upper(vehicle_body_type), \"SUV|SPORT UTILITY|UTILITY VEHICLE\") ~ \"SUV\",\n      str_detect(str_to_upper(vehicle_body_type), \"PICKUP|LIGHT TRUCK\") ~ \"PICKUP_TRUCK\",\n      str_detect(str_to_upper(vehicle_body_type), \"VAN|CARGO\") ~ \"VAN\",\n      str_detect(str_to_upper(vehicle_body_type), \"BUS\") ~ \"BUS\",\n      str_detect(str_to_upper(vehicle_body_type), \"MOTORCYCLE|MOPED\") ~ \"MOTORCYCLE\",\n      str_detect(str_to_upper(vehicle_body_type), \"EMERGENCY|POLICE|FIRE|AMBULANCE\") ~ \"EMERGENCY_VEHICLE\",\n      vehicle_body_type %in% c(\"N/A\", \"Unknown\", \"UNKNOWN\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\nInjury Severity Standardization:\n\n# standardize injury severity\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    injury_severity = case_when(\n      str_detect(str_to_upper(injury_severity), \"FATAL\") ~ \"FATAL\",\n      str_detect(str_to_upper(injury_severity), \"NO APPARENT|NO INJURY\") ~ \"NO_INJURY\",\n      str_detect(str_to_upper(injury_severity), \"POSSIBLE\") ~ \"POSSIBLE_INJURY\",\n      str_detect(str_to_upper(injury_severity), \"MINOR\") ~ \"MINOR_INJURY\",\n      str_detect(str_to_upper(injury_severity), \"SERIOUS\") ~ \"SERIOUS_INJURY\",\n      TRUE ~ \"UNKNOWN\"\n    )\n  )\n\nHandle Missing Values Strategy (based on the missing percentage analysis):\n\n# handle missing values based on context\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # high missing percentage (&gt;90%) - keep as NA but add flag\n    has_non_motorist = !is.na(related_non_motorist),\n    \n    # medium missing percentage (10-90%) - add meaningful categories\n    municipality = if_else(is.na(municipality), \"UNINCORPORATED\", municipality),\n    road_name = if_else(is.na(road_name), \"OFF_ROAD\", road_name),\n    cross_street_name = if_else(is.na(cross_street_name), \"NOT_APPLICABLE\", cross_street_name),\n    \n    # low missing percentage (&lt;10%) - impute with \"UNKNOWN\"\n    drivers_license_state = if_else(is.na(drivers_license_state), \"UNKNOWN\", drivers_license_state),\n    circumstance = if_else(is.na(circumstance), \"UNKNOWN\", circumstance),\n    vehicle_going_dir = if_else(is.na(vehicle_going_dir), \"UNKNOWN\", vehicle_going_dir)\n  )\n\nHandle Data Quality Issues (based on validation results):\n\n# add data quality flags\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    data_quality_flags = case_when(\n      speed_limit &gt; 70 | speed_limit &lt; 0 ~ \"INVALID_SPEED_LIMIT\",\n      vehicle_year &lt; 1900 | vehicle_year &gt; year(Sys.Date()) + 1 ~ \"INVALID_VEHICLE_YEAR\",\n      latitude &lt; 38 | latitude &gt; 40 | longitude &gt; -76 | longitude &lt; -78 ~ \"INVALID_COORDINATES\",\n      TRUE ~ \"VALID\"\n    ),\n    \n    # clean speed limits\n    speed_limit = case_when(\n      speed_limit &gt; 70 ~ NA_real_,\n      speed_limit &lt; 0 ~ NA_real_,\n      TRUE ~ speed_limit\n    ),\n    \n    # clean vehicle years\n    vehicle_year = if_else(\n      vehicle_year &lt; 1900 | vehicle_year &gt; year(Sys.Date()) + 1,\n      NA_real_,\n      vehicle_year\n    )\n  )\n\nAdditional cleaning for specific columns:\n\n# additional cleaning steps for specific columns and vehicle data\n\n# standardize vehicle makes (common misspellings and abbreviations)\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    vehicle_make = case_when(\n      # honda variations\n      str_detect(str_to_upper(vehicle_make), \"HOND|HDA\") ~ \"HONDA\",\n      \n      # toyota variations\n      str_detect(str_to_upper(vehicle_make), \"TOY|TOYT\") ~ \"TOYOTA\",\n      \n      # ford\n      str_detect(str_to_upper(vehicle_make), \"^FRD|FORD\") ~ \"FORD\",\n      \n      # chevrolet variations\n      str_detect(str_to_upper(vehicle_make), \"CHEV|CHEVY|CHV\") ~ \"CHEVROLET\",\n      \n      # nissan variations\n      str_detect(str_to_upper(vehicle_make), \"NISS|NISN\") ~ \"NISSAN\",\n      \n      # hyundai variations\n      str_detect(str_to_upper(vehicle_make), \"HYUN|HYU\") ~ \"HYUNDAI\",\n      \n      # volkswagen variations\n      str_detect(str_to_upper(vehicle_make), \"VW|VOLK|VOLKS\") ~ \"VOLKSWAGEN\",\n      \n      # BMW\n      str_detect(str_to_upper(vehicle_make), \"BMW|BMV\") ~ \"BMW\",\n      \n      # Mercedes-Benz variations\n      str_detect(str_to_upper(vehicle_make), \"MERZ|MENZ|MERCEDES|BENZ\") ~ \"MERCEDES-BENZ\",\n      \n      # lexus\n      str_detect(str_to_upper(vehicle_make), \"LEX|LEXS\") ~ \"LEXUS\",\n      \n      # mazda variations\n      str_detect(str_to_upper(vehicle_make), \"MAZ|MAZD\") ~ \"MAZDA\",\n      \n      # subaru variations\n      str_detect(str_to_upper(vehicle_make), \"SUB|SUBR\") ~ \"SUBARU\",\n      \n      # kia\n      str_detect(str_to_upper(vehicle_make), \"^KIA\") ~ \"KIA\",\n      \n      # audi\n      str_detect(str_to_upper(vehicle_make), \"^AUD\") ~ \"AUDI\",\n      \n      # acura\n      str_detect(str_to_upper(vehicle_make), \"ACUR|ACU\") ~ \"ACURA\",\n      \n      # infinity\n      str_detect(str_to_upper(vehicle_make), \"INF|INFIN\") ~ \"INFINITI\",\n      \n      TRUE ~ str_to_upper(vehicle_make)\n    )\n  )\n\n# clean vehicle models\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    vehicle_model = case_when(\n      # clean common abbreviations\n      str_detect(str_to_upper(vehicle_model), \"^CRV|CR-V\") ~ \"CR-V\",\n      str_detect(str_to_upper(vehicle_model), \"^RAV|RAV4\") ~ \"RAV4\",\n      str_detect(str_to_upper(vehicle_model), \"ACCORD|ACRD\") ~ \"ACCORD\",\n      str_detect(str_to_upper(vehicle_model), \"CAMRY|CAM\") ~ \"CAMRY\",\n      str_detect(str_to_upper(vehicle_model), \"CIVIC|CVC\") ~ \"CIVIC\",\n      str_detect(str_to_upper(vehicle_model), \"ALTIMA|ALT\") ~ \"ALTIMA\",\n      str_detect(str_to_upper(vehicle_model), \"COROLLA|COR\") ~ \"COROLLA\",\n      str_detect(str_to_upper(vehicle_model), \"EXPLORER|EXPLR\") ~ \"EXPLORER\",\n      str_detect(str_to_upper(vehicle_model), \"F-150|F150\") ~ \"F-150\",\n      str_detect(str_to_upper(vehicle_model), \"HIGHLANDER|HGLDR\") ~ \"HIGHLANDER\",\n      TRUE ~ str_to_upper(vehicle_model)\n    )\n  )\n\n# additional cleaning for other specific columns\n\n# clean light conditions\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    light = case_when(\n      str_detect(str_to_upper(light), \"DAYLIGHT\") ~ \"DAYLIGHT\",\n      str_detect(str_to_upper(light), \"DARK.*LIGHT.*ON|LIGHTED\") ~ \"DARK_WITH_LIGHTING\",\n      str_detect(str_to_upper(light), \"DARK.*NO.*LIGHT|UNLIGHTED\") ~ \"DARK_NO_LIGHTING\",\n      str_detect(str_to_upper(light), \"DAWN\") ~ \"DAWN\",\n      str_detect(str_to_upper(light), \"DUSK\") ~ \"DUSK\",\n      str_detect(str_to_upper(light), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# clean surface conditions\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    surface_condition = case_when(\n      str_detect(str_to_upper(surface_condition), \"DRY\") ~ \"DRY\",\n      str_detect(str_to_upper(surface_condition), \"WET\") ~ \"WET\",\n      str_detect(str_to_upper(surface_condition), \"ICE|ICY\") ~ \"ICE\",\n      str_detect(str_to_upper(surface_condition), \"SNOW|SLUSH\") ~ \"SNOW\",\n      str_detect(str_to_upper(surface_condition), \"SAND|DIRT|MUD\") ~ \"SAND_DIRT_MUD\",\n      str_detect(str_to_upper(surface_condition), \"OIL|GREASE\") ~ \"OIL_GREASE\",\n      is.na(surface_condition) | str_detect(str_to_upper(surface_condition), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# clean traffic control\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    traffic_control = case_when(\n      str_detect(str_to_upper(traffic_control), \"SIGNAL|TRAFFIC LIGHT\") ~ \"TRAFFIC_SIGNAL\",\n      str_detect(str_to_upper(traffic_control), \"STOP SIGN\") ~ \"STOP_SIGN\",\n      str_detect(str_to_upper(traffic_control), \"YIELD\") ~ \"YIELD_SIGN\",\n      str_detect(str_to_upper(traffic_control), \"FLASHING\") ~ \"FLASHING_SIGNAL\",\n      str_detect(str_to_upper(traffic_control), \"NO CONTROL|NONE\") ~ \"NO_CONTROL\",\n      str_detect(str_to_upper(traffic_control), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# clean driver substance abuse\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    driver_substance_abuse = case_when(\n      str_detect(str_to_upper(driver_substance_abuse), \"NONE|NO ABUSE\") ~ \"NONE\",\n      str_detect(str_to_upper(driver_substance_abuse), \"ALCOHOL\") ~ \"ALCOHOL\",\n      str_detect(str_to_upper(driver_substance_abuse), \"DRUG\") ~ \"DRUGS\",\n      str_detect(str_to_upper(driver_substance_abuse), \"BOTH|ALCOHOL.*DRUG|DRUG.*ALCOHOL\") ~ \"ALCOHOL_AND_DRUGS\",\n      str_detect(str_to_upper(driver_substance_abuse), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# clean driver distracted by\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    driver_distracted_by = case_when(\n      str_detect(str_to_upper(driver_distracted_by), \"NOT DISTRACTED\") ~ \"NOT_DISTRACTED\",\n      str_detect(str_to_upper(driver_distracted_by), \"CELL|PHONE|MOBILE\") ~ \"CELL_PHONE\",\n      str_detect(str_to_upper(driver_distracted_by), \"PASSENGER\") ~ \"PASSENGER\",\n      str_detect(str_to_upper(driver_distracted_by), \"RADIO|AUDIO\") ~ \"AUDIO_EQUIPMENT\",\n      str_detect(str_to_upper(driver_distracted_by), \"EAT|DRINK\") ~ \"EATING_DRINKING\",\n      str_detect(str_to_upper(driver_distracted_by), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# verify our cleaning by checking the unique values in each cleaned column\nverification_check &lt;- function(data, columns) {\n  map(columns, ~{\n    unique_vals &lt;- data %&gt;% \n      pull(.) %&gt;% \n      unique() %&gt;% \n      sort()\n    \n    cat(\"\\nUnique values in\", ., \":\\n\")\n    print(unique_vals)\n    cat(\"\\n\")\n  })\n}\n\n# verify the cleaning results for key columns\ncolumns_to_verify &lt;- c(\n  \"vehicle_make\", \n  \"vehicle_model\",\n  \"light\",\n  \"surface_condition\",\n  \"traffic_control\",\n  \"driver_substance_abuse\",\n  \"driver_distracted_by\"\n)\n\nverification_check(drivers_data_clean, columns_to_verify)\n\nvehicle_summary &lt;- drivers_data_clean %&gt;%\n  group_by(vehicle_make, vehicle_model) %&gt;%\n  summarise(\n    count = n(),\n    .groups = 'drop'\n  ) %&gt;%\n  arrange(desc(count))\n\nprint(vehicle_summary)\n\n\n# clean and standardize agency names\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    agency_name = case_when(\n      str_detect(str_to_upper(agency_name), \"MONTGOMERY|MONT|MCP\") ~ \"MONTGOMERY COUNTY POLICE\",\n      str_detect(str_to_upper(agency_name), \"GAITHERSBURG|GAITH\") ~ \"GAITHERSBURG POLICE\",\n      str_detect(str_to_upper(agency_name), \"ROCKVILLE|ROCK\") ~ \"ROCKVILLE POLICE\",\n      str_detect(str_to_upper(agency_name), \"TAKOMA|TAK\") ~ \"TAKOMA PARK POLICE\",\n      str_detect(str_to_upper(agency_name), \"PARK|MNCPP\") ~ \"MD NATIONAL CAPITAL PARK POLICE\",\n      TRUE ~ str_to_upper(agency_name)\n    )\n  )\n\n# clean and standardize vehicle damage extent\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    vehicle_damage_extent = case_when(\n      str_detect(str_to_upper(vehicle_damage_extent), \"NONE|NO DAMAGE\") ~ \"NO_DAMAGE\",\n      str_detect(str_to_upper(vehicle_damage_extent), \"MINOR|SUPERFICIAL\") ~ \"MINOR\",\n      str_detect(str_to_upper(vehicle_damage_extent), \"FUNCTIONAL\") ~ \"FUNCTIONAL\",\n      str_detect(str_to_upper(vehicle_damage_extent), \"DISABLE|DISABLING\") ~ \"DISABLING\",\n      str_detect(str_to_upper(vehicle_damage_extent), \"DESTROYED\") ~ \"DESTROYED\",\n      str_detect(str_to_upper(vehicle_damage_extent), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# clean and standardize vehicle first impact location\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    vehicle_first_impact_location = case_when(\n      str_detect(str_to_upper(vehicle_first_impact_location), \"FRONT|TWELVE|12\") ~ \"FRONT\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"RIGHT FRONT|ONE|1\") ~ \"RIGHT_FRONT\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"RIGHT SIDE|THREE|3\") ~ \"RIGHT_SIDE\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"RIGHT REAR|FOUR|4\") ~ \"RIGHT_REAR\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"REAR|SIX|6\") ~ \"REAR\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"LEFT REAR|SEVEN|7|EIGHT|8\") ~ \"LEFT_REAR\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"LEFT SIDE|NINE|9\") ~ \"LEFT_SIDE\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"LEFT FRONT|TEN|10|ELEVEN|11\") ~ \"LEFT_FRONT\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# clean and standardize vehicle movement\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    vehicle_movement = case_when(\n      str_detect(str_to_upper(vehicle_movement), \"STRAIGHT|CONSTANT\") ~ \"STRAIGHT\",\n      str_detect(str_to_upper(vehicle_movement), \"TURN.*RIGHT\") ~ \"TURNING_RIGHT\",\n      str_detect(str_to_upper(vehicle_movement), \"TURN.*LEFT\") ~ \"TURNING_LEFT\",\n      str_detect(str_to_upper(vehicle_movement), \"STOP|SLOW\") ~ \"SLOWING_STOPPING\",\n      str_detect(str_to_upper(vehicle_movement), \"BACK|REVERSE\") ~ \"BACKING\",\n      str_detect(str_to_upper(vehicle_movement), \"PARK\") ~ \"PARKED\",\n      str_detect(str_to_upper(vehicle_movement), \"START\") ~ \"STARTING\",\n      str_detect(str_to_upper(vehicle_movement), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n\n# create a comprehensive validation function\nvalidate_crash_data &lt;- function(data) {\n  # initialize empty list for validation results\n  validation_results &lt;- list()\n  \n  # temporal validations\n  validation_results$temporal &lt;- data %&gt;%\n    summarise(\n      future_dates = sum(crash_date/time &gt; Sys.time()),\n      weekend_crashes = sum(lubridate::wday(crash_date/time, week_start = 1) %in% c(6,7)),\n      night_crashes = sum(format(crash_date/time, \"%H:%M:%S\") &gt;= \"18:00:00\" | \n                         format(crash_date/time, \"%H:%M:%S\") &lt;= \"06:00:00\"),\n      crashes_by_hour = table(format(crash_date/time, \"%H\"))\n    )\n  \n  # geographic validations\n  validation_results$geographic &lt;- data %&gt;%\n    summarise(\n      invalid_lat = sum(latitude &lt; 38 | latitude &gt; 40, na.rm = TRUE),\n      invalid_lon = sum(longitude &gt; -76 | longitude &lt; -78, na.rm = TRUE),\n      missing_coords = sum(is.na(latitude) | is.na(longitude)),\n      unique_locations = n_distinct(location, na.rm = TRUE)\n    )\n  \n  # vehicle validations\n  validation_results$vehicle &lt;- data %&gt;%\n    summarise(\n      invalid_years = sum(vehicle_year &lt; 1900 | vehicle_year &gt; year(Sys.Date()) + 1, na.rm = TRUE),\n      missing_makes = sum(is.na(vehicle_make)),\n      missing_models = sum(is.na(vehicle_model)),\n      unique_makes = n_distinct(vehicle_make, na.rm = TRUE),\n      unique_models = n_distinct(vehicle_model, na.rm = TRUE)\n    )\n  \n  # logical consistency checks\n  validation_results$logical &lt;- data %&gt;%\n    summarise(\n      # check if parked vehicles are marked as having movement\n      parked_moving = sum(parked_vehicle == \"Yes\" & vehicle_movement != \"PARKED\", na.rm = TRUE),\n      \n      # check for injuries in no-damage crashes\n      injuries_no_damage = sum(injury_severity != \"NO_INJURY\" & \n                             vehicle_damage_extent == \"NO_DAMAGE\", na.rm = TRUE),\n      \n      # check for fatal crashes without severe damage\n      fatal_minor_damage = sum(injury_severity == \"FATAL\" & \n                             vehicle_damage_extent %in% c(\"NO_DAMAGE\", \"MINOR\"), na.rm = TRUE),\n      \n      # check for driverless vehicles marked with driver characteristics\n      driverless_with_driver = sum(driverless_vehicle == \"Yes\" & \n                                  !is.na(driver_substance_abuse), na.rm = TRUE)\n    )\n  \n  # cross-reference checks\n  validation_results$cross_reference &lt;- data %&gt;%\n    summarise(\n      # light condition vs time of day consistency\n      light_time_mismatch = sum(\n        (format(crash_date/time, \"%H:%M:%S\") &gt;= \"18:00:00\" | \n         format(crash_date/time, \"%H:%M:%S\") &lt;= \"06:00:00\") & \n        light == \"DAYLIGHT\", na.rm = TRUE\n      ),\n      \n      # weather vs surface condition consistency\n      weather_surface_mismatch = sum(\n        (weather %in% c(\"RAIN\", \"SNOW\") & surface_condition == \"DRY\") |\n        (weather == \"CLEAR\" & surface_condition %in% c(\"ICE\", \"SNOW\")), \n        na.rm = TRUE\n      )\n    )\n  \n  # value distribution checks\n  validation_results$distributions &lt;- list(\n    speed_distribution = table(data$speed_limit),\n    damage_by_speed = table(data$vehicle_damage_extent, cut(data$speed_limit, \n                                                          breaks = c(0, 25, 35, 45, 55, Inf))),\n    injury_by_speed = table(data$injury_severity, cut(data$speed_limit, \n                                                    breaks = c(0, 25, 35, 45, 55, Inf)))\n  )\n  \n  return(validation_results)\n}\n\n# run validation and create summary report\nvalidation_summary &lt;- validate_crash_data(drivers_data_clean)\n\n# create a function to print validation results in a readable format\nprint_validation_summary &lt;- function(validation_results) {\n  cat(\"\\n=== VALIDATION SUMMARY ===\\n\")\n  \n  cat(\"\\nTEMPORAL VALIDATION:\")\n  cat(\"\\n- Future dates:\", validation_results$temporal$future_dates)\n  cat(\"\\n- Weekend crashes:\", validation_results$temporal$weekend_crashes)\n  cat(\"\\n- Night crashes:\", validation_results$temporal$night_crashes)\n  \n  cat(\"\\n\\nGEOGRAPHIC VALIDATION:\")\n  print(validation_results$geographic)\n  \n  cat(\"\\n\\nVEHICLE VALIDATION:\")\n  print(validation_results$vehicle)\n  \n  cat(\"\\n\\nLOGICAL CONSISTENCY CHECKS:\")\n  print(validation_results$logical)\n  \n  cat(\"\\n\\nCROSS-REFERENCE CHECKS:\")\n  print(validation_results$cross_reference)\n  \n  cat(\"\\n\\nDISTRIBUTIONS:\")\n  cat(\"\\nSpeed Limit Distribution:\\n\")\n  print(validation_results$distributions$speed_distribution)\n  \n  cat(\"\\nDamage by Speed Range:\\n\")\n  print(validation_results$distributions$damage_by_speed)\n}\n\n# run and print validation summary\nprint_validation_summary(validation_summary)\n\n# create data quality flags based on validation results\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    data_quality_flags = case_when(\n      # temporal flags\n      crash_date/time &gt; Sys.time() ~ \"FUTURE_DATE\",\n      \n      # geographic flags\n      (latitude &lt; 38 | latitude &gt; 40 | longitude &gt; -76 | longitude &lt; -78) ~ \"INVALID_COORDINATES\",\n      \n      # vehicle flags\n      (vehicle_year &lt; 1900 | vehicle_year &gt; year(Sys.Date()) + 1) ~ \"INVALID_VEHICLE_YEAR\",\n      \n      # logical consistency flags\n      (parked_vehicle == \"Yes\" & vehicle_movement != \"PARKED\") ~ \"PARKED_MOVING_MISMATCH\",\n      (injury_severity != \"NO_INJURY\" & vehicle_damage_extent == \"NO_DAMAGE\") ~ \"INJURY_NO_DAMAGE_MISMATCH\",\n      (injury_severity == \"FATAL\" & vehicle_damage_extent %in% c(\"NO_DAMAGE\", \"MINOR\")) ~ \"FATAL_MINOR_DAMAGE_MISMATCH\",\n      \n      # cross-reference flags\n      (format(crash_date/time, \"%H:%M:%S\") &gt;= \"18:00:00\" | \n       format(crash_date/time, \"%H:%M:%S\") &lt;= \"06:00:00\") & \n      light == \"DAYLIGHT\" ~ \"LIGHT_TIME_MISMATCH\",\n      \n      (weather %in% c(\"RAIN\", \"SNOW\") & surface_condition == \"DRY\") |\n      (weather == \"CLEAR\" & surface_condition %in% c(\"ICE\", \"SNOW\")) ~ \"WEATHER_SURFACE_MISMATCH\",\n      \n      TRUE ~ \"VALID\"\n    )\n  )\n\nquality_flag_summary &lt;- drivers_data_clean %&gt;%\n  group_by(data_quality_flags) %&gt;%\n  summarise(\n    count = n(),\n    percentage = n() / nrow(drivers_data_clean) * 100\n  ) %&gt;%\n  arrange(desc(count))\n\nprint(\"\\nData Quality Flag Summary:\")\nprint(quality_flag_summary)\n\nSomething still seems off with the data quality flags.\n\n# fix Light-Time Mismatches (largest issue ~5.2% of data)\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # extract hour from crash_date/time\n    crash_hour = as.numeric(format(crash_date/time, \"%H\")),\n    \n    # correct light condition based on time\n    light = case_when(\n      # dawn hours (5-7 AM)\n      crash_hour &gt;= 5 & crash_hour &lt; 7 ~ \"DAWN\",\n      \n      # daylight hours (7 AM - 6 PM)\n      crash_hour &gt;= 7 & crash_hour &lt; 18 ~ \"DAYLIGHT\",\n      \n      # dusk hours (6-8 PM)\n      crash_hour &gt;= 18 & crash_hour &lt; 20 ~ \"DUSK\",\n      \n      # night hours\n      crash_hour &gt;= 20 | crash_hour &lt; 5 ~ \"DARK_WITH_LIGHTING\",\n      \n      TRUE ~ light\n    )\n  ) %&gt;%\n  select(-crash_hour) # remove temporary column\n\n# fix Weather-Surface Condition Mismatches (~0.37% of data)\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # Adjust surface condition based on weather\n    surface_condition = case_when(\n      weather %in% c(\"RAIN\", \"RAINING\") & surface_condition == \"DRY\" ~ \"WET\",\n      weather == \"SNOW\" & surface_condition == \"DRY\" ~ \"SNOW\",\n      weather == \"CLEAR\" & surface_condition %in% c(\"ICE\", \"SNOW\") & \n        format(crash_date/time, \"%m\") %in% c(\"12\", \"01\", \"02\") ~ surface_condition, # Keep if winter months\n      weather == \"CLEAR\" & surface_condition %in% c(\"ICE\", \"SNOW\") ~ \"UNKNOWN\", # Change to unknown if non-winter\n      TRUE ~ surface_condition\n    )\n  )\n\n# fix Injury-Damage Inconsistencies\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # adjust injury severity or damage extent for logical consistency\n    injury_severity = case_when(\n      injury_severity != \"NO_INJURY\" & vehicle_damage_extent == \"NO_DAMAGE\" ~ \"NO_INJURY\",\n      injury_severity == \"FATAL\" & vehicle_damage_extent %in% c(\"NO_DAMAGE\", \"MINOR\") ~ \"SERIOUS_INJURY\",\n      TRUE ~ injury_severity\n    ),\n    \n    # ensure fatal crashes have appropriate damage extent\n    vehicle_damage_extent = case_when(\n      injury_severity == \"FATAL\" & vehicle_damage_extent %in% c(\"NO_DAMAGE\", \"MINOR\") ~ \"DISABLING\",\n      TRUE ~ vehicle_damage_extent\n    )\n  )\n\n# fix Invalid Coordinates (Montgomery County, MD boundaries)\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # montgomery County approximate boundaries\n    latitude = case_when(\n      latitude &lt; 38.9 | latitude &gt; 39.4 ~ NA_real_,\n      TRUE ~ latitude\n    ),\n    longitude = case_when(\n      longitude &lt; -77.5 | longitude &gt; -76.9 ~ NA_real_,\n      TRUE ~ longitude\n    )\n  )\n\n# clean Vehicle Makes and Models with High Frequency Patterns\n# analyze the most common patterns\nvehicle_patterns &lt;- drivers_data_clean %&gt;%\n  group_by(vehicle_make, vehicle_model) %&gt;%\n  summarise(\n    count = n(),\n    .groups = 'drop'\n  ) %&gt;%\n  arrange(desc(count)) %&gt;%\n  head(20)  # Look at top 20 patterns\n\n# now clean based on common patterns\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # standardize common model names\n    vehicle_model = case_when(\n      # remove digits from end of model names\n      str_detect(vehicle_model, \"[A-Z]+\\\\d+$\") ~ str_extract(vehicle_model, \"[A-Z]+\"),\n      # remove common suffixes\n      str_detect(vehicle_model, \"SDN|CPE|CVT|HBK\") ~ str_replace(vehicle_model, \"SDN|CPE|CVT|HBK\", \"\"),\n      TRUE ~ vehicle_model\n    ),\n    \n    # trim whitespace and standardize case\n    vehicle_model = str_trim(vehicle_model),\n    vehicle_make = str_trim(vehicle_make)\n  )\n\n# add Data Quality Score\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    data_quality_score = case_when(\n      data_quality_flags == \"VALID\" ~ 100,\n      data_quality_flags == \"LIGHT_TIME_MISMATCH\" ~ 90,\n      data_quality_flags == \"WEATHER_SURFACE_MISMATCH\" ~ 85,\n      data_quality_flags == \"INJURY_NO_DAMAGE_MISMATCH\" ~ 80,\n      data_quality_flags == \"INVALID_COORDINATES\" ~ 75,\n      data_quality_flags %in% c(\"FATAL_MINOR_DAMAGE_MISMATCH\", \"PARKED_MOVING_MISMATCH\") ~ 70,\n      TRUE ~ 60\n    )\n  )\n\n# verify cleaning results\ncleaning_verification &lt;- list()\n\n# check light-time consistency after cleaning\ncleaning_verification$light_time &lt;- drivers_data_clean %&gt;%\n  summarise(\n    light_time_mismatch_after = sum(\n      (format(crash_date/time, \"%H:%M:%S\") &gt;= \"18:00:00\" | \n       format(crash_date/time, \"%H:%M:%S\") &lt;= \"06:00:00\") & \n      light == \"DAYLIGHT\", \n      na.rm = TRUE\n    )\n  )\n\n# check weather-surface consistency after cleaning\ncleaning_verification$weather_surface &lt;- drivers_data_clean %&gt;%\n  summarise(\n    weather_surface_mismatch_after = sum(\n      (weather %in% c(\"RAIN\", \"SNOW\") & surface_condition == \"DRY\") |\n      (weather == \"CLEAR\" & surface_condition %in% c(\"ICE\", \"SNOW\")), \n      na.rm = TRUE\n    )\n  )\n\n# check data quality scores distribution\ncleaning_verification$quality_scores &lt;- drivers_data_clean %&gt;%\n  group_by(data_quality_score) %&gt;%\n  summarise(\n    count = n(),\n    percentage = n() / nrow(drivers_data_clean) * 100,\n    .groups = 'drop'\n  ) %&gt;%\n  arrange(desc(data_quality_score))\n\ncat(\"\\n=== CLEANING VERIFICATION RESULTS ===\\n\")\nprint(cleaning_verification)\n\nFinal Cleaning Steps Based on Verification Results\n\n# handle remaining weather-surface mismatches (577 cases)\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # create a more detailed weather-surface relationship\n    surface_condition = case_when(\n      # if it's raining, surface should be wet\n      weather %in% c(\"RAIN\", \"RAINING\") ~ \"WET\",\n      \n      # if it's snowing, surface should be snow or ice\n      weather == \"SNOW\" ~ \"SNOW\",\n      \n      # if it's freezing rain, surface should be ice\n      weather == \"FREEZING RAIN\" ~ \"ICE\",\n      \n      # for clear weather, keep existing surface condition if reasonable\n      weather == \"CLEAR\" & surface_condition %in% c(\"DRY\", \"WET\") ~ surface_condition,\n      \n      # for cloudy weather, keep existing surface condition\n      weather == \"CLOUDY\" ~ surface_condition,\n      \n      # for other cases, mark as unknown if inconsistent\n      TRUE ~ case_when(\n        surface_condition %in% c(\"DRY\", \"WET\", \"SNOW\", \"ICE\") ~ surface_condition,\n        TRUE ~ \"UNKNOWN\"\n      )\n    )\n  )\n\n# add more detailed quality flags and improve quality score\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # create compound quality flags for multiple issues\n    detailed_quality_flags = map2_chr(\n      data_quality_flags,\n      surface_condition,\n      ~case_when(\n        .x != \"VALID\" & .y == \"UNKNOWN\" ~ paste(.x, \"WITH_UNKNOWN_SURFACE\"),\n        TRUE ~ .x\n      )\n    ),\n    \n    # create a more nuanced quality score (0-100)\n    refined_quality_score = case_when(\n      data_quality_flags == \"VALID\" & !is.na(surface_condition) & \n      surface_condition != \"UNKNOWN\" ~ 100,\n      \n      data_quality_flags == \"VALID\" & \n      (is.na(surface_condition) | surface_condition == \"UNKNOWN\") ~ 95,\n      \n      data_quality_flags == \"LIGHT_TIME_MISMATCH\" ~ 90,\n      \n      data_quality_flags == \"WEATHER_SURFACE_MISMATCH\" & \n      !is.na(surface_condition) ~ 85,\n      \n      data_quality_flags == \"WEATHER_SURFACE_MISMATCH\" & \n      is.na(surface_condition) ~ 80,\n      \n      data_quality_flags == \"INJURY_NO_DAMAGE_MISMATCH\" ~ 75,\n      \n      data_quality_flags == \"INVALID_COORDINATES\" ~ 70,\n      \n      data_quality_flags %in% c(\"FATAL_MINOR_DAMAGE_MISMATCH\", \n                               \"PARKED_MOVING_MISMATCH\") ~ 65,\n      \n      TRUE ~ 60\n    )\n  )\n\n# create final data quality summary\nfinal_quality_summary &lt;- drivers_data_clean %&gt;%\n  summarise(\n    total_records = n(),\n    high_quality_records = sum(refined_quality_score &gt;= 95),\n    medium_quality_records = sum(refined_quality_score &gt;= 80 & refined_quality_score &lt; 95),\n    low_quality_records = sum(refined_quality_score &lt; 80),\n    mean_quality_score = mean(refined_quality_score),\n    median_quality_score = median(refined_quality_score),\n    \n    # percentage calculations\n    high_quality_percentage = (high_quality_records / total_records) * 100,\n    medium_quality_percentage = (medium_quality_records / total_records) * 100,\n    low_quality_percentage = (low_quality_records / total_records) * 100\n  )\n\ncat(\"\\n=== FINAL DATA QUALITY SUMMARY ===\\n\")\nprint(final_quality_summary)\n\n# create recommendations for further improvements\ncat(\"\\n=== RECOMMENDATIONS FOR FURTHER IMPROVEMENTS ===\\n\")\ncat(\"1. Weather-Surface Condition Relationship:\\n\")\ncat(\"   - \", sum(drivers_data_clean$surface_condition == \"UNKNOWN\"), \n    \"records still have unknown surface conditions\\n\")\ncat(\"   - Consider adding temperature data for better ice/snow validation\\n\\n\")\n\ncat(\"2. Geographic Data Quality:\\n\")\ncat(\"   - \", sum(is.na(drivers_data_clean$latitude) | is.na(drivers_data_clean$longitude)),\n    \"records have missing or invalid coordinates\\n\")\ncat(\"   - Consider implementing address geocoding for missing coordinates\\n\\n\")\n\ncat(\"3. Vehicle Information:\\n\")\ncat(\"   - \", sum(is.na(drivers_data_clean$vehicle_make)), \"records with missing vehicle makes\\n\")\ncat(\"   - \", sum(is.na(drivers_data_clean$vehicle_model)), \"records with missing vehicle models\\n\")\ncat(\"   - Consider implementing VIN decoding for missing vehicle information\\n\\n\")\n\nprocessing_summary &lt;- list(\n  initial_issues = list(\n    light_time_mismatch = 9744,\n    weather_surface_mismatch = 702,\n    injury_damage_mismatch = 306,\n    invalid_coordinates = 26,\n    fatal_minor_damage = 1,\n    parked_moving = 1\n  ),\n  \n  final_status = list(\n    quality_summary = final_quality_summary,\n    remaining_weather_surface_mismatch = 577,\n    remaining_unknown_surface = sum(drivers_data_clean$surface_condition == \"UNKNOWN\", na.rm = TRUE)\n  )\n)\n\ncat(\"\\n=== DATA PROCESSING SUMMARY ===\\n\")\ncat(\"Initial Issues vs. Final Status:\\n\")\ncat(\"- Light-Time Mismatches: \", processing_summary$initial_issues$light_time_mismatch, \n    \" -&gt; 0\\n\")\ncat(\"- Weather-Surface Mismatches: \", processing_summary$initial_issues$weather_surface_mismatch,\n    \" -&gt; \", processing_summary$final_status$remaining_weather_surface_mismatch, \"\\n\")\n\nVisualizations.\n\ninstall.packages(\"viridis\")\n\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(patchwork)  # for combining plots\nlibrary(viridis)    # for colorblind-friendly palettes\nlibrary(lubridate)\n\n# crash Time Patterns\ntime_plot &lt;- drivers_data_clean %&gt;%\n  mutate(\n    hour = hour(crash_date/time),\n    weekday = wday(crash_date/time, label = TRUE),\n    month = month(crash_date/time, label = TRUE)\n  ) %&gt;%\n  ggplot(aes(x = hour)) +\n  geom_histogram(binwidth = 1, fill = \"steelblue\", color = \"white\") +\n  labs(title = \"Distribution of Crashes by Hour of Day\",\n       x = \"Hour of Day\",\n       y = \"Number of Crashes\") +\n  theme_minimal() +\n  scale_x_continuous(breaks = seq(0, 23, 2))\n\n# injury Severity by Vehicle Type\nseverity_vehicle_plot &lt;- drivers_data_clean %&gt;%\n  filter(!is.na(vehicle_body_type), !is.na(injury_severity)) %&gt;%\n  count(vehicle_body_type, injury_severity) %&gt;%\n  group_by(vehicle_body_type) %&gt;%\n  mutate(pct = n/sum(n) * 100) %&gt;%\n  ggplot(aes(x = reorder(vehicle_body_type, -pct), y = pct, fill = injury_severity)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(title = \"Injury Severity by Vehicle Type\",\n       x = \"Vehicle Type\",\n       y = \"Percentage\",\n       fill = \"Injury Severity\") +\n  theme_minimal() +\n  scale_fill_viridis_d()\n\n# weather Conditions and Crash Frequency\nweather_plot &lt;- drivers_data_clean %&gt;%\n  count(weather) %&gt;%\n  ggplot(aes(x = reorder(weather, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"darkgreen\") +\n  coord_flip() +\n  labs(title = \"Crashes by Weather Condition\",\n       x = \"Weather Condition\",\n       y = \"Number of Crashes\") +\n  theme_minimal()\n\n# geographic Distribution of Crashes\nmap_plot &lt;- drivers_data_clean %&gt;%\n  filter(!is.na(latitude), !is.na(longitude)) %&gt;%\n  ggplot(aes(x = longitude, y = latitude)) +\n  geom_point(alpha = 0.1, color = \"red\") +\n  labs(title = \"Geographic Distribution of Crashes\",\n       x = \"Longitude\",\n       y = \"Latitude\") +\n  theme_minimal()\n\n# collision Types\ncollision_plot &lt;- drivers_data_clean %&gt;%\n  count(collision_type) %&gt;%\n  mutate(pct = n/sum(n) * 100) %&gt;%\n  ggplot(aes(x = reorder(collision_type, -pct), y = pct)) +\n  geom_bar(stat = \"identity\", fill = \"orange\") +\n  coord_flip() +\n  labs(title = \"Distribution of Collision Types\",\n       x = \"Collision Type\",\n       y = \"Percentage\") +\n  theme_minimal()\n\n# speed Limit and Crash Severity\nspeed_severity_plot &lt;- drivers_data_clean %&gt;%\n  filter(!is.na(speed_limit), !is.na(injury_severity)) %&gt;%\n  ggplot(aes(x = factor(speed_limit), fill = injury_severity)) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"Injury Severity by Speed Limit\",\n       x = \"Speed Limit\",\n       y = \"Proportion\",\n       fill = \"Injury Severity\") +\n  theme_minimal() +\n  scale_fill_viridis_d()\n\n# vehicle Age Distribution\nvehicle_age_plot &lt;- drivers_data_clean %&gt;%\n  mutate(vehicle_age = year(crash_date/time) - vehicle_year) %&gt;%\n  filter(vehicle_age &gt;= 0, vehicle_age &lt;= 30) %&gt;%\n  ggplot(aes(x = vehicle_age)) +\n  geom_histogram(binwidth = 1, fill = \"purple\", color = \"white\") +\n  labs(title = \"Distribution of Vehicle Age\",\n       x = \"Vehicle Age (Years)\",\n       y = \"Count\") +\n  theme_minimal()\n\n# surface Condition and Weather Relationship\nsurface_weather_plot &lt;- drivers_data_clean %&gt;%\n  filter(!is.na(surface_condition), !is.na(weather)) %&gt;%\n  count(surface_condition, weather) %&gt;%\n  group_by(weather) %&gt;%\n  mutate(pct = n/sum(n) * 100) %&gt;%\n  ggplot(aes(x = weather, y = surface_condition, fill = pct)) +\n  geom_tile() +\n  scale_fill_viridis_c() +\n  labs(title = \"Surface Condition by Weather\",\n       x = \"Weather\",\n       y = \"Surface Condition\",\n       fill = \"Percentage\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n# combine plots using patchwork\nlayout &lt;- \"\nAABB\nCCDD\nEEFF\nGGHH\n\"\n\ncombined_plots &lt;- time_plot + severity_vehicle_plot + \n                 weather_plot + map_plot + \n                 collision_plot + speed_severity_plot + \n                 vehicle_age_plot + surface_weather_plot +\n                 plot_layout(design = layout)\n\nprint(combined_plots)\n\nggsave(\"crash_analysis_dashboard.pdf\", combined_plots, width = 20, height = 24)\n\nsummary_stats &lt;- list(\n  \n  time_stats = drivers_data_clean %&gt;%\n    mutate(\n      hour = hour(crash_date/time),\n      weekday = wday(crash_date/time, label = TRUE)\n    ) %&gt;%\n    summarise(\n      peak_hour = names(which.max(table(hour))),\n      weekend_crashes = mean(weekday %in% c(\"Sat\", \"Sun\")) * 100\n    ),\n  \n  severity_stats = drivers_data_clean %&gt;%\n    group_by(injury_severity) %&gt;%\n    summarise(\n      count = n(),\n      percentage = n() / nrow(drivers_data_clean) * 100\n    ),\n  \n  weather_stats = drivers_data_clean %&gt;%\n    group_by(weather) %&gt;%\n    summarise(\n      count = n(),\n      percentage = n() / nrow(drivers_data_clean) * 100\n    ),\n  \n  vehicle_stats = drivers_data_clean %&gt;%\n    summarise(\n      avg_vehicle_age = mean(year(crash_date/time) - vehicle_year, na.rm = TRUE),\n      most_common_make = names(which.max(table(vehicle_make)))\n    )\n)\n\nprint(summary_stats)\n\nSave the final cleaned dataset.\n\ncurrent_timestamp &lt;- format(Sys.time(), \"%Y%m%d_%H%M\")\n\n# create directories if they don't exist\ndir.create(\"cleaned_data\", showWarnings = FALSE)\ndir.create(\"data_documentation\", showWarnings = FALSE)\n\nstr_result &lt;- capture.output(str(drivers_data_clean))\nprint(\"Data structure:\")\nprint(str_result)\n\ncolumn_info &lt;- data.frame(\n  column_name = names(drivers_data_clean),\n  data_type = sapply(drivers_data_clean, function(x) class(x)[1]),  # Take first class if multiple\n  stringsAsFactors = FALSE\n)\n\ncolumn_info$description &lt;- sapply(column_info$column_name, function(col_name) {\n  case_when(\n    col_name == \"report_number\" ~ \"Unique report identifier\",\n    col_name == \"local_case_number\" ~ \"Local case number for the incident\",\n    col_name == \"agency_name\" ~ \"Name of reporting agency\",\n    col_name == \"acrs_report_type\" ~ \"Type of crash report\",\n    col_name == \"crash_date/time\" ~ \"Date and time of the crash\",\n    col_name == \"route_type\" ~ \"Type of route where crash occurred\",\n    col_name == \"road_name\" ~ \"Name of the road where crash occurred\",\n    col_name == \"cross_street_name\" ~ \"Name of the nearest cross-street\",\n    col_name == \"off_road_description\" ~ \"Description for off-road incidents\",\n    col_name == \"municipality\" ~ \"Municipality where crash occurred\",\n    col_name == \"related_non_motorist\" ~ \"Type of non-motorist involved\",\n    col_name == \"collision_type\" ~ \"Type of collision\",\n    col_name == \"weather\" ~ \"Weather conditions during crash\",\n    col_name == \"surface_condition\" ~ \"Road surface condition\",\n    col_name == \"light\" ~ \"Light conditions\",\n    col_name == \"traffic_control\" ~ \"Traffic control present\",\n    col_name == \"driver_substance_abuse\" ~ \"Driver substance abuse status\",\n    col_name == \"non_motorist_substance_abuse\" ~ \"Non-motorist substance abuse status\",\n    col_name == \"person_id\" ~ \"Unique identifier for person involved\",\n    col_name == \"driver_at_fault\" ~ \"Indicator if driver was at fault\",\n    col_name == \"injury_severity\" ~ \"Severity of injuries\",\n    col_name == \"circumstance\" ~ \"Contributing circumstances\",\n    col_name == \"driver_distracted_by\" ~ \"Driver distraction factors\",\n    col_name == \"drivers_license_state\" ~ \"State of driver's license\",\n    col_name == \"vehicle_id\" ~ \"Unique identifier for vehicle\",\n    col_name == \"vehicle_damage_extent\" ~ \"Extent of vehicle damage\",\n    col_name == \"vehicle_first_impact_location\" ~ \"Location of first impact on vehicle\",\n    col_name == \"vehicle_body_type\" ~ \"Type of vehicle body\",\n    col_name == \"vehicle_movement\" ~ \"Vehicle movement during crash\",\n    col_name == \"vehicle_going_dir\" ~ \"Direction vehicle was traveling\",\n    col_name == \"speed_limit\" ~ \"Posted speed limit\",\n    col_name == \"driverless_vehicle\" ~ \"Indicator if vehicle was driverless\",\n    col_name == \"parked_vehicle\" ~ \"Indicator if vehicle was parked\",\n    col_name == \"vehicle_year\" ~ \"Year of vehicle manufacture\",\n    col_name == \"vehicle_make\" ~ \"Vehicle manufacturer\",\n    col_name == \"vehicle_model\" ~ \"Vehicle model\",\n    col_name == \"latitude\" ~ \"Latitude of crash location\",\n    col_name == \"longitude\" ~ \"Longitude of crash location\",\n    col_name == \"location\" ~ \"Combined location coordinates\",\n    col_name == \"data_quality_flags\" ~ \"Data quality flags from cleaning process\",\n    col_name == \"detailed_quality_flags\" ~ \"Detailed quality flags from cleaning process\",\n    col_name == \"refined_quality_score\" ~ \"Numerical score indicating data quality\",\n    TRUE ~ paste(\"Description for\", col_name)  # Default description for any new columns\n  )\n})\n\ncolumn_info$example_values &lt;- sapply(drivers_data_clean, function(x) {\n  if (is.numeric(x)) {\n    paste(\"Range:\", min(x, na.rm = TRUE), \"to\", max(x, na.rm = TRUE))\n  } else {\n    unique_vals &lt;- unique(na.omit(x))\n    if (length(unique_vals) &gt; 5) {\n      paste(paste(unique_vals[1:5], collapse = \", \"), \"... and\", length(unique_vals) - 5, \"more values\")\n    } else {\n      paste(unique_vals, collapse = \", \")\n    }\n  }\n})\n\nwrite_csv(drivers_data_clean, \n          file = paste0(\"cleaned_data/crash_data_clean_\", current_timestamp, \".csv\"))\n\nsaveRDS(drivers_data_clean, \n        file = paste0(\"cleaned_data/crash_data_clean_\", current_timestamp, \".rds\"))\n\nwrite_csv(column_info, \n          file = paste0(\"data_documentation/data_dictionary_\", current_timestamp, \".csv\"))\n\nreadme_text &lt;- sprintf(\"\n# Crash Data Cleaning Documentation\n\n## Version Information\n- Date Created: %s\n- Original Records: 186,931\n- Final Records: %d\n\n## Data Quality Metrics\n- Total Records: %d\n- Records with Quality Flags: %d\n- Clean Records: %d\n\n## Files in this Package\n1. crash_data_clean_%s.csv - Main data file (CSV format)\n2. crash_data_clean_%s.rds - R data file (RDS format)\n3. data_dictionary_%s.csv - Data dictionary with column descriptions\n\n## Column Summary\nTotal number of columns: %d\nSee data dictionary file for detailed information about each column.\n\n## Data Quality Notes\n- Data has been cleaned and standardized\n- Quality flags have been added to mark potential issues\n- Missing values have been handled according to context\n- Inconsistent categories have been standardized\n\n## Usage Notes\n- Please refer to the data dictionary for column descriptions\n- Check quality flags before analysis\n- Some columns may contain standardized values\n\n## Contact\nFor questions about this dataset, please contact [Your Contact Information]\n\n## Last Updated\n%s\n\",\n    format(Sys.time(), \"%Y-%m-%d %H:%M:%S\"),\n    nrow(drivers_data_clean),\n    nrow(drivers_data_clean),\n    sum(drivers_data_clean$data_quality_flags != \"VALID\"),\n    sum(drivers_data_clean$data_quality_flags == \"VALID\"),\n    current_timestamp,\n    current_timestamp,\n    current_timestamp,\n    ncol(drivers_data_clean),\n    format(Sys.time(), \"%Y-%m-%d %H:%M:%S\")\n)\n\nwriteLines(readme_text, \n           paste0(\"data_documentation/README_\", current_timestamp, \".md\"))\n\nzip_files &lt;- c(\n    paste0(\"cleaned_data/crash_data_clean_\", current_timestamp, \".csv\"),\n    paste0(\"cleaned_data/crash_data_clean_\", current_timestamp, \".rds\"),\n    paste0(\"data_documentation/data_dictionary_\", current_timestamp, \".csv\"),\n    paste0(\"data_documentation/README_\", current_timestamp, \".md\")\n)\n\nzip(paste0(\"cleaned_data/crash_data_package_\", current_timestamp, \".zip\"),\n    files = zip_files)\n\ncat(\"\\nData saving complete!\\n\")\ncat(\"Files saved:\\n\")\ncat(\"1. CSV data file\\n\")\ncat(\"2. RDS data file\\n\")\ncat(\"3. Data dictionary\\n\")\ncat(\"4. README documentation\\n\")\ncat(\"5. Complete package (zip)\\n\")\ncat(\"\\nLocation: ./cleaned_data/ and ./data_documentation/\\n\")"
  },
  {
    "objectID": "dataset/Cleaning_Xiang.html#setup",
    "href": "dataset/Cleaning_Xiang.html#setup",
    "title": "crash car data clean",
    "section": "",
    "text": "options(repos = c(CRAN = \"https://cran.r-project.org\"))\ninstall.packages(\"skimr\")\n\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(lubridate)\nlibrary(skimr)  # for quick summary statistics\n\n\ninstall.packages(\"RCurl\")\nlibrary(RCurl)\nx &lt;- getURL(\"https://raw.githubusercontent.com/sussmanbu/ma-4615-fa24-final-project-group-1/main/Crash_Reporting_Drivers_Data.csv\")\ndrivers_data &lt;- read.csv(text = x)\n\n\nglimpse(drivers_data)\n\nskim(drivers_data)\n\nClean Column Names\n\ndrivers_data_clean &lt;- drivers_data %&gt;%\n  clean_names()\n\ncolnames(drivers_data_clean)\n\nHandle Date/Time Format\n\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    crash_date/time &lt;- mdy_hms(crash_date/time),\n    # create separate date and time columns if needed\n    crash_date = date(crash_date/time),\n    crash_time = format(crash_date/time, \"%H:%M:%S\")\n  )\n\nStandardize Categorical Variables\n\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # standardize agency names\n    agency_name = str_trim(agency_name),\n    \n    # clean and standardize route type\n    route_type = case_when(\n      is.na(route_type) ~ \"Unknown\",\n      TRUE ~ route_type\n    ),\n    \n    # standardize weather conditions\n    weather = str_to_title(weather),\n    \n    # clean surface condition\n    surface_condition = case_when(\n      is.na(surface_condition) ~ \"Unknown\",\n      TRUE ~ surface_condition\n    ),\n    \n    # standardize driver at fault\n    driver_at_fault = case_when(\n      str_to_lower(driver_at_fault) == \"yes\" ~ \"Yes\",\n      str_to_lower(driver_at_fault) == \"no\" ~ \"No\",\n      TRUE ~ \"Unknown\"\n    )\n  )\n\nHandle Missing Values\n\n# calculate missing values percentage\nmissing_values &lt;- drivers_data_clean %&gt;%\n  summarise(across(everything(), ~sum(is.na(.))/n()*100)) %&gt;%\n  pivot_longer(everything(), \n              names_to = \"column\", \n              values_to = \"missing_percentage\") %&gt;%\n  arrange(desc(missing_percentage))\n\n# display columns with missing values\nprint(missing_values %&gt;% filter(missing_percentage &gt; 0))\n\n# handle missing values based on column type\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # for route_type, road_name, and cross_street_name\n    # keep NA as is since they might be meaningful (e.g., off-road incidents)\n    \n    # for numeric columns, consider if 0 or NA is more appropriate\n    speed_limit = if_else(is.na(speed_limit), 0, speed_limit),\n    \n    # for categorical columns, mark unknown\n    surface_condition = if_else(is.na(surface_condition), \"Unknown\", surface_condition),\n    traffic_control = if_else(is.na(traffic_control), \"Unknown\", traffic_control)\n  )\n\nData Validation and Consistency Checks\n\n# check for logical consistencies\nvalidation_results &lt;- drivers_data_clean %&gt;%\n  summarise(\n    # check for future dates\n    future_dates = sum(crash_date &gt; Sys.Date()),\n    \n    # check for valid speed limits\n    invalid_speed = sum(speed_limit &gt; 70 | speed_limit &lt; 0, na.rm = TRUE),\n    \n    # check for valid vehicle years\n    invalid_vehicle_year = sum(vehicle_year &lt; 1900 | vehicle_year &gt; year(Sys.Date()) + 1, na.rm = TRUE),\n    \n    # check for valid coordinates\n    invalid_lat = sum(latitude &lt; 38 | latitude &gt; 40, na.rm = TRUE),\n    invalid_lon = sum(longitude &gt; -76 | longitude &lt; -78, na.rm = TRUE)\n  )\n\nprint(validation_results)\n\n# create flags for potential data quality issues\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    data_quality_flag = case_when(\n      crash_date &gt; Sys.Date() ~ \"Future date\",\n      speed_limit &gt; 70 | speed_limit &lt; 0 ~ \"Invalid speed limit\",\n      vehicle_year &lt; 1900 | vehicle_year &gt; year(Sys.Date()) + 1 ~ \"Invalid vehicle year\",\n      latitude &lt; 38 | latitude &gt; 40 | longitude &gt; -76 | longitude &lt; -78 ~ \"Invalid coordinates\",\n      TRUE ~ \"Valid\"\n    )\n  )\n\nCheck Value Distributions\n\n# check common categories\ncategory_summaries &lt;- list(\n  collision_types = table(drivers_data_clean$collision_type),\n  weather_conditions = table(drivers_data_clean$weather),\n  vehicle_types = table(drivers_data_clean$vehicle_body_type),\n  injury_severity = table(drivers_data_clean$injury_severity)\n)\n\nprint(category_summaries)\n\nCollision Types Need Standardization:\n\n# standardize collision types\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    collision_type = case_when(\n      collision_type %in% c(\"Angle\", \"ANGLE MEETS LEFT HEAD ON\", \"ANGLE MEETS LEFT TURN\",\n                           \"ANGLE MEETS RIGHT TURN\", \"STRAIGHT MOVEMENT ANGLE\") ~ \"ANGLE\",\n      collision_type %in% c(\"Front to Front\", \"HEAD ON\", \"HEAD ON LEFT TURN\") ~ \"HEAD_ON\",\n      collision_type %in% c(\"Front to Rear\", \"SAME DIR REAR END\", \n                           \"SAME DIR REND LEFT TURN\", \"SAME DIR REND RIGHT TURN\") ~ \"REAR_END\",\n      collision_type %in% c(\"SAME DIRECTION SIDESWIPE\", \"Sideswipe, Same Direction\") ~ \"SIDESWIPE_SAME_DIR\",\n      collision_type %in% c(\"OPPOSITE DIRECTION SIDESWIPE\", \"Sideswipe, Opposite Direction\") ~ \"SIDESWIPE_OPPOSITE_DIR\",\n      collision_type %in% c(\"SINGLE VEHICLE\", \"Single Vehicle\") ~ \"SINGLE_VEHICLE\",\n      collision_type %in% c(\"Other\", \"OTHER\") ~ \"OTHER\",\n      collision_type %in% c(\"Unknown\", \"UNKNOWN\", \"N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\nWeather Conditions Need Standardization:\n\n# standardize weather conditions\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    weather = case_when(\n      str_detect(str_to_upper(weather), \"CLEAR\") ~ \"CLEAR\",\n      str_detect(str_to_upper(weather), \"CLOUD\") ~ \"CLOUDY\",\n      str_detect(str_to_upper(weather), \"RAIN|RAINING\") ~ \"RAIN\",\n      str_detect(str_to_upper(weather), \"SNOW|BLOWING SNOW\") ~ \"SNOW\",\n      str_detect(str_to_upper(weather), \"FOG|SMOG|SMOKE\") ~ \"FOG\",\n      weather %in% c(\"N/A\", \"Unknown\", \"UNKNOWN\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\nVehicle Types Need Major Cleanup:\n\n# standardize vehicle types\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    vehicle_body_type = case_when(\n      str_detect(str_to_upper(vehicle_body_type), \"PASSENGER CAR|PASSENGER|CAR\") ~ \"PASSENGER_CAR\",\n      str_detect(str_to_upper(vehicle_body_type), \"SUV|SPORT UTILITY|UTILITY VEHICLE\") ~ \"SUV\",\n      str_detect(str_to_upper(vehicle_body_type), \"PICKUP|LIGHT TRUCK\") ~ \"PICKUP_TRUCK\",\n      str_detect(str_to_upper(vehicle_body_type), \"VAN|CARGO\") ~ \"VAN\",\n      str_detect(str_to_upper(vehicle_body_type), \"BUS\") ~ \"BUS\",\n      str_detect(str_to_upper(vehicle_body_type), \"MOTORCYCLE|MOPED\") ~ \"MOTORCYCLE\",\n      str_detect(str_to_upper(vehicle_body_type), \"EMERGENCY|POLICE|FIRE|AMBULANCE\") ~ \"EMERGENCY_VEHICLE\",\n      vehicle_body_type %in% c(\"N/A\", \"Unknown\", \"UNKNOWN\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\nInjury Severity Standardization:\n\n# standardize injury severity\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    injury_severity = case_when(\n      str_detect(str_to_upper(injury_severity), \"FATAL\") ~ \"FATAL\",\n      str_detect(str_to_upper(injury_severity), \"NO APPARENT|NO INJURY\") ~ \"NO_INJURY\",\n      str_detect(str_to_upper(injury_severity), \"POSSIBLE\") ~ \"POSSIBLE_INJURY\",\n      str_detect(str_to_upper(injury_severity), \"MINOR\") ~ \"MINOR_INJURY\",\n      str_detect(str_to_upper(injury_severity), \"SERIOUS\") ~ \"SERIOUS_INJURY\",\n      TRUE ~ \"UNKNOWN\"\n    )\n  )\n\nHandle Missing Values Strategy (based on the missing percentage analysis):\n\n# handle missing values based on context\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # high missing percentage (&gt;90%) - keep as NA but add flag\n    has_non_motorist = !is.na(related_non_motorist),\n    \n    # medium missing percentage (10-90%) - add meaningful categories\n    municipality = if_else(is.na(municipality), \"UNINCORPORATED\", municipality),\n    road_name = if_else(is.na(road_name), \"OFF_ROAD\", road_name),\n    cross_street_name = if_else(is.na(cross_street_name), \"NOT_APPLICABLE\", cross_street_name),\n    \n    # low missing percentage (&lt;10%) - impute with \"UNKNOWN\"\n    drivers_license_state = if_else(is.na(drivers_license_state), \"UNKNOWN\", drivers_license_state),\n    circumstance = if_else(is.na(circumstance), \"UNKNOWN\", circumstance),\n    vehicle_going_dir = if_else(is.na(vehicle_going_dir), \"UNKNOWN\", vehicle_going_dir)\n  )\n\nHandle Data Quality Issues (based on validation results):\n\n# add data quality flags\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    data_quality_flags = case_when(\n      speed_limit &gt; 70 | speed_limit &lt; 0 ~ \"INVALID_SPEED_LIMIT\",\n      vehicle_year &lt; 1900 | vehicle_year &gt; year(Sys.Date()) + 1 ~ \"INVALID_VEHICLE_YEAR\",\n      latitude &lt; 38 | latitude &gt; 40 | longitude &gt; -76 | longitude &lt; -78 ~ \"INVALID_COORDINATES\",\n      TRUE ~ \"VALID\"\n    ),\n    \n    # clean speed limits\n    speed_limit = case_when(\n      speed_limit &gt; 70 ~ NA_real_,\n      speed_limit &lt; 0 ~ NA_real_,\n      TRUE ~ speed_limit\n    ),\n    \n    # clean vehicle years\n    vehicle_year = if_else(\n      vehicle_year &lt; 1900 | vehicle_year &gt; year(Sys.Date()) + 1,\n      NA_real_,\n      vehicle_year\n    )\n  )\n\nAdditional cleaning for specific columns:\n\n# additional cleaning steps for specific columns and vehicle data\n\n# standardize vehicle makes (common misspellings and abbreviations)\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    vehicle_make = case_when(\n      # honda variations\n      str_detect(str_to_upper(vehicle_make), \"HOND|HDA\") ~ \"HONDA\",\n      \n      # toyota variations\n      str_detect(str_to_upper(vehicle_make), \"TOY|TOYT\") ~ \"TOYOTA\",\n      \n      # ford\n      str_detect(str_to_upper(vehicle_make), \"^FRD|FORD\") ~ \"FORD\",\n      \n      # chevrolet variations\n      str_detect(str_to_upper(vehicle_make), \"CHEV|CHEVY|CHV\") ~ \"CHEVROLET\",\n      \n      # nissan variations\n      str_detect(str_to_upper(vehicle_make), \"NISS|NISN\") ~ \"NISSAN\",\n      \n      # hyundai variations\n      str_detect(str_to_upper(vehicle_make), \"HYUN|HYU\") ~ \"HYUNDAI\",\n      \n      # volkswagen variations\n      str_detect(str_to_upper(vehicle_make), \"VW|VOLK|VOLKS\") ~ \"VOLKSWAGEN\",\n      \n      # BMW\n      str_detect(str_to_upper(vehicle_make), \"BMW|BMV\") ~ \"BMW\",\n      \n      # Mercedes-Benz variations\n      str_detect(str_to_upper(vehicle_make), \"MERZ|MENZ|MERCEDES|BENZ\") ~ \"MERCEDES-BENZ\",\n      \n      # lexus\n      str_detect(str_to_upper(vehicle_make), \"LEX|LEXS\") ~ \"LEXUS\",\n      \n      # mazda variations\n      str_detect(str_to_upper(vehicle_make), \"MAZ|MAZD\") ~ \"MAZDA\",\n      \n      # subaru variations\n      str_detect(str_to_upper(vehicle_make), \"SUB|SUBR\") ~ \"SUBARU\",\n      \n      # kia\n      str_detect(str_to_upper(vehicle_make), \"^KIA\") ~ \"KIA\",\n      \n      # audi\n      str_detect(str_to_upper(vehicle_make), \"^AUD\") ~ \"AUDI\",\n      \n      # acura\n      str_detect(str_to_upper(vehicle_make), \"ACUR|ACU\") ~ \"ACURA\",\n      \n      # infinity\n      str_detect(str_to_upper(vehicle_make), \"INF|INFIN\") ~ \"INFINITI\",\n      \n      TRUE ~ str_to_upper(vehicle_make)\n    )\n  )\n\n# clean vehicle models\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    vehicle_model = case_when(\n      # clean common abbreviations\n      str_detect(str_to_upper(vehicle_model), \"^CRV|CR-V\") ~ \"CR-V\",\n      str_detect(str_to_upper(vehicle_model), \"^RAV|RAV4\") ~ \"RAV4\",\n      str_detect(str_to_upper(vehicle_model), \"ACCORD|ACRD\") ~ \"ACCORD\",\n      str_detect(str_to_upper(vehicle_model), \"CAMRY|CAM\") ~ \"CAMRY\",\n      str_detect(str_to_upper(vehicle_model), \"CIVIC|CVC\") ~ \"CIVIC\",\n      str_detect(str_to_upper(vehicle_model), \"ALTIMA|ALT\") ~ \"ALTIMA\",\n      str_detect(str_to_upper(vehicle_model), \"COROLLA|COR\") ~ \"COROLLA\",\n      str_detect(str_to_upper(vehicle_model), \"EXPLORER|EXPLR\") ~ \"EXPLORER\",\n      str_detect(str_to_upper(vehicle_model), \"F-150|F150\") ~ \"F-150\",\n      str_detect(str_to_upper(vehicle_model), \"HIGHLANDER|HGLDR\") ~ \"HIGHLANDER\",\n      TRUE ~ str_to_upper(vehicle_model)\n    )\n  )\n\n# additional cleaning for other specific columns\n\n# clean light conditions\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    light = case_when(\n      str_detect(str_to_upper(light), \"DAYLIGHT\") ~ \"DAYLIGHT\",\n      str_detect(str_to_upper(light), \"DARK.*LIGHT.*ON|LIGHTED\") ~ \"DARK_WITH_LIGHTING\",\n      str_detect(str_to_upper(light), \"DARK.*NO.*LIGHT|UNLIGHTED\") ~ \"DARK_NO_LIGHTING\",\n      str_detect(str_to_upper(light), \"DAWN\") ~ \"DAWN\",\n      str_detect(str_to_upper(light), \"DUSK\") ~ \"DUSK\",\n      str_detect(str_to_upper(light), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# clean surface conditions\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    surface_condition = case_when(\n      str_detect(str_to_upper(surface_condition), \"DRY\") ~ \"DRY\",\n      str_detect(str_to_upper(surface_condition), \"WET\") ~ \"WET\",\n      str_detect(str_to_upper(surface_condition), \"ICE|ICY\") ~ \"ICE\",\n      str_detect(str_to_upper(surface_condition), \"SNOW|SLUSH\") ~ \"SNOW\",\n      str_detect(str_to_upper(surface_condition), \"SAND|DIRT|MUD\") ~ \"SAND_DIRT_MUD\",\n      str_detect(str_to_upper(surface_condition), \"OIL|GREASE\") ~ \"OIL_GREASE\",\n      is.na(surface_condition) | str_detect(str_to_upper(surface_condition), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# clean traffic control\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    traffic_control = case_when(\n      str_detect(str_to_upper(traffic_control), \"SIGNAL|TRAFFIC LIGHT\") ~ \"TRAFFIC_SIGNAL\",\n      str_detect(str_to_upper(traffic_control), \"STOP SIGN\") ~ \"STOP_SIGN\",\n      str_detect(str_to_upper(traffic_control), \"YIELD\") ~ \"YIELD_SIGN\",\n      str_detect(str_to_upper(traffic_control), \"FLASHING\") ~ \"FLASHING_SIGNAL\",\n      str_detect(str_to_upper(traffic_control), \"NO CONTROL|NONE\") ~ \"NO_CONTROL\",\n      str_detect(str_to_upper(traffic_control), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# clean driver substance abuse\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    driver_substance_abuse = case_when(\n      str_detect(str_to_upper(driver_substance_abuse), \"NONE|NO ABUSE\") ~ \"NONE\",\n      str_detect(str_to_upper(driver_substance_abuse), \"ALCOHOL\") ~ \"ALCOHOL\",\n      str_detect(str_to_upper(driver_substance_abuse), \"DRUG\") ~ \"DRUGS\",\n      str_detect(str_to_upper(driver_substance_abuse), \"BOTH|ALCOHOL.*DRUG|DRUG.*ALCOHOL\") ~ \"ALCOHOL_AND_DRUGS\",\n      str_detect(str_to_upper(driver_substance_abuse), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# clean driver distracted by\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    driver_distracted_by = case_when(\n      str_detect(str_to_upper(driver_distracted_by), \"NOT DISTRACTED\") ~ \"NOT_DISTRACTED\",\n      str_detect(str_to_upper(driver_distracted_by), \"CELL|PHONE|MOBILE\") ~ \"CELL_PHONE\",\n      str_detect(str_to_upper(driver_distracted_by), \"PASSENGER\") ~ \"PASSENGER\",\n      str_detect(str_to_upper(driver_distracted_by), \"RADIO|AUDIO\") ~ \"AUDIO_EQUIPMENT\",\n      str_detect(str_to_upper(driver_distracted_by), \"EAT|DRINK\") ~ \"EATING_DRINKING\",\n      str_detect(str_to_upper(driver_distracted_by), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# verify our cleaning by checking the unique values in each cleaned column\nverification_check &lt;- function(data, columns) {\n  map(columns, ~{\n    unique_vals &lt;- data %&gt;% \n      pull(.) %&gt;% \n      unique() %&gt;% \n      sort()\n    \n    cat(\"\\nUnique values in\", ., \":\\n\")\n    print(unique_vals)\n    cat(\"\\n\")\n  })\n}\n\n# verify the cleaning results for key columns\ncolumns_to_verify &lt;- c(\n  \"vehicle_make\", \n  \"vehicle_model\",\n  \"light\",\n  \"surface_condition\",\n  \"traffic_control\",\n  \"driver_substance_abuse\",\n  \"driver_distracted_by\"\n)\n\nverification_check(drivers_data_clean, columns_to_verify)\n\nvehicle_summary &lt;- drivers_data_clean %&gt;%\n  group_by(vehicle_make, vehicle_model) %&gt;%\n  summarise(\n    count = n(),\n    .groups = 'drop'\n  ) %&gt;%\n  arrange(desc(count))\n\nprint(vehicle_summary)\n\n\n# clean and standardize agency names\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    agency_name = case_when(\n      str_detect(str_to_upper(agency_name), \"MONTGOMERY|MONT|MCP\") ~ \"MONTGOMERY COUNTY POLICE\",\n      str_detect(str_to_upper(agency_name), \"GAITHERSBURG|GAITH\") ~ \"GAITHERSBURG POLICE\",\n      str_detect(str_to_upper(agency_name), \"ROCKVILLE|ROCK\") ~ \"ROCKVILLE POLICE\",\n      str_detect(str_to_upper(agency_name), \"TAKOMA|TAK\") ~ \"TAKOMA PARK POLICE\",\n      str_detect(str_to_upper(agency_name), \"PARK|MNCPP\") ~ \"MD NATIONAL CAPITAL PARK POLICE\",\n      TRUE ~ str_to_upper(agency_name)\n    )\n  )\n\n# clean and standardize vehicle damage extent\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    vehicle_damage_extent = case_when(\n      str_detect(str_to_upper(vehicle_damage_extent), \"NONE|NO DAMAGE\") ~ \"NO_DAMAGE\",\n      str_detect(str_to_upper(vehicle_damage_extent), \"MINOR|SUPERFICIAL\") ~ \"MINOR\",\n      str_detect(str_to_upper(vehicle_damage_extent), \"FUNCTIONAL\") ~ \"FUNCTIONAL\",\n      str_detect(str_to_upper(vehicle_damage_extent), \"DISABLE|DISABLING\") ~ \"DISABLING\",\n      str_detect(str_to_upper(vehicle_damage_extent), \"DESTROYED\") ~ \"DESTROYED\",\n      str_detect(str_to_upper(vehicle_damage_extent), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# clean and standardize vehicle first impact location\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    vehicle_first_impact_location = case_when(\n      str_detect(str_to_upper(vehicle_first_impact_location), \"FRONT|TWELVE|12\") ~ \"FRONT\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"RIGHT FRONT|ONE|1\") ~ \"RIGHT_FRONT\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"RIGHT SIDE|THREE|3\") ~ \"RIGHT_SIDE\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"RIGHT REAR|FOUR|4\") ~ \"RIGHT_REAR\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"REAR|SIX|6\") ~ \"REAR\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"LEFT REAR|SEVEN|7|EIGHT|8\") ~ \"LEFT_REAR\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"LEFT SIDE|NINE|9\") ~ \"LEFT_SIDE\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"LEFT FRONT|TEN|10|ELEVEN|11\") ~ \"LEFT_FRONT\",\n      str_detect(str_to_upper(vehicle_first_impact_location), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n# clean and standardize vehicle movement\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    vehicle_movement = case_when(\n      str_detect(str_to_upper(vehicle_movement), \"STRAIGHT|CONSTANT\") ~ \"STRAIGHT\",\n      str_detect(str_to_upper(vehicle_movement), \"TURN.*RIGHT\") ~ \"TURNING_RIGHT\",\n      str_detect(str_to_upper(vehicle_movement), \"TURN.*LEFT\") ~ \"TURNING_LEFT\",\n      str_detect(str_to_upper(vehicle_movement), \"STOP|SLOW\") ~ \"SLOWING_STOPPING\",\n      str_detect(str_to_upper(vehicle_movement), \"BACK|REVERSE\") ~ \"BACKING\",\n      str_detect(str_to_upper(vehicle_movement), \"PARK\") ~ \"PARKED\",\n      str_detect(str_to_upper(vehicle_movement), \"START\") ~ \"STARTING\",\n      str_detect(str_to_upper(vehicle_movement), \"UNKNOWN|N/A\") ~ \"UNKNOWN\",\n      TRUE ~ \"OTHER\"\n    )\n  )\n\n\n# create a comprehensive validation function\nvalidate_crash_data &lt;- function(data) {\n  # initialize empty list for validation results\n  validation_results &lt;- list()\n  \n  # temporal validations\n  validation_results$temporal &lt;- data %&gt;%\n    summarise(\n      future_dates = sum(crash_date/time &gt; Sys.time()),\n      weekend_crashes = sum(lubridate::wday(crash_date/time, week_start = 1) %in% c(6,7)),\n      night_crashes = sum(format(crash_date/time, \"%H:%M:%S\") &gt;= \"18:00:00\" | \n                         format(crash_date/time, \"%H:%M:%S\") &lt;= \"06:00:00\"),\n      crashes_by_hour = table(format(crash_date/time, \"%H\"))\n    )\n  \n  # geographic validations\n  validation_results$geographic &lt;- data %&gt;%\n    summarise(\n      invalid_lat = sum(latitude &lt; 38 | latitude &gt; 40, na.rm = TRUE),\n      invalid_lon = sum(longitude &gt; -76 | longitude &lt; -78, na.rm = TRUE),\n      missing_coords = sum(is.na(latitude) | is.na(longitude)),\n      unique_locations = n_distinct(location, na.rm = TRUE)\n    )\n  \n  # vehicle validations\n  validation_results$vehicle &lt;- data %&gt;%\n    summarise(\n      invalid_years = sum(vehicle_year &lt; 1900 | vehicle_year &gt; year(Sys.Date()) + 1, na.rm = TRUE),\n      missing_makes = sum(is.na(vehicle_make)),\n      missing_models = sum(is.na(vehicle_model)),\n      unique_makes = n_distinct(vehicle_make, na.rm = TRUE),\n      unique_models = n_distinct(vehicle_model, na.rm = TRUE)\n    )\n  \n  # logical consistency checks\n  validation_results$logical &lt;- data %&gt;%\n    summarise(\n      # check if parked vehicles are marked as having movement\n      parked_moving = sum(parked_vehicle == \"Yes\" & vehicle_movement != \"PARKED\", na.rm = TRUE),\n      \n      # check for injuries in no-damage crashes\n      injuries_no_damage = sum(injury_severity != \"NO_INJURY\" & \n                             vehicle_damage_extent == \"NO_DAMAGE\", na.rm = TRUE),\n      \n      # check for fatal crashes without severe damage\n      fatal_minor_damage = sum(injury_severity == \"FATAL\" & \n                             vehicle_damage_extent %in% c(\"NO_DAMAGE\", \"MINOR\"), na.rm = TRUE),\n      \n      # check for driverless vehicles marked with driver characteristics\n      driverless_with_driver = sum(driverless_vehicle == \"Yes\" & \n                                  !is.na(driver_substance_abuse), na.rm = TRUE)\n    )\n  \n  # cross-reference checks\n  validation_results$cross_reference &lt;- data %&gt;%\n    summarise(\n      # light condition vs time of day consistency\n      light_time_mismatch = sum(\n        (format(crash_date/time, \"%H:%M:%S\") &gt;= \"18:00:00\" | \n         format(crash_date/time, \"%H:%M:%S\") &lt;= \"06:00:00\") & \n        light == \"DAYLIGHT\", na.rm = TRUE\n      ),\n      \n      # weather vs surface condition consistency\n      weather_surface_mismatch = sum(\n        (weather %in% c(\"RAIN\", \"SNOW\") & surface_condition == \"DRY\") |\n        (weather == \"CLEAR\" & surface_condition %in% c(\"ICE\", \"SNOW\")), \n        na.rm = TRUE\n      )\n    )\n  \n  # value distribution checks\n  validation_results$distributions &lt;- list(\n    speed_distribution = table(data$speed_limit),\n    damage_by_speed = table(data$vehicle_damage_extent, cut(data$speed_limit, \n                                                          breaks = c(0, 25, 35, 45, 55, Inf))),\n    injury_by_speed = table(data$injury_severity, cut(data$speed_limit, \n                                                    breaks = c(0, 25, 35, 45, 55, Inf)))\n  )\n  \n  return(validation_results)\n}\n\n# run validation and create summary report\nvalidation_summary &lt;- validate_crash_data(drivers_data_clean)\n\n# create a function to print validation results in a readable format\nprint_validation_summary &lt;- function(validation_results) {\n  cat(\"\\n=== VALIDATION SUMMARY ===\\n\")\n  \n  cat(\"\\nTEMPORAL VALIDATION:\")\n  cat(\"\\n- Future dates:\", validation_results$temporal$future_dates)\n  cat(\"\\n- Weekend crashes:\", validation_results$temporal$weekend_crashes)\n  cat(\"\\n- Night crashes:\", validation_results$temporal$night_crashes)\n  \n  cat(\"\\n\\nGEOGRAPHIC VALIDATION:\")\n  print(validation_results$geographic)\n  \n  cat(\"\\n\\nVEHICLE VALIDATION:\")\n  print(validation_results$vehicle)\n  \n  cat(\"\\n\\nLOGICAL CONSISTENCY CHECKS:\")\n  print(validation_results$logical)\n  \n  cat(\"\\n\\nCROSS-REFERENCE CHECKS:\")\n  print(validation_results$cross_reference)\n  \n  cat(\"\\n\\nDISTRIBUTIONS:\")\n  cat(\"\\nSpeed Limit Distribution:\\n\")\n  print(validation_results$distributions$speed_distribution)\n  \n  cat(\"\\nDamage by Speed Range:\\n\")\n  print(validation_results$distributions$damage_by_speed)\n}\n\n# run and print validation summary\nprint_validation_summary(validation_summary)\n\n# create data quality flags based on validation results\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    data_quality_flags = case_when(\n      # temporal flags\n      crash_date/time &gt; Sys.time() ~ \"FUTURE_DATE\",\n      \n      # geographic flags\n      (latitude &lt; 38 | latitude &gt; 40 | longitude &gt; -76 | longitude &lt; -78) ~ \"INVALID_COORDINATES\",\n      \n      # vehicle flags\n      (vehicle_year &lt; 1900 | vehicle_year &gt; year(Sys.Date()) + 1) ~ \"INVALID_VEHICLE_YEAR\",\n      \n      # logical consistency flags\n      (parked_vehicle == \"Yes\" & vehicle_movement != \"PARKED\") ~ \"PARKED_MOVING_MISMATCH\",\n      (injury_severity != \"NO_INJURY\" & vehicle_damage_extent == \"NO_DAMAGE\") ~ \"INJURY_NO_DAMAGE_MISMATCH\",\n      (injury_severity == \"FATAL\" & vehicle_damage_extent %in% c(\"NO_DAMAGE\", \"MINOR\")) ~ \"FATAL_MINOR_DAMAGE_MISMATCH\",\n      \n      # cross-reference flags\n      (format(crash_date/time, \"%H:%M:%S\") &gt;= \"18:00:00\" | \n       format(crash_date/time, \"%H:%M:%S\") &lt;= \"06:00:00\") & \n      light == \"DAYLIGHT\" ~ \"LIGHT_TIME_MISMATCH\",\n      \n      (weather %in% c(\"RAIN\", \"SNOW\") & surface_condition == \"DRY\") |\n      (weather == \"CLEAR\" & surface_condition %in% c(\"ICE\", \"SNOW\")) ~ \"WEATHER_SURFACE_MISMATCH\",\n      \n      TRUE ~ \"VALID\"\n    )\n  )\n\nquality_flag_summary &lt;- drivers_data_clean %&gt;%\n  group_by(data_quality_flags) %&gt;%\n  summarise(\n    count = n(),\n    percentage = n() / nrow(drivers_data_clean) * 100\n  ) %&gt;%\n  arrange(desc(count))\n\nprint(\"\\nData Quality Flag Summary:\")\nprint(quality_flag_summary)\n\nSomething still seems off with the data quality flags.\n\n# fix Light-Time Mismatches (largest issue ~5.2% of data)\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # extract hour from crash_date/time\n    crash_hour = as.numeric(format(crash_date/time, \"%H\")),\n    \n    # correct light condition based on time\n    light = case_when(\n      # dawn hours (5-7 AM)\n      crash_hour &gt;= 5 & crash_hour &lt; 7 ~ \"DAWN\",\n      \n      # daylight hours (7 AM - 6 PM)\n      crash_hour &gt;= 7 & crash_hour &lt; 18 ~ \"DAYLIGHT\",\n      \n      # dusk hours (6-8 PM)\n      crash_hour &gt;= 18 & crash_hour &lt; 20 ~ \"DUSK\",\n      \n      # night hours\n      crash_hour &gt;= 20 | crash_hour &lt; 5 ~ \"DARK_WITH_LIGHTING\",\n      \n      TRUE ~ light\n    )\n  ) %&gt;%\n  select(-crash_hour) # remove temporary column\n\n# fix Weather-Surface Condition Mismatches (~0.37% of data)\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # Adjust surface condition based on weather\n    surface_condition = case_when(\n      weather %in% c(\"RAIN\", \"RAINING\") & surface_condition == \"DRY\" ~ \"WET\",\n      weather == \"SNOW\" & surface_condition == \"DRY\" ~ \"SNOW\",\n      weather == \"CLEAR\" & surface_condition %in% c(\"ICE\", \"SNOW\") & \n        format(crash_date/time, \"%m\") %in% c(\"12\", \"01\", \"02\") ~ surface_condition, # Keep if winter months\n      weather == \"CLEAR\" & surface_condition %in% c(\"ICE\", \"SNOW\") ~ \"UNKNOWN\", # Change to unknown if non-winter\n      TRUE ~ surface_condition\n    )\n  )\n\n# fix Injury-Damage Inconsistencies\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # adjust injury severity or damage extent for logical consistency\n    injury_severity = case_when(\n      injury_severity != \"NO_INJURY\" & vehicle_damage_extent == \"NO_DAMAGE\" ~ \"NO_INJURY\",\n      injury_severity == \"FATAL\" & vehicle_damage_extent %in% c(\"NO_DAMAGE\", \"MINOR\") ~ \"SERIOUS_INJURY\",\n      TRUE ~ injury_severity\n    ),\n    \n    # ensure fatal crashes have appropriate damage extent\n    vehicle_damage_extent = case_when(\n      injury_severity == \"FATAL\" & vehicle_damage_extent %in% c(\"NO_DAMAGE\", \"MINOR\") ~ \"DISABLING\",\n      TRUE ~ vehicle_damage_extent\n    )\n  )\n\n# fix Invalid Coordinates (Montgomery County, MD boundaries)\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # montgomery County approximate boundaries\n    latitude = case_when(\n      latitude &lt; 38.9 | latitude &gt; 39.4 ~ NA_real_,\n      TRUE ~ latitude\n    ),\n    longitude = case_when(\n      longitude &lt; -77.5 | longitude &gt; -76.9 ~ NA_real_,\n      TRUE ~ longitude\n    )\n  )\n\n# clean Vehicle Makes and Models with High Frequency Patterns\n# analyze the most common patterns\nvehicle_patterns &lt;- drivers_data_clean %&gt;%\n  group_by(vehicle_make, vehicle_model) %&gt;%\n  summarise(\n    count = n(),\n    .groups = 'drop'\n  ) %&gt;%\n  arrange(desc(count)) %&gt;%\n  head(20)  # Look at top 20 patterns\n\n# now clean based on common patterns\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # standardize common model names\n    vehicle_model = case_when(\n      # remove digits from end of model names\n      str_detect(vehicle_model, \"[A-Z]+\\\\d+$\") ~ str_extract(vehicle_model, \"[A-Z]+\"),\n      # remove common suffixes\n      str_detect(vehicle_model, \"SDN|CPE|CVT|HBK\") ~ str_replace(vehicle_model, \"SDN|CPE|CVT|HBK\", \"\"),\n      TRUE ~ vehicle_model\n    ),\n    \n    # trim whitespace and standardize case\n    vehicle_model = str_trim(vehicle_model),\n    vehicle_make = str_trim(vehicle_make)\n  )\n\n# add Data Quality Score\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    data_quality_score = case_when(\n      data_quality_flags == \"VALID\" ~ 100,\n      data_quality_flags == \"LIGHT_TIME_MISMATCH\" ~ 90,\n      data_quality_flags == \"WEATHER_SURFACE_MISMATCH\" ~ 85,\n      data_quality_flags == \"INJURY_NO_DAMAGE_MISMATCH\" ~ 80,\n      data_quality_flags == \"INVALID_COORDINATES\" ~ 75,\n      data_quality_flags %in% c(\"FATAL_MINOR_DAMAGE_MISMATCH\", \"PARKED_MOVING_MISMATCH\") ~ 70,\n      TRUE ~ 60\n    )\n  )\n\n# verify cleaning results\ncleaning_verification &lt;- list()\n\n# check light-time consistency after cleaning\ncleaning_verification$light_time &lt;- drivers_data_clean %&gt;%\n  summarise(\n    light_time_mismatch_after = sum(\n      (format(crash_date/time, \"%H:%M:%S\") &gt;= \"18:00:00\" | \n       format(crash_date/time, \"%H:%M:%S\") &lt;= \"06:00:00\") & \n      light == \"DAYLIGHT\", \n      na.rm = TRUE\n    )\n  )\n\n# check weather-surface consistency after cleaning\ncleaning_verification$weather_surface &lt;- drivers_data_clean %&gt;%\n  summarise(\n    weather_surface_mismatch_after = sum(\n      (weather %in% c(\"RAIN\", \"SNOW\") & surface_condition == \"DRY\") |\n      (weather == \"CLEAR\" & surface_condition %in% c(\"ICE\", \"SNOW\")), \n      na.rm = TRUE\n    )\n  )\n\n# check data quality scores distribution\ncleaning_verification$quality_scores &lt;- drivers_data_clean %&gt;%\n  group_by(data_quality_score) %&gt;%\n  summarise(\n    count = n(),\n    percentage = n() / nrow(drivers_data_clean) * 100,\n    .groups = 'drop'\n  ) %&gt;%\n  arrange(desc(data_quality_score))\n\ncat(\"\\n=== CLEANING VERIFICATION RESULTS ===\\n\")\nprint(cleaning_verification)\n\nFinal Cleaning Steps Based on Verification Results\n\n# handle remaining weather-surface mismatches (577 cases)\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # create a more detailed weather-surface relationship\n    surface_condition = case_when(\n      # if it's raining, surface should be wet\n      weather %in% c(\"RAIN\", \"RAINING\") ~ \"WET\",\n      \n      # if it's snowing, surface should be snow or ice\n      weather == \"SNOW\" ~ \"SNOW\",\n      \n      # if it's freezing rain, surface should be ice\n      weather == \"FREEZING RAIN\" ~ \"ICE\",\n      \n      # for clear weather, keep existing surface condition if reasonable\n      weather == \"CLEAR\" & surface_condition %in% c(\"DRY\", \"WET\") ~ surface_condition,\n      \n      # for cloudy weather, keep existing surface condition\n      weather == \"CLOUDY\" ~ surface_condition,\n      \n      # for other cases, mark as unknown if inconsistent\n      TRUE ~ case_when(\n        surface_condition %in% c(\"DRY\", \"WET\", \"SNOW\", \"ICE\") ~ surface_condition,\n        TRUE ~ \"UNKNOWN\"\n      )\n    )\n  )\n\n# add more detailed quality flags and improve quality score\ndrivers_data_clean &lt;- drivers_data_clean %&gt;%\n  mutate(\n    # create compound quality flags for multiple issues\n    detailed_quality_flags = map2_chr(\n      data_quality_flags,\n      surface_condition,\n      ~case_when(\n        .x != \"VALID\" & .y == \"UNKNOWN\" ~ paste(.x, \"WITH_UNKNOWN_SURFACE\"),\n        TRUE ~ .x\n      )\n    ),\n    \n    # create a more nuanced quality score (0-100)\n    refined_quality_score = case_when(\n      data_quality_flags == \"VALID\" & !is.na(surface_condition) & \n      surface_condition != \"UNKNOWN\" ~ 100,\n      \n      data_quality_flags == \"VALID\" & \n      (is.na(surface_condition) | surface_condition == \"UNKNOWN\") ~ 95,\n      \n      data_quality_flags == \"LIGHT_TIME_MISMATCH\" ~ 90,\n      \n      data_quality_flags == \"WEATHER_SURFACE_MISMATCH\" & \n      !is.na(surface_condition) ~ 85,\n      \n      data_quality_flags == \"WEATHER_SURFACE_MISMATCH\" & \n      is.na(surface_condition) ~ 80,\n      \n      data_quality_flags == \"INJURY_NO_DAMAGE_MISMATCH\" ~ 75,\n      \n      data_quality_flags == \"INVALID_COORDINATES\" ~ 70,\n      \n      data_quality_flags %in% c(\"FATAL_MINOR_DAMAGE_MISMATCH\", \n                               \"PARKED_MOVING_MISMATCH\") ~ 65,\n      \n      TRUE ~ 60\n    )\n  )\n\n# create final data quality summary\nfinal_quality_summary &lt;- drivers_data_clean %&gt;%\n  summarise(\n    total_records = n(),\n    high_quality_records = sum(refined_quality_score &gt;= 95),\n    medium_quality_records = sum(refined_quality_score &gt;= 80 & refined_quality_score &lt; 95),\n    low_quality_records = sum(refined_quality_score &lt; 80),\n    mean_quality_score = mean(refined_quality_score),\n    median_quality_score = median(refined_quality_score),\n    \n    # percentage calculations\n    high_quality_percentage = (high_quality_records / total_records) * 100,\n    medium_quality_percentage = (medium_quality_records / total_records) * 100,\n    low_quality_percentage = (low_quality_records / total_records) * 100\n  )\n\ncat(\"\\n=== FINAL DATA QUALITY SUMMARY ===\\n\")\nprint(final_quality_summary)\n\n# create recommendations for further improvements\ncat(\"\\n=== RECOMMENDATIONS FOR FURTHER IMPROVEMENTS ===\\n\")\ncat(\"1. Weather-Surface Condition Relationship:\\n\")\ncat(\"   - \", sum(drivers_data_clean$surface_condition == \"UNKNOWN\"), \n    \"records still have unknown surface conditions\\n\")\ncat(\"   - Consider adding temperature data for better ice/snow validation\\n\\n\")\n\ncat(\"2. Geographic Data Quality:\\n\")\ncat(\"   - \", sum(is.na(drivers_data_clean$latitude) | is.na(drivers_data_clean$longitude)),\n    \"records have missing or invalid coordinates\\n\")\ncat(\"   - Consider implementing address geocoding for missing coordinates\\n\\n\")\n\ncat(\"3. Vehicle Information:\\n\")\ncat(\"   - \", sum(is.na(drivers_data_clean$vehicle_make)), \"records with missing vehicle makes\\n\")\ncat(\"   - \", sum(is.na(drivers_data_clean$vehicle_model)), \"records with missing vehicle models\\n\")\ncat(\"   - Consider implementing VIN decoding for missing vehicle information\\n\\n\")\n\nprocessing_summary &lt;- list(\n  initial_issues = list(\n    light_time_mismatch = 9744,\n    weather_surface_mismatch = 702,\n    injury_damage_mismatch = 306,\n    invalid_coordinates = 26,\n    fatal_minor_damage = 1,\n    parked_moving = 1\n  ),\n  \n  final_status = list(\n    quality_summary = final_quality_summary,\n    remaining_weather_surface_mismatch = 577,\n    remaining_unknown_surface = sum(drivers_data_clean$surface_condition == \"UNKNOWN\", na.rm = TRUE)\n  )\n)\n\ncat(\"\\n=== DATA PROCESSING SUMMARY ===\\n\")\ncat(\"Initial Issues vs. Final Status:\\n\")\ncat(\"- Light-Time Mismatches: \", processing_summary$initial_issues$light_time_mismatch, \n    \" -&gt; 0\\n\")\ncat(\"- Weather-Surface Mismatches: \", processing_summary$initial_issues$weather_surface_mismatch,\n    \" -&gt; \", processing_summary$final_status$remaining_weather_surface_mismatch, \"\\n\")\n\nVisualizations.\n\ninstall.packages(\"viridis\")\n\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(patchwork)  # for combining plots\nlibrary(viridis)    # for colorblind-friendly palettes\nlibrary(lubridate)\n\n# crash Time Patterns\ntime_plot &lt;- drivers_data_clean %&gt;%\n  mutate(\n    hour = hour(crash_date/time),\n    weekday = wday(crash_date/time, label = TRUE),\n    month = month(crash_date/time, label = TRUE)\n  ) %&gt;%\n  ggplot(aes(x = hour)) +\n  geom_histogram(binwidth = 1, fill = \"steelblue\", color = \"white\") +\n  labs(title = \"Distribution of Crashes by Hour of Day\",\n       x = \"Hour of Day\",\n       y = \"Number of Crashes\") +\n  theme_minimal() +\n  scale_x_continuous(breaks = seq(0, 23, 2))\n\n# injury Severity by Vehicle Type\nseverity_vehicle_plot &lt;- drivers_data_clean %&gt;%\n  filter(!is.na(vehicle_body_type), !is.na(injury_severity)) %&gt;%\n  count(vehicle_body_type, injury_severity) %&gt;%\n  group_by(vehicle_body_type) %&gt;%\n  mutate(pct = n/sum(n) * 100) %&gt;%\n  ggplot(aes(x = reorder(vehicle_body_type, -pct), y = pct, fill = injury_severity)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(title = \"Injury Severity by Vehicle Type\",\n       x = \"Vehicle Type\",\n       y = \"Percentage\",\n       fill = \"Injury Severity\") +\n  theme_minimal() +\n  scale_fill_viridis_d()\n\n# weather Conditions and Crash Frequency\nweather_plot &lt;- drivers_data_clean %&gt;%\n  count(weather) %&gt;%\n  ggplot(aes(x = reorder(weather, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"darkgreen\") +\n  coord_flip() +\n  labs(title = \"Crashes by Weather Condition\",\n       x = \"Weather Condition\",\n       y = \"Number of Crashes\") +\n  theme_minimal()\n\n# geographic Distribution of Crashes\nmap_plot &lt;- drivers_data_clean %&gt;%\n  filter(!is.na(latitude), !is.na(longitude)) %&gt;%\n  ggplot(aes(x = longitude, y = latitude)) +\n  geom_point(alpha = 0.1, color = \"red\") +\n  labs(title = \"Geographic Distribution of Crashes\",\n       x = \"Longitude\",\n       y = \"Latitude\") +\n  theme_minimal()\n\n# collision Types\ncollision_plot &lt;- drivers_data_clean %&gt;%\n  count(collision_type) %&gt;%\n  mutate(pct = n/sum(n) * 100) %&gt;%\n  ggplot(aes(x = reorder(collision_type, -pct), y = pct)) +\n  geom_bar(stat = \"identity\", fill = \"orange\") +\n  coord_flip() +\n  labs(title = \"Distribution of Collision Types\",\n       x = \"Collision Type\",\n       y = \"Percentage\") +\n  theme_minimal()\n\n# speed Limit and Crash Severity\nspeed_severity_plot &lt;- drivers_data_clean %&gt;%\n  filter(!is.na(speed_limit), !is.na(injury_severity)) %&gt;%\n  ggplot(aes(x = factor(speed_limit), fill = injury_severity)) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"Injury Severity by Speed Limit\",\n       x = \"Speed Limit\",\n       y = \"Proportion\",\n       fill = \"Injury Severity\") +\n  theme_minimal() +\n  scale_fill_viridis_d()\n\n# vehicle Age Distribution\nvehicle_age_plot &lt;- drivers_data_clean %&gt;%\n  mutate(vehicle_age = year(crash_date/time) - vehicle_year) %&gt;%\n  filter(vehicle_age &gt;= 0, vehicle_age &lt;= 30) %&gt;%\n  ggplot(aes(x = vehicle_age)) +\n  geom_histogram(binwidth = 1, fill = \"purple\", color = \"white\") +\n  labs(title = \"Distribution of Vehicle Age\",\n       x = \"Vehicle Age (Years)\",\n       y = \"Count\") +\n  theme_minimal()\n\n# surface Condition and Weather Relationship\nsurface_weather_plot &lt;- drivers_data_clean %&gt;%\n  filter(!is.na(surface_condition), !is.na(weather)) %&gt;%\n  count(surface_condition, weather) %&gt;%\n  group_by(weather) %&gt;%\n  mutate(pct = n/sum(n) * 100) %&gt;%\n  ggplot(aes(x = weather, y = surface_condition, fill = pct)) +\n  geom_tile() +\n  scale_fill_viridis_c() +\n  labs(title = \"Surface Condition by Weather\",\n       x = \"Weather\",\n       y = \"Surface Condition\",\n       fill = \"Percentage\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n# combine plots using patchwork\nlayout &lt;- \"\nAABB\nCCDD\nEEFF\nGGHH\n\"\n\ncombined_plots &lt;- time_plot + severity_vehicle_plot + \n                 weather_plot + map_plot + \n                 collision_plot + speed_severity_plot + \n                 vehicle_age_plot + surface_weather_plot +\n                 plot_layout(design = layout)\n\nprint(combined_plots)\n\nggsave(\"crash_analysis_dashboard.pdf\", combined_plots, width = 20, height = 24)\n\nsummary_stats &lt;- list(\n  \n  time_stats = drivers_data_clean %&gt;%\n    mutate(\n      hour = hour(crash_date/time),\n      weekday = wday(crash_date/time, label = TRUE)\n    ) %&gt;%\n    summarise(\n      peak_hour = names(which.max(table(hour))),\n      weekend_crashes = mean(weekday %in% c(\"Sat\", \"Sun\")) * 100\n    ),\n  \n  severity_stats = drivers_data_clean %&gt;%\n    group_by(injury_severity) %&gt;%\n    summarise(\n      count = n(),\n      percentage = n() / nrow(drivers_data_clean) * 100\n    ),\n  \n  weather_stats = drivers_data_clean %&gt;%\n    group_by(weather) %&gt;%\n    summarise(\n      count = n(),\n      percentage = n() / nrow(drivers_data_clean) * 100\n    ),\n  \n  vehicle_stats = drivers_data_clean %&gt;%\n    summarise(\n      avg_vehicle_age = mean(year(crash_date/time) - vehicle_year, na.rm = TRUE),\n      most_common_make = names(which.max(table(vehicle_make)))\n    )\n)\n\nprint(summary_stats)\n\nSave the final cleaned dataset.\n\ncurrent_timestamp &lt;- format(Sys.time(), \"%Y%m%d_%H%M\")\n\n# create directories if they don't exist\ndir.create(\"cleaned_data\", showWarnings = FALSE)\ndir.create(\"data_documentation\", showWarnings = FALSE)\n\nstr_result &lt;- capture.output(str(drivers_data_clean))\nprint(\"Data structure:\")\nprint(str_result)\n\ncolumn_info &lt;- data.frame(\n  column_name = names(drivers_data_clean),\n  data_type = sapply(drivers_data_clean, function(x) class(x)[1]),  # Take first class if multiple\n  stringsAsFactors = FALSE\n)\n\ncolumn_info$description &lt;- sapply(column_info$column_name, function(col_name) {\n  case_when(\n    col_name == \"report_number\" ~ \"Unique report identifier\",\n    col_name == \"local_case_number\" ~ \"Local case number for the incident\",\n    col_name == \"agency_name\" ~ \"Name of reporting agency\",\n    col_name == \"acrs_report_type\" ~ \"Type of crash report\",\n    col_name == \"crash_date/time\" ~ \"Date and time of the crash\",\n    col_name == \"route_type\" ~ \"Type of route where crash occurred\",\n    col_name == \"road_name\" ~ \"Name of the road where crash occurred\",\n    col_name == \"cross_street_name\" ~ \"Name of the nearest cross-street\",\n    col_name == \"off_road_description\" ~ \"Description for off-road incidents\",\n    col_name == \"municipality\" ~ \"Municipality where crash occurred\",\n    col_name == \"related_non_motorist\" ~ \"Type of non-motorist involved\",\n    col_name == \"collision_type\" ~ \"Type of collision\",\n    col_name == \"weather\" ~ \"Weather conditions during crash\",\n    col_name == \"surface_condition\" ~ \"Road surface condition\",\n    col_name == \"light\" ~ \"Light conditions\",\n    col_name == \"traffic_control\" ~ \"Traffic control present\",\n    col_name == \"driver_substance_abuse\" ~ \"Driver substance abuse status\",\n    col_name == \"non_motorist_substance_abuse\" ~ \"Non-motorist substance abuse status\",\n    col_name == \"person_id\" ~ \"Unique identifier for person involved\",\n    col_name == \"driver_at_fault\" ~ \"Indicator if driver was at fault\",\n    col_name == \"injury_severity\" ~ \"Severity of injuries\",\n    col_name == \"circumstance\" ~ \"Contributing circumstances\",\n    col_name == \"driver_distracted_by\" ~ \"Driver distraction factors\",\n    col_name == \"drivers_license_state\" ~ \"State of driver's license\",\n    col_name == \"vehicle_id\" ~ \"Unique identifier for vehicle\",\n    col_name == \"vehicle_damage_extent\" ~ \"Extent of vehicle damage\",\n    col_name == \"vehicle_first_impact_location\" ~ \"Location of first impact on vehicle\",\n    col_name == \"vehicle_body_type\" ~ \"Type of vehicle body\",\n    col_name == \"vehicle_movement\" ~ \"Vehicle movement during crash\",\n    col_name == \"vehicle_going_dir\" ~ \"Direction vehicle was traveling\",\n    col_name == \"speed_limit\" ~ \"Posted speed limit\",\n    col_name == \"driverless_vehicle\" ~ \"Indicator if vehicle was driverless\",\n    col_name == \"parked_vehicle\" ~ \"Indicator if vehicle was parked\",\n    col_name == \"vehicle_year\" ~ \"Year of vehicle manufacture\",\n    col_name == \"vehicle_make\" ~ \"Vehicle manufacturer\",\n    col_name == \"vehicle_model\" ~ \"Vehicle model\",\n    col_name == \"latitude\" ~ \"Latitude of crash location\",\n    col_name == \"longitude\" ~ \"Longitude of crash location\",\n    col_name == \"location\" ~ \"Combined location coordinates\",\n    col_name == \"data_quality_flags\" ~ \"Data quality flags from cleaning process\",\n    col_name == \"detailed_quality_flags\" ~ \"Detailed quality flags from cleaning process\",\n    col_name == \"refined_quality_score\" ~ \"Numerical score indicating data quality\",\n    TRUE ~ paste(\"Description for\", col_name)  # Default description for any new columns\n  )\n})\n\ncolumn_info$example_values &lt;- sapply(drivers_data_clean, function(x) {\n  if (is.numeric(x)) {\n    paste(\"Range:\", min(x, na.rm = TRUE), \"to\", max(x, na.rm = TRUE))\n  } else {\n    unique_vals &lt;- unique(na.omit(x))\n    if (length(unique_vals) &gt; 5) {\n      paste(paste(unique_vals[1:5], collapse = \", \"), \"... and\", length(unique_vals) - 5, \"more values\")\n    } else {\n      paste(unique_vals, collapse = \", \")\n    }\n  }\n})\n\nwrite_csv(drivers_data_clean, \n          file = paste0(\"cleaned_data/crash_data_clean_\", current_timestamp, \".csv\"))\n\nsaveRDS(drivers_data_clean, \n        file = paste0(\"cleaned_data/crash_data_clean_\", current_timestamp, \".rds\"))\n\nwrite_csv(column_info, \n          file = paste0(\"data_documentation/data_dictionary_\", current_timestamp, \".csv\"))\n\nreadme_text &lt;- sprintf(\"\n# Crash Data Cleaning Documentation\n\n## Version Information\n- Date Created: %s\n- Original Records: 186,931\n- Final Records: %d\n\n## Data Quality Metrics\n- Total Records: %d\n- Records with Quality Flags: %d\n- Clean Records: %d\n\n## Files in this Package\n1. crash_data_clean_%s.csv - Main data file (CSV format)\n2. crash_data_clean_%s.rds - R data file (RDS format)\n3. data_dictionary_%s.csv - Data dictionary with column descriptions\n\n## Column Summary\nTotal number of columns: %d\nSee data dictionary file for detailed information about each column.\n\n## Data Quality Notes\n- Data has been cleaned and standardized\n- Quality flags have been added to mark potential issues\n- Missing values have been handled according to context\n- Inconsistent categories have been standardized\n\n## Usage Notes\n- Please refer to the data dictionary for column descriptions\n- Check quality flags before analysis\n- Some columns may contain standardized values\n\n## Contact\nFor questions about this dataset, please contact [Your Contact Information]\n\n## Last Updated\n%s\n\",\n    format(Sys.time(), \"%Y-%m-%d %H:%M:%S\"),\n    nrow(drivers_data_clean),\n    nrow(drivers_data_clean),\n    sum(drivers_data_clean$data_quality_flags != \"VALID\"),\n    sum(drivers_data_clean$data_quality_flags == \"VALID\"),\n    current_timestamp,\n    current_timestamp,\n    current_timestamp,\n    ncol(drivers_data_clean),\n    format(Sys.time(), \"%Y-%m-%d %H:%M:%S\")\n)\n\nwriteLines(readme_text, \n           paste0(\"data_documentation/README_\", current_timestamp, \".md\"))\n\nzip_files &lt;- c(\n    paste0(\"cleaned_data/crash_data_clean_\", current_timestamp, \".csv\"),\n    paste0(\"cleaned_data/crash_data_clean_\", current_timestamp, \".rds\"),\n    paste0(\"data_documentation/data_dictionary_\", current_timestamp, \".csv\"),\n    paste0(\"data_documentation/README_\", current_timestamp, \".md\")\n)\n\nzip(paste0(\"cleaned_data/crash_data_package_\", current_timestamp, \".zip\"),\n    files = zip_files)\n\ncat(\"\\nData saving complete!\\n\")\ncat(\"Files saved:\\n\")\ncat(\"1. CSV data file\\n\")\ncat(\"2. RDS data file\\n\")\ncat(\"3. Data dictionary\\n\")\ncat(\"4. README documentation\\n\")\ncat(\"5. Complete package (zip)\\n\")\ncat(\"\\nLocation: ./cleaned_data/ and ./data_documentation/\\n\")"
  },
  {
    "objectID": "dataset/cleaning_for_crashes.html",
    "href": "dataset/cleaning_for_crashes.html",
    "title": "Data",
    "section": "",
    "text": "library(lubridate)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(skimr)\n\n\ncrash_df &lt;- read.csv(\"dataset-ignore/Crash_Reporting_Drivers_Data.csv\")\n\n# remove columns with a high percentage of missing values\n\n# Define threshold for column selection\nthreshold &lt;- 0.8\n\n# Remove columns with a high percentage of missing values\ndata_clean &lt;- crash_df |&gt;\n  select_if(~ mean(is.na(.)) &lt; threshold)\n# convert case number column to character format\n\ndata_clean$Local.Case.Number &lt;- as.character(data_clean$Local.Case.Number)\n\n# convert date and time column to datetime format\n\ndata_clean$Crash.Date.Time &lt;- mdy_hms(data_clean$Crash.Date.Time)\n\n\n# fill missing values in route type column with the most common value\n\nmost_common_route &lt;- data_clean$Route_Type |&gt; \n\n  na.omit() |&gt;\n\n  table() |&gt; \n\n  which.max()\n\n\n\ndata_clean$Route_Type[is.na(data_clean$Route_Type)] &lt;- names(most_common_route)\n\n\n\n# remove duplicate rows\n\ndata_clean &lt;- data_clean |&gt; distinct()\n\nsubset_data &lt;- data_clean %&gt;%\n  slice(1:1000) # Adjust based on your requirements\n\n\n# check the summary of cleaned data\n\nsummary(data_clean)\n\nsaveRDS(subset_data,\"dataset/cleaned_dataset.rds\")"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is a website for the final project for MA[46]15 Data Science with R by Team [TEAMNAME]."
  },
  {
    "objectID": "about.html#our-team",
    "href": "about.html#our-team",
    "title": "About",
    "section": "Our Team",
    "text": "Our Team\n\nYifei Zhang\nMajor: Economics & Mathematics Home: China\n\n\nPrimah Muwanga\nMajor: Data Science Minor: Engineering Science Home: Uganda\n\n\nXiang Fu\nMajor: Data Science Minor: Statistics Home: China\n\n\nNora O’Neill\nMajor: Mathematics (Statistics Concentration) Minor: Business Home: Newton, Massachusetts\n\n\nTejas Kaur\nMajor: Economics and Mathematics Minor: Data Science Home: Jaipur, India\n\n\n\nAbout this Template.\nThis is based off of the standard Quarto website template from RStudio (2023.09.0 Build 463)."
  },
  {
    "objectID": "big_picture.html",
    "href": "big_picture.html",
    "title": "Big Picture",
    "section": "",
    "text": "This comes from the file big_picture.Rmd.\nThink of this page as your 538/Upshot style article. This means that you should try to tell a story through the data and your analysis. Read articles from those sites and similar sites to get a feeling for what they are like. Try to write in the style of a news or popular article. Importantly, this page should be geared towards the general public. You shouldn’t assume the reader understands how to interpret a linear regression or a complicated plot. Focus on interpretation and visualizations."
  },
  {
    "objectID": "big_picture.html#rubric-on-this-page",
    "href": "big_picture.html#rubric-on-this-page",
    "title": "Big Picture",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nTitle\n\nYour big picture page should have a creative/click-bait-y title/headline that provides a hint about your thesis.\n\nClarity of Explanation\n\nYou should have a clear thesis/goal for this page. What are you trying to show? Make sure that you explain your analysis in detail but don’t go into top much mathematics or statistics. The audience for this page is the general public (to the extent possible). Your thesis should be a statement, not a question.\nEach figure should be very polished and also not too complicated. There should be a clear interpretation of the figure so the figure has a clear purpose. Even something like a histogram can be difficult to interpret for non-experts.\n\nCreativity\n\nDo your best to make things interesting. Think of a story. Think of how each part of your analysis supports the previous part or provides a different perspective.\n\nInteractive component\n\nQuality and ease of use of the interactive components. Is it clear what can be explored using your interactive components? Does it enhance and reinforce your conclusions?\n\nThis page should be self-contained.\n\nNote: This page should have no code visible, i.e. use #| echo: FALSE."
  },
  {
    "objectID": "big_picture.html#rubric-other-components",
    "href": "big_picture.html#rubric-other-components",
    "title": "Big Picture",
    "section": "Rubric: Other components",
    "text": "Rubric: Other components\n\nVideo Recording\nMake a video recording (probably using Zoom) demonstrating your interactive components. You should provide a quick explanation of your data and demonstrate some of the conclusions from your EDA. This video should be no longer than 4 minutes. Include a link to your video (and password if needed) in your README.md file on your Github repository. You are not required to provide a link on the website. This can be presented by any subset of the team members.\n\n\nRest of the Site\nFinally, here are important things to keep in mind for the rest of the site.\nThe main title of your page is informative. Each post has an author/description/informative title. All lab required posts are present. Each page (including the home page) has a nice featured image associated with it. Your about page is up to date and clean. You have removed the generic posts from the initial site template."
  }
]